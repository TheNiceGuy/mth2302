\documentclass[11pt]{article}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{standalone}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{siunitx}
\usepackage{gphys}
\usepackage{thmtools}
\usepackage{mdframed}
\usepackage{float}
\usepackage{tikz}
\usepackage{contour}
\usepackage{scrextend}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{pgfplots}
\usepackage{slashbox}
\usepackage[explicit]{titlesec}
\usepackage{fix-cm}

\usetikzlibrary{babel}
\usetikzlibrary{calc}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{decorations.markings}

%%%%%%%%%%%%%%%%%%
% Configurations %
%%%%%%%%%%%%%%%%%%

\geometry{
	top=34mm,
	bottom=34mm
}

%%%%%%%%%%
% Macros %
%%%%%%%%%%

\input{eq}

\newcommand\card{%
	\ensuremath{%
		\mathrm{card}\,%
	}%
}%

\newcommand\comb[2]{%
	\ensuremath{%
		\mathcal{C}_{#2}^{#1}%
	}%
}%

\newcommand\perm[2]{%
	\ensuremath{%
		\mathcal{P}_{#2}^{#1}%
	}%
}%

\renewcommand\P[1]{%
	\ensuremath{%
		\mathbb{P}\left[\,#1\,\right]%
	}%
}%

\newcommand\Pg[2]{%
	\ensuremath{%
		\mathbb{P}\left[\,#1\middle\rvert#2\,\right]%
	}%
}%

\newcommand\e{%
	\ensuremath{%
		\mathrm{e}%
	}%
}%

\newcommand\Bin[2]{%
	\ensuremath{%
		\mathcal{B}\left(#1,#2\right)%
	}%
}%

\newcommand\Geo[1]{%
	\ensuremath{%
		\mathcal{G}\left(#1\right)%
	}%
}%

\newcommand\Poi[1]{%
	\ensuremath{%
		\mathrm{Poi}\left(#1\right)%
	}%
}%

\newcommand\Uni[2]{%
	\ensuremath{%
		\mathcal{U}\left(#1,#2\right)%
	}%
}%

\newcommand\Exp[1]{%
	\ensuremath{%
		\mathrm{Exp}\left(#1\right)%
	}%
}%

\newcommand\Gam[2]{%
	\ensuremath{%
		\mathrm{Gam}\left(#1,#2\right)%
	}%
}%

\newcommand\Norm[2]{%
	\ensuremath{%
		\mathcal{N}\left(#1,#2\right)%
	}%
}%

\newcommand\BiNorm[5]{%
	\ensuremath{%
		\mathcal{N}\left(#1,#2,#3,#4,#5\right)%
	}%
}%

\newcommand\Esp[1]{%
	\ensuremath{%
		\mathrm{E}\left[\,#1\,\right]%
	}%
}%

\newcommand\Espg[2]{%
	\ensuremath{%
		\mathrm{E}\left[\,#1\middle\vert#2\,\right]%
	}%
}%

\newcommand\Var[1]{%
	\ensuremath{%
		\mathrm{Var}\left[\,#1\,\right]%
	}%
}%

\newcommand\Varg[2]{%
	\ensuremath{%
		\mathrm{Var}\left[\,#1\middle\vert#2\,\right]%
	}%
}%

\newcommand\Std[1]{%
	\ensuremath{%
		\mathrm{Std}\left[\,#1\,\right]%
	}%
}%

\newcommand\Cov[2]{%
	\ensuremath{%
		\mathrm{Cov}\left[#1,#2\right]%
	}%
}%

\newcommand\Min{%
	\ensuremath{%
		\mathrm{min}\,%
	}%
}%

\newcommand\invec[1]{%
	\ensuremath{%
		\left[%
			\begin{matrix}%
				#1%
			\end{matrix}%
		\right]^T%
	}%
}%

\newcommand\Biais[1]{%
	\ensuremath{%
		\mathrm{Biais}\left[\,#1\,\right]\,%
	}%
}%

\newcommand\EQM[1]{%
	\ensuremath{%
		\mathrm{EQM}\left[\,#1\,\right]\,%
	}%
}%

\newtheorem{axiome}{Axiome}
\newtheorem{theoreme}{Théoreme}[section]
\newtheorem*{proposition}{Proposition}
\newtheorem*{notation}{Notation}
\newtheorem*{exemple*}{Exemple}
\newtheorem{property}{Propriété}
\newtheorem*{corollary}{Corollaire}

\theoremstyle{remark}
\newtheorem*{remark}{Remarque}

% we encapsulate the definition inside a custom environment
\makeatletter
\theoremstyle{definition}
\newtheorem*{@definition}{Définition}

\newenvironment{definition}{%
	\begin{@definition}%
}{%
	\end{@definition}%
	\setcounter{property}{0}%
}
\makeatother

\newmdtheoremenv[
	linecolor=black,
	backgroundcolor=gray!40,
	ntheorem
]{exemple}{Exemple}[section]

\titleformat{\section}{}{}{0em}{\fancysection[\thesection]{#1}}

%%%%%%%%%%%%%
% Documents %
%%%%%%%%%%%%%

\begin{document}

\input{fancysection}

\tableofcontents

\pagebreak
\section{Introduction}
\begin{definition}
	Une expérience est \textit{aléatoire} si un observateur peut la répéter
	dans les mêmes conditions, mais sans pouvoir en prédire le résultat.
\end{definition}

\begin{definition}
	Un \textit{espace échantilion} est un ensemble $S$ des résultats possibles.
\end{definition}

\begin{remark}
	Un espace d'échantilion peut être \textit{qualitatif} ou
	\textit{quantitatif}, ainsi que \textit{discret}, \textit{continu} ou
	\textit{mixte}. Il peut aussi être \textit{dénombrable} ou
	\textit{non-dénombrable}.
\end{remark}

\begin{definition}
	Un \textit{évènement} $A$ est un sous-ensemble de $S$ d'intêret à
	l'observateur.
\end{definition}

\begin{definition}
	Un \textit{évènement élémentaire} $A$ est un résultat particulier,
	c'est-à-dire, un élèment de $S$.
\end{definition}

\begin{exemple}
	On observe le résultat du lancer de deux pièces de monnaie. On note $P$
	comme un lancer pile et $F$ comme un lancer face. L'ensemble est donc
	\begin{equation*}
		S=\left\{PP, FF, PF, FP\right\}
	\end{equation*}
	avec chaque résultat ayant \SI{25}{\percent} d'arriver. L'ensemble $S$ est
	qualitatif, soit pile ou face, et discret.
\end{exemple}

\begin{exemple}
	On observer la somme obtenue lors du lancer de deux dés à 6 faces.
	L'ensemble de résultat possible est
	\begin{equation*}
		S=\left\{2,3,4,5,6,7,8,9,10,11,12\right\}.
	\end{equation*}
	Il est quantitatif et discret.
\end{exemple}

\begin{exemple}
	On compte le nombre de lancers d'une pièce pour obtenir une première fois
	un pile. L'espace échantilion est
	\begin{equation*}
		S=\left\{1,2,3,4,5,\dots\right\},
	\end{equation*}
	car il est possible qu'une grande quantité de lancer est effectuée avant
	d'obtenir un pile. L'ensemble $S$ est quantitatif, discret et
	infini dénombrable.
\end{exemple}

\pagebreak
\begin{exemple}
	On mesure le temps d'attente à l'arrêt d'autobus. L'espace échantilion est
	\begin{equation*}
		S=\left[0,\infty\right[.
	\end{equation*}
	L'ensemble $S$ est quantitatif, continu et infini non-dénombrable.
\end{exemple}

\begin{exemple}
	On mesure le temps d'attente à l'arrêt d'autobus ainsi que le nombre de
	personnes en file à l'arrivée de l'autobus. L'espace échantilion est
	\begin{equation*}
		S=T\times U,
	\end{equation*}
	avec
	\begin{equation*}
		T=\left[0,\infty\right[
	\end{equation*}
	et
	\begin{equation*}
		U=\left\{1,2,3,4,5,\dots\right\}.
	\end{equation*}
	L'ensemble $S$ est quantitatif et mixte. Un exemple d'évènement élèmentaire
	peut être un couple tel que $\left(\SI{4.25}{\second}, 4\right)$.
\end{exemple}

\subsection{Lien entre l'expérience aléatoire et son modèle}
\begin{definition}
	La \textit{fréquence relative} $f_A$ d'un évènement $A$ est le rapport
	entre le nombre d'observations $n_A$ de l'évèmenent et le nombre $n$ de
	répétition de l'expérience, c'est-à-dire
	\begin{equation*}
		f_A=\frac{n_A}{n}.
	\end{equation*}

\end{definition}

\begin{definition}
	La limite lorsque l'expérience est répétée infiniment est la
	\textit{probabilité} de l'évènement $A$, dénotée
	\begin{equation*}
		\P{A}=\lim_{n\rightarrow\infty}f_A.
	\end{equation*}
\end{definition}

\subsection{Opérations sur les ensembles}
Soit deux ensembles $A$ et $B$ tel que $A,B\subset S$. La figure
\ref{fig:venn_intersection} montre une intersection tandis que la figure
\ref{fig:venn_union} montre une union entre $A$ et $B$. La figure
\ref{fig:venn_complement} montre le complémenet de $A$ et la figure
\ref{fig:venn_exclusion} montre l'exclusion de deux ensembles.

\begin{figure}[H]
	\centering
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/intersection}
		\caption{$A\cap B$}
		\label{fig:venn_intersection}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/union}
		\caption{$A\cup B$}
		\label{fig:venn_union}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/complement}
		\caption{$A^c$}
		\label{fig:venn_complement}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/combinaison}
		\caption{$\left(A\cup B\right)\setminus\left(A\cap B\right)$}
		\label{fig:venn_exclusion}
	\end{subfigure}
	\caption{Opérations d'ensembles démontrées sur des diagrammes de Venn}
\end{figure}

\section{Probabilités Élémentaires}
\subsection{Axiomes fondamentales de la probabilité}
\begin{axiome}
	La probabilité d'un évèment $A$ est plus grand ou égal à 0, c'est-à-dire
	\begin{equation*}
		\P{A}\geq 0,
	\end{equation*}
	pour tout $A\in S$.
\end{axiome}

\begin{axiome}
	La probabilité de l'espace d'échantilion $S$ est 1, c'est-à-dire
	\begin{equation*}
		\P{S}=1.
	\end{equation*}
\end{axiome}

\begin{axiome}
	La probabilité d'un évènement $A$ ou d'un évènement $B$ est équivalent à la
	somme de leur probabilité, c'est-à-dire
	\begin{equation*}
		\P{A\cup B}=\P{A}+\P{B},
	\end{equation*}
	si $A\cup B=\emptyset$.

	En général, 
	\begin{equation*}
		\P{\bigcup_{k=1}^\infty A_k}=\sum_{k=1}^\infty\P{A_k},
	\end{equation*}
	si $A_i\cup A_j=\emptyset$, $\forall i,j$.
\end{axiome}

\begin{theoreme}
	$\P{A^c}=1-\P{A}$.
\end{theoreme}

\begin{proof}
	On sait que $A\cup A^c=S$ et $A\cap A^c=\emptyset$. Hors,
	$\P{A\cup A^c}=\P{S}\Leftrightarrow\P{A}+\P{A^c}=1$, car
	$A\cup A^c=\emptyset$.
	En réarrangeant, on obtient que $\P{A^c}=1-\P{A}.$
\end{proof}

\begin{theoreme}
	$\P{A}\leq 1$.
\end{theoreme}

\begin{proof}
	On sait que $\P{A^c}\geq 0$ et $\P{A^c}=1-\P{A}$. En réarrangeant, on
	obtient que $\P{A}\leq 1$.
\end{proof}

\begin{theoreme}
	$\P{\emptyset}=0$.
\end{theoreme}

\begin{proof}
	On sait que $S^c=\emptyset$. Par conséquent, $\P{\emptyset}=1-\P{S}=1-1=0$.
\end{proof}

\begin{theoreme}
	$A\subset B\Rightarrow \P{A}\leq\P{B}$.
\end{theoreme}

\begin{proof}
	La différence entre $A$ et $B$ est $A^c\cap B$ de sorte qu'on peut écrire
	$B=A\cup(A^c\cap B)$. Par conséquent, $\P{B}=\P{A}+\P{A^c\cap B}
	\Leftrightarrow\P{B}-\P{A}=\P{A^c\cap B}\geq 0$, car
	$A\cup(A^c\cap B)=\emptyset$. En réarrangeant, on obtient $\P{A}\leq\P{B}$.
\end{proof}

\begin{theoreme}
	$\P{A\cup B}=\P{A}+\P{B}-\P{A\cap B}$.
\end{theoreme}

\subsection{Principe d'équiprobabilité}
\begin{definition}
	Une expérience aléatoire est dites \textit{equiprobable} si $\P{e_i}=c$,
	où $0\leq c\leq 1$, pour tout $e_i\in S$.
\end{definition}

\subsubsection{Ensemble fini}
\begin{theoreme}
	Soit $S=\left\{e_1,e_2,\dots,e_n\right\}$ un ensemble d'événements fini
	et equiprobable, alors
	\begin{equation*}
		\P{e_1}=\P{e_2}=\cdots=\P{e_n}=\frac{1}{n}.
	\end{equation*}
\end{theoreme}

\begin{corollary}
	Soit $A\subset S$, alors $\P{A}=\dfrac{n_A}{n}$.
\end{corollary}

\subsubsection{Ensemble infini dénombrable}
\begin{theoreme}
	Soit $S=\left\{e_1,e_2,\dots\right\}$ un ensemble d'événements infini
	et dénombrable. Alors, les résultats d'une expérience aléatoire ne peuvent
	pas être équiprobables.
\end{theoreme}

\begin{proof}
	On suppose que $\P{e_i}=\epsilon$ pour tout $i=\left\{1,2,\dots\right\}$,
	où $0<\epsilon<1$. Par conséquent, $\P{S}=\P{e_1}+\P{e_2}+\cdots=\infty$,
	ce qui est en contradiction avec les axiomes de la probabilité. D'une
	manière similaire, si $\P{e_i}=0$ pour tout $i=\left\{1,2,\dots\right\}$,
	alors $\P{S}=0$. Ce qui est aussi en contradiction avec les axiomes de la
	probabilités. Par conséquent, un ensemble infini et dénombrable ne peut
	pas être équiprobable.
\end{proof}

\subsubsection{Ensemble infini non-dénombrable}
\begin{theoreme}
	Soit $S=\left[a,b\right]$, où $a<b$, un ensemble d'événements infini,
	non-dénombrable, et equiprobable, alors
	\begin{equation*}
		\P{A}=\frac{d-c}{b-a},
	\end{equation*}
	où $A=\left[c,d\right]\in S$.
\end{theoreme}

\begin{corollary}
	Soit $A\subset S$ un événement élémentaire, alors $\P{A}=0$.
\end{corollary}

\begin{exemple}
	On calcule la somme de deux dés lancés. Quelle est la probabilité d'obtenir
	chaque somme possible?
	
	L'espace échantilion est $S=L\times L$, où $L=\left\{1,2,3,4,5,6\right\}$
	est le résultat possible d'un dé. On peut écrire
	$S=\left\{(1,1),(1,2),\dots,(6,6)\right\}$. On suppose qu'il y a
	équiprobabilité de sorte que la probabilité d'obtenir un évènement
	élémentaire est $\nicefrac{1}{36}$.

	Hors, certaines des sommes sont dupliquées de sorte que la probabilité
	d'obtenir une somme particulière est donnée par la table suivante.
	\begin{table}[H]
		\centering
		\begin{tabular}{r|ccccccccccc}
			$\phantom{\mathbb{P}(}A\phantom{)}$ &
			$2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$ & $11$ & $12$\\
			$\P{A}$ &
			$\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ &
			$\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ &
			$\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\\
		\end{tabular}
	\end{table}
\end{exemple}

\subsection{Diagramme en arbre}
\begin{exemple}
	On lance un dé, puis une pièce de monnaie. Combien de résultats est-il
	possible?
	
	On peut représenté la situation par l'arbre suivant.
	\begin{figure}[H]
		\centering
		\input{figure/diagramme_arbre.tex}
	\end{figure}
	Par le principe de multiplication, il y a $6\cdot 2=12$ possibilités.
\end{exemple}

\subsection{Permutations}
\begin{definition}
	Une \textit{permutation} correspond à un choix de $k$ objets parmi $n$
	objets distincts. Le choix se fait sans remise et dans un ordre spécifique.
\end{definition}

La table \ref{tb:permutation} résume le principe d'une permutation à l'aide
d'une pige d'objet. À chaque pige, la quantité d'objets diminue de 1.

\begin{table}[H]
	\centering
	\caption{Pige d'objets}
	\begin{tabular}{r|cccc}
		\toprule
		Choix d'objet   &   1 &   2   & $\dots$ & k\\
		Objets restants & $n$ & $n-1$ & $\dots$ & $n-(n-k)$\\
		\bottomrule
	\end{tabular}
	\label{tb:permutation}
\end{table}

À l'aide du principe de multiplication, il y a $n\cdot(n-1)\cdots(n-k+1)$
combinaisons. On dénote le nombre de permutations sans remise de $n$ éléments
par
\begin{equation*}
	\perm{k}{n}=\frac{n!}{(n-k)!}.
\end{equation*}
Lorsqu'il y a remise, le nombre de permutations de $n$ éléments est $n^k$.

\begin{exemple}
	On dispose de 10 composantes dont 4 défectueuses. On pige 3 composantes au
	hasard et sans remise. Quelle est la probabilité d'obtenir uniquement des
	composantes non défectueuses?

	On suppose qu'il y a équiprobabilité du choix des 3 composantes. La
	probabilité d'obtenir 3 composantes non défecteuses $F$ est
	\begin{equation*}
		\P{F}
		=\frac{6\cdot 5\cdot 4}{10\cdot 9\cdot 8}
		=\frac{\perm{3}{6}}{\perm{3}{10}}
		=\frac{1}{6}.
	\end{equation*}
\end{exemple}

\subsection{Combinaisons}
\begin{definition}
	Une \textit{combinaison} correspond au choix de $k$ objets parmis $n$
	objets distincts. Le choix se fait sans remise et sans ordre spécifique.
\end{definition}

Soit $C_k^n$ le nombre de combinaisons. Le nombre de permutations est égal au
nombre de combinaisons multiplié par le nombre d'arrangement possible $k!$,
c'est-à-dire 
\begin{equation*}
	\perm{k}{n}=\comb{k}{n}\cdot k!,
\end{equation*}
et en réarrangeant, on obtient
\begin{equation*}
	\comb{k}{n}={{n}\choose{k}}=\frac{n!}{k!(n-k)!}.
\end{equation*}

\begin{exemple}
	Combien de codes de deux lettres peut-on former à partir du mot
	\texttt{OUI}?
	
	Avec ordre, il y a $\perm{2}{3}=6$ permutations et sans ordre, il y a
	$\comb{2}{3}=3$ combinaisons.
\end{exemple}

\begin{exemple}
	Quel est la probabilité de gagner le gros lot à la $6/49$?
	
	Sans ordre, il y a 1 seul cas favorable et $\comb{6}{49}$ cas possibles.
	Par conséquent, la probabilité de gagner $G$ est
	\begin{equation*}
		\P{G}=\frac{1}{\comb{6}{49}}=\frac{1}{\SI{13983816}{}}.
	\end{equation*}
	Avec ordre, il y a $6!$ cas favorables et $\perm{6}{49}$ cas possibles de
	sorte que la probabilité est
	\begin{equation*}
		\P{G}=\frac{6!}{\perm{6}{49}}=\frac{720}{\SI{10068347520}{}}.
	\end{equation*}
\end{exemple}

\subsubsection{Triangle de Pascal}
Une combinaison peut se représenter avec le triangle de Pascal comme le montre
la figure \ref{fig:pascal}.

\begin{figure}[H]
	\centering
	\input{figure/pascal}
	\caption{représentation du triangle de Pascal}
	\label{fig:pascal}
\end{figure}

\begin{theoreme}
	$\comb{k}{n}=\comb{k-1}{n-1}+\comb{k}{n-1}$
\end{theoreme}

\begin{proof}
	La démonstration est triviale et est laissée au lecteur.
\end{proof}

\subsubsection{Binôme de Newton}
La combinaison est souvent appliquée dans le cas de la puissance d'un binôme
tel que
\begin{equation*}
	\left(a+b\right)^n
	=\sum_{k=0}^n\comb{k}{n}\cdot a^k\cdot b^{n-k}
	=\sum_{k=0}^n{{k}\choose{n}}\cdot a^k\cdot b^{n-k},
\end{equation*}
mais elle est plus souvent utilisée avec la deuxième notation.

\subsection{Permutation d'objets semblables}
\begin{exemple}
	Combien y a-t-il d'ordres possibles des lettres <<ppfff>>?

	Soit 5 cases distinctes représentant l'ordre d'une pige dans les lettres.
	Il faut choisir les cases où mettre les <<p>>, soit
	\begin{equation*}
		\comb{2}{5}=\frac{5!}{2!3!}=10
	\end{equation*}
	ordres possibles.
	
	Si on échange un <<p>> et un <<f>> dans une séquence $A$, on obtient une
	séquence $B$ différente de $A$. Si on échange un <<p>> avec une autre <<p>>
	dans une séquence $A$, on retrouve la même séquence $A$.

	En analysant la formule, le facteur $5!$ représente le nombre d'ordres si
	toutes les lettres étaient différentes. Le facteur $2!$ représente les
	<<p>> s'ils étaient différents et le facteur $3!$ représente le nombres des
	<<f>> s'ils étaient différents.
\end{exemple}

En général, avec $n$ objets objets comprenant $n_1$ objets de classes $1$,
$n_2$ objets de classe $2$, $\dots$, $n_k$ objets à la classe $k$, on a
\begin{equation*}
	\frac{n!}{n_1!n_2!\cdots n_k!}
\end{equation*}
ordres possibles.

\begin{exemple}\label{ex:network}
	Soit le graphe $n\times n$ suivant.
	\begin{figure}[H]
		\vspace{-3mm}
		\centering
		\input{figure/network}
		\vspace{-3mm}
	\end{figure}

	Combien de chemins existe entre $A$ et $B$ suivant, s'il est seulement
	permis d'aller vers la droite ou vert le haut?
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	On suppose que tous les chemins possibles sont équiprobables. Peu importe
	le chemin, il faut avancer $n-1$ vers la droite et $n-1$ vers le haut pour
	un total de $2n-2$ mouvement.
	
	Il suffit de calculer le nombre de permutations de ces mouvements sachant
	qu'il y des objets semblables. Le nombre chemins est
	\begin{equation*}
		\frac{\left(2n-2\right)!}{\left(n-1\right)!\left(n-1\right)!},
	\end{equation*}
	ce qui est équivalent à $\comb{n-1}{2n-2}$.
\end{exemple}


\subsection{Équivalence}
\begin{exemple}\label{ex:network_broken}
	Soit le graphe $n\times n$ suivant.
	\begin{figure}[H]
		\vspace{-3mm}
		\centering
		\input{figure/network_broken}
		\vspace{-3mm}
	\end{figure}

	Quel est la probabilité qu'un chemin choisi au hasard, allant seulement
	vers la droite ou le haut, ne traverse pas les noeuds au-dessus de la
	diagonale entre $A$ et $B$?

	Les <<bons>> chemins ne passent pas par la ligne critique en pointillée et
	les <<mauvais>> chemins passent par la ligne critique. La probabilité d'un
	bon chemin $B$ peut s'écrire en fonction de son complément, soit
	\begin{equation*}
		\P{B}=1-\P{M}=1-\frac{n_m}{C_{n-1}^{2n-2}},
	\end{equation*}
	où $M$ est un mauvais chemin et $n_m$ est le nombre de mauvais chemins.

	Soit un mauvais chemin $M$. On définit le point $X$ comme étant le premier
	point au-dessus de la diagonale que $M$ atteint. On applique une
	transformation mirroir au chemin suivant $X$ comme la prochaine figure.

	\noindent
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/network_bad}
		\end{figure}
	\end{minipage}%
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/network_transformed}
		\end{figure}
	\end{minipage}
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Il s'avère que tous les mauvais chemins dans le graphe $n\times n$
	correspond à un chemin unique dans le graphe $(n-1)\times(n+1)$.
	C'est-à-dire la transformation est bijective.
	
	Par conséquent, le problème est équivalent à trouver le nombre de chemins
	dans un graphe $(n-1)\times(n+1)$. D'une manière similaire à l'exemple
	\ref{ex:network}, ce nombre de chemins est donné par
	\begin{equation*}
		\frac{(2n-2)!}{(n-2)!n!},
	\end{equation*}
	ce qui est équivalent à $\comb{2n-2}{n-2}$.

	Par conséquent, la probabilité d'avoir un bon chemin au hasard est
	\begin{equation*}
		\P{B}
		=1-\frac{\comb{n-2}{2n-2}}{\comb{n-1}{2n-2}}
		=\frac{1}{n}.
	\end{equation*}
\end{exemple}

\subsection{Réccurence}
\begin{exemple}
	Il est possible de résoudre le problème à l'exemple \ref{ex:network_broken}
	à l'aide de la réccursion.
	
	En effet, le nombre de <<bons>> chemins $b_{i,j}$ à partir d'un noeud est
	la somme des <<bons>> chemins des noeuds à droite et en haut, c'est-à-dire
	\begin{equation*}
		b_{i,j}=b_{i-1,j}+b_{i,j-1},
	\end{equation*}
	où $i$ est le nombre de noeuds restants vers la droite et $j$ la nombre de
	noeuds restants vers le haut.

	On sait que $b_{0,0}=1$, car il ne reste plus de noeud à parcourir fois
	rendu à $B$. Aussi, $b_{0,j}=1$, car il est seulement possible de se rendre
	à $B$ en allant vers le haut. Sur la diagonale, on a $b_{i,i}=b_{i-1,j}$,
	car on ne peut pas aller vers le haut. Par conséquent, on obtient le
	système d'équations à reccurence suivant,
	\begin{equation*}
		b_{i,j}=\left\{
			\begin{matrix}
				0,                 &\text{si}&i=0,j=0\\
				1,                 &\text{si}&i=0\\
				b_{i-1,j},         &\text{si}&i=j\\
				b_{i-1,j}+b_{i,j-1}&\text{sinon}
			\end{matrix}
		\right.
	\end{equation*}

	Pour une grille $4\times 4$, on peut calculer le nombre de <<bons>> chemins
	en développant la récurrence afin d'obtenir $b_{3,3}=5$ chemins.
\end{exemple}

\subsection{Probabilité conditionnelles}
\begin{definition}
	Une \textit{probabilité conditionnelle} $\Pg{A}{B}$ est la probabilité
	qu'un évènement $A$ se réalise, si $B$ s'est réalisé, soit
	\begin{equation*}
		\Pg{A}{B}=\frac{\P{A\cap B}}{\P{B}}.
	\end{equation*}
\end{definition}

\begin{property}
	$\P{A\cap B}=\Pg{A}{B}\cdot\P{B}$.
\end{property}

\begin{property}
	$\Pg{A}{B}=\Pg{B}{A}=0$, si $A\cap B=\emptyset$.
\end{property}

\begin{property}
	$\Pg{A}{B}\neq\Pg{B}{A}$, en général.
\end{property}

\begin{property}
	$\Pg{A}{S}=\P{A}$, $A\in S$.
\end{property}

\begin{remark}
	La dernière propriété résulte que toutes probabilités peut s'exprimer sous
	la forme d'une probabilité conditionnelle.
\end{remark}

\begin{exemple}
	Soit le lancement d'un dé avec les évènements $A=\left\{5,6\right\}$ et
	$B=\left\{2,4,6\right\}$, alors $\P{A}=\nicefrac{1}{3}$ et 
	\begin{equation*}
		\Pg{A}{B}=\frac{\P{\left\{6\right\}}}{\P{B}}=\frac{1}{3}.
	\end{equation*}

	Si $A=\left\{6\right\}$, alors $\P{A}=\nicefrac{1}{6}$ et
	\begin{equation*}
		\Pg{A}{B}=\frac{\P{\left\{6\right\}}}{\P{B}}=\frac{1}{3}.
	\end{equation*}
\end{exemple}

\begin{exemple}
	On pige sans remise 3 composantes non défecteuses parmis 10 composantes, donc 4 sont
	défecteuses. Soit $A_i$ le $i^\text{e}$ composante non défecteuses.
	\begin{equation*}
		\P{A_1\cap A_2\cap A_3}=\Pg{A_3}{A_1\cap A_2}\Pg{A_2}{A_1}\P{A_1}
		=\frac{4}{8}\cdot\frac{5}{9}\cdot\frac{6}{10}
	\end{equation*}
\end{exemple}

\subsection{Probabilités totales}
\begin{definition}
	Les évènements $B_1,B_2,\dots,B_n$ forment une \textit{partition} si les
	évènements $B_i\cap B_j=\emptyset$, $\forall i\neq j$, et
	$B_1\cup B_2\cup\cdots\cup B_n=S$.
\end{definition}

\begin{theoreme}
	Si $B_1,B_2,\dots,B_n\in S$ forment une partition de $S$, alors la
	probabilité d'un événement $A\in S$ est donnée par
	\begin{equation*}
		\P{A}=\sum_{i=1}^n\P{A\cap B_i}=\sum_{i=1}^n\Pg{A}{B_i}\P{B_i}.
	\end{equation*}
\end{theoreme}

\begin{exemple}
	Soit la partition $B_1$, $B_2$ et $B_3$ représenté dans l'arbre suivant.
	\begin{figure}[H]
		\centering
		\input{figure/partition_arbre}
	\end{figure}

	À partir de l'arbre, il facile de déterminer les probabilités
	conditionnelles de $A$ et $A^c$. Par exemple,
	$\Pg{A}{B_1}=\nicefrac{3}{6}\cdot\nicefrac{1}{10}=\nicefrac{1}{30}$ et
	$\Pg{A^c}{B_2}=\nicefrac{2}{6}\cdot\nicefrac{9}{10}=\nicefrac{3}{10}$.
\end{exemple}

\begin{theoreme}[Règle d'inversion]
	$\Pg{B}{A}=\dfrac{\Pg{A}{B}\P{B}}{\P{A}}$
\end{theoreme}

\begin{theoreme}[Règles de Bayes]
	$\Pg{B_j}{A}=\dfrac{\Pg{A}{B_j}\P{B_j}}{\sum_{i=1}^n\Pg{A}{B_i}\P{B_i}}$, où $B_i$ et
	$B_j$ sont des partitions.
\end{theoreme}

\begin{exemple}
	On dépiste le cancer du poumon dans une clinique. On sait que 
	\begin{itemize}
		\item\SI{25}{\percent} des individus sont fumeurs
		\item\SI{75}{\percent} des individus sont non fumeurs
		\item\SI{10}{\percent} des fumeurs développent un cancer
		\item\SI{1 }{\percent} des non fumeurs développent un cancer
	\end{itemize}

	On détecte un cancer des poumons chez un individu sélectionné au hasard
	pour dépistage. Quelle est la probabilité que ça soit un fumeur?

	Soit $B$ les fumeurs, $B^c$ les non fumeurs, $A$ la présence de cancer du
	poumon et $A^c$ son absence. Par conséquent, on cherche $\Pg{B}{A}$. Avec les
	données, on sait que $\Pg{A}{B}=\SI{0.10}{}$, $\Pg{A}{B^c}=\SI{0.01}{}$,
	$\P{B}=\SI{0.25}{}$ et $\P{B^c}=\SI{0.75}{}$, de sorte que le théorème de
	Bayes nous donne
	\begin{equation*}
		\Pg{B}{A}
		=\frac{\Pg{A}{B}\P{B}}{\Pg{A}{B}\P{B}+\Pg{A}{B^c}\P{B^c}}
		\approx\SI{0.769}{}.
	\end{equation*}
\end{exemple}

\subsection{Notion d'indépendance}
\begin{definition}
	On dit que les événements $A$ et $B$ sont \textit{indépendants} si la
	réalisation d'un n'affecte pas l'autre, soit
	\begin{equation*}
		\Pg{A}{B}=\P{A}\Leftrightarrow
		\Pg{B}{A}=\P{B}.
	\end{equation*}
\end{definition}

\begin{theoreme}
	Si $A$ et $B$ sont indépendants, alors $\P{A\cap B}=\P{A}\P{B}$.
\end{theoreme}

\begin{exemple}
	Soit un système en série ayant $n$ composantes comme à la figure suivante.
	\begin{figure}[H]
		\centering
		\input{figure/series}
	\end{figure}
	Un système en série fonction si et seulement si tous ses composantes
	fonctionnent. Quelle est la probabilité que le système fonctionne si chaque
	composante a une probabilité de \SI{75}{\percent} de fonctionner?

	Soit l'ensemble échantillion $S=\left\{F,D\right\}$ avec $F$ une composante
	qui fonctionne et $D$ une composante défectueuse. De plus, on définit
	$A_i=\left\{F\right\}\in S$ comme étant la composante $i$ qui fonctionne et
	$A_i^c=\left\{D\right\}\in S$ comme étant la composante $i$ qui est
	défecteuse.

	On suppose que les composantes fonctionnent et tombent en panne
	indépendamment les uns des autres. Le système fonctionne si
	\begin{equation*}
		A_S=\bigcap_{i=1}^nA_i,
	\end{equation*}
	de sorte que
	\begin{equation*}
		\P{A_S}
		=\P{\bigcap_{i=1}^nA_i}
		=\prod_{i=1}^n\P{A_i}
		=\SI{0.75}{}^n.
	\end{equation*}
\end{exemple}

\begin{exemple}
	On génère un point au hasard dans le carré $S=[0,10]\times[0,10]$. On
	définit $A$ comme ayant l'abscisse du point entre 2 et 4, et $B$ comme
	ayant l'ordonnée du point entre 3 et 6. Est-ce que $A$ et $B$ sont
	indépendants?
	\begin{figure}[H]
		\centering
		\input{figure/square}
		\vspace{-3mm}
	\end{figure}
	Puisqu'il y a équiprobabilité continue, alors les probabilités sont données
	par le rapport entre l'aire de $A$ ou $B$ sur $S$, alors
	$\P{A}=\nicefrac{2}{10}$ et $\P{B}=\nicefrac{3}{10}$. De plus,
	$\P{A\cap B}=\nicefrac{6}{10}$. Puisque $\P{A\cap B}=\P{A}\P{B}$, alors les
	évènements $A$ et $B$ sont indépendants.
\end{exemple}

\section{Variables Aléatoires}
\begin{definition}
	Une \textit{variable aléatoire} correspond à une expérience aléatoire dont
	les résultats sont quantitatifs.
\end{definition}

On peut décrire une variable aléatoire par sa
\begin{itemize}
	\item fonction de répartition
	\item fonction de masse (variable aléatoire discrète)
	\item fonction de densité (variable aléatoire continue)
\end{itemize}

\begin{exemple}
	Soit $X$ le nombre de défaults de soudure d'un transistor à 3 pattes. Quel
	est l'espace échantillion de $X$?

	L'espace échantillion est $S_X=\left\{0,1,2,3\right\}$, alors $X$ est une
	variable aléatoire discrète.
\end{exemple}

\begin{exemple}
	Soit $X$ un nombre réel choisie au hasard dans l'intervalle $[0,2]$. Quel
	est l'espace échantillion de $X$?

	L'espace échantillion est $S_X=[0,2]$, alors $X$ est une variable aléatoire
	continue.
\end{exemple}

\begin{exemple}
	Soit $X$ le temps d'attente d'un client au guichet automatique. Quel est
	l'espace d'échantillion de $X$?

	L'espace échantillion est $S_X={0}\cup]0,\infty[$. Il y a une partie
	discrète et continue, alors $X$ est une variable aléatoire mixte.
\end{exemple}

\subsection{Fonction de répartition}
\begin{definition}
	Un \textit{fonction de répartition} $F_X(x)$ est égal à la probabilité
	qu'une variable aléatoire $X$ soit plus petite qu'une valeur $x$,
	c'est-à-dire
	\begin{equation*}
		F_X(x)=\P{X\leq x},\quad\forall x\in\mathbb{R}.
	\end{equation*}
\end{definition}

\begin{property}
	$0\leq F_X(x)\leq 1$.
\end{property}

\begin{property}
	$\displaystyle\lim_{X\rightarrow-\infty}F_X(x)=0$.
\end{property}

\begin{property}
	$\displaystyle\lim_{X\rightarrow \infty}F_X(x)=1$.
\end{property}

\begin{property}[non décroissance]
	$x_0<x_1\Leftrightarrow F_X(x_0)<F_X(x_1)$.
\end{property}

\begin{property}[continuité à droite]
	$F_X(x^+)=F_X(x)$.
\end{property}

Il en résulte que toutes fonctions de répartition $F_X(x)$ sont croissantes,
mais peuvent contenir des discontinuités. Elles peuvent représenté des
variables aléatoires discrètes, continues ou mixtes. La figure
\ref{fig:maxwell} est une exemple de la fonction de répartition d'une
distribution de Maxwell-Boltzmann pour certains paramètres différents.

\begin{figure}[H]
	\centering
	\caption{$F_X(x)$ de certaines distributions de Maxwell-Boltzmann}
	\input{figure/maxwell_boltzmann}
	\label{fig:maxwell}
\end{figure}

\subsection{Fonction de masse}
\begin{definition}
	Une \textit{fonction de masse} $p_X(x_k)$ est égal à la probabilité qu'une
	variable aléatoire discrète $X$ soit prenne une valeur $x_k\in S_X$,
	c'est-à-dire 
	\begin{equation*}
		p_X(x_k)=\P{X=x_k},
	\end{equation*}
	où $S_X=\left\{x_1,x_2,\dots,x_n\middle\rvert x_1<x_2<\cdots<x_n\right\}$.
\end{definition}

\begin{property}
	$0\leq p_X(x_k)\leq 1$, si $x_k\in S_X$.
\end{property}

\begin{property}
	$p_X(x_k)=0$, si $x_k\notin S_X$.
\end{property}

\begin{property}
	$\displaystyle\sum_{a< x_k\leq b}p_X(x_k)=\P{a<X\leq b}$.
\end{property}\vspace{-6mm}

\begin{property}
	$\displaystyle\sum_{k=1}^\infty p_X(x_k)=1$.
\end{property}

\begin{remark}
	Une fonction de masse est nulle en tout point sauf aux valeurs discrètes
	possibles. De plus, la somme de toutes les valeurs discrètes donne 1. La
	figure \ref{fig:fonction_masse} montre un exemple d'une fonction de masse.
\end{remark}

\begin{figure}[H]
	\centering
	\caption{exemple d'une fonction de masse}
	\input{figure/fonction_masse}
	\label{fig:fonction_masse}
\end{figure}

\subsection{Fonction de densité}
\begin{definition}
	Une \textit{fonction de densité} $f_X(x)$ est égal à la probabilité de
	qu'une variable aléatoire $X$ soit autour d'une valeur $x$, c'est-à-dire
	\begin{equation*}
		f_X(x)=\lim_{\epsilon\rightarrow 0}\frac{1}{\epsilon}
		\P{x-\frac{\epsilon}{2}\leq X\leq x+\frac{\epsilon}{2}},
	\end{equation*}
	où $\epsilon$ est positif.
\end{definition}

\begin{property}
	$f_X(x)\geq 0$.
\end{property}

\begin{property}
	$\displaystyle\int_{-\infty}^\infty f_X(x)\d{x}=1$.
\end{property}

\begin{remark}
	La fonction de densité n'est pas une probabilité à cause de sa définition
	infinitisémale. Par conséquent, l'inégalité  $0\leq f_X(x)\leq 1$ n'est
	pas valide.
\end{remark}

\subsection{Règles de calcul fondamentale}
\begin{theoreme}\label{th:calc_fond}
	$\P{a<X\leq b}=F_X(b)-F_X(a)$
\end{theoreme}

\begin{proof}
	Soit les ensembles $A=\left\{X\leq a\right\}$, $B=\left\{X\leq b\right\}$
	et $C=\left\{a< X\leq b\right\}$, avec $a<b$, comme à la figure
	\ref{fig:droite_num}.
	\begin{figure}[H]
		\centering
		\input{figure/droite_numerique}
		\caption{représentation sur une droite numérique}
		\label{fig:droite_num}
	\end{figure}
	
	On sait que $A\cap C=\emptyset$ et $A\cup C=B$. Par conséquent, 
	$\P{B}=\P{A}+\P{C}\Leftrightarrow\P{C}=\P{B}-\P{A}=F_X(b)-F_X(a)$.
\end{proof}

\begin{theoreme}
	$\P{X=x}=F_X(x)-F_X(x^-)$.
\end{theoreme}

\begin{proof}
	Soit $a=x-\epsilon$ et $b=x$, où $\epsilon$ est positif. Selon le théorème
	\ref{th:calc_fond}, on a
	\begin{equation*}
		\P{x-\epsilon<X\leq x}=F_X(x)-F_X(x-\epsilon).
	\end{equation*}
	En prenant la limite lorsque $\epsilon\rightarrow 0$, on obtient
	\begin{equation*}
		\lim_{\epsilon\rightarrow 0}\P{x-\epsilon<X\leq x}
		=F_X(x)-\lim_{\epsilon\rightarrow 0}F_X(x-\epsilon).
	\end{equation*}
	Avec $F_X(x^-)=\lim_{\epsilon\rightarrow 0}F_X(x-\epsilon)$ et
	$(x-\epsilon<X\leq x)\equiv (X=x)$ lorsque $\epsilon\rightarrow 0$, on
	obtient
	\begin{equation*}
		\P{X=x}=F_X(x)-F_X(x^-).\qedhere
	\end{equation*}
\end{proof}

\subsection{Liens entre les différentes fonctions}
Toutes variables aléatoires peuvent être décrites par une fonction de
répartition. Lorsque la variable aléatoire est continue, alors elle peut aussi
être décrite pas une fonction de densité de probabilité. Si la variable
aléatoire est discrète, alors elle peut être décrite pas une fonction de masse.

\begin{theoreme}
	$f_X(x)=\dfrac{\d{}}{\d{x}}F_X(x)$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\displaystyle\int_{-\infty}^xf_X(t)\d{t}$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}\label{th:masse_repartition}
	$F_X(x)=\displaystyle\sum_{x_k\leq x}P_X(x_k)$ si $X$ est discrète.
\end{theoreme}

\begin{theoreme}
	$p_X(x_k)=\left\{
		\begin{matrix}
			F_X(x_1)              & \text{si} & k=1,\\
			F_X(x_k)-F_X(x_{k-1}) & \text{si} & k\neq 1.\\
		\end{matrix}
	\right.$
\end{theoreme}

\begin{exemple}
	Soit $X$ un nombre réel choisi au hasard dans l'intervalle $[0,2]$. Quelle
	est la fonction de répartition de $X$?

	On sait que $F_X(x)=\P{X\leq x}=\nicefrac{x}{2}$ lorsque $0\leq x\leq 2$,
	$F_X(x)=0$ si $x<0$ et $F_X(x)=1$ si $x>2$. On obtient donc la fonction de
	répartition
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				0            & \text{si} & x<0,\\
				\dfrac{x}{2} & \text{si} & 0\leq x\leq 2,\\
				1            & \text{si} & 2\leq x.\\
			\end{matrix}
		\right.
	\end{equation*}
\end{exemple}

\pagebreak
\begin{exemple}
	Soit $X$ le temps d'attente d'un client au guichet automatique. On suppose
	que \SI{10}{\percent} des visites sont sans attente. Quel est la fonction
	de répartition?
	
	La variable aléatoire est mixte. On sait que $F_X(0)=\nicefrac{1}{10}$ et
	que $F_X(x)=0$ si $x<0$. La fonction de répartition est
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				0                 & \text{si} & x<0,\\
				\dfrac{x+1}{x+10} & \text{si} & x\geq 0.\\
			\end{matrix}
		\right.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit $X$ le nombre de défaults de soudure d'un transistor à 3 pattes. La
	fonction de masse de $X$ est donnée par
	\begin{equation*}
		p_X(k)=\left\{
			\begin{matrix}
				0.7 & \text{si} & k=0,\\
				0.1 & \text{si} & k=1,\\
				0.1 & \text{si} & k=2,\\
				0.1 & \text{si} & k=3.\\
			\end{matrix}
		\right.
	\end{equation*}
	Quelle est la fonction de répartition de $P_X(k)$?

	On applique le théorème \ref{th:masse_repartition} pour obtenir la
	fonction de répartition. Lorsque $x<0$, alors $F_X(x)=0$. Lorsque
	$0\leq x<1$, alors $F_X(x)=\SI{0.7}{}$. Lorsque $1\leq x<2$, alors
	$F_X(x)=\SI{0.8}{}$. En continuant, on obtient
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				0.0 & \text{si} & k<0,\\
				0.7 & \text{si} & 0\leq k<1,\\
				0.8 & \text{si} & 1\leq k<2,\\
				0.9 & \text{si} & 2\leq k<3,\\
				1.0 & \text{si} & 3\leq k.\\
			\end{matrix}
		\right.
	\end{equation*}

	Les fonctions de masse et de répartition sont montrées dans les figures
	suivantes.

	\noindent
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/transistor_PX}
		\end{figure}
	\end{minipage}%
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/transistor_FX}
		\end{figure}
	\end{minipage}
\end{exemple}

\subsection{Fonction conditionnelle}
\begin{definition}
	Une \textit{fonction de répartition conditionnelle} $F_X(x|A)$ est égal à 
	la probabilité qu'une variable aléatoire $X$ soit plus petite ou égal à une
	valeur $x$ sachant un certain événement $A$, c'est-à-dire
	\begin{equation*}
		F_X(x|A)=\dfrac{\P{\left\{X\leq x\right\}\cap A}}{\P{A}}.
	\end{equation*}
\end{definition}

\begin{definition}
	Une \textit{fonction de masse conditionnelle} $p_X(x_k|A)$ est égal à la
	probabilité qu'une variable aléatoire discrète prenne une valeur $x_k\in
	S_X$ sachant un certain événement $A$, c'est-à-dire
	\begin{equation*}
		p_X(x_k|A)=\frac{\P{\left\{X=x\right\}\cap A}}{\P{A}},
	\end{equation*}
	où $S_X=\left\{x_1,x_2,\dots,x_n\middle\rvert x_1<x_2<\cdots<x_n\right\}$.
\end{definition}

\begin{definition}
	Une \textit{fonction de densité conditionnelle} $f_X(x|A)$ est égal à la
	probabilité qu'une variable aléatoire $X$ soit autour d'une valeur $x$
	sachant un certain événement $A$, c'est-à-dire
	\begin{equation*}
		f_X(x|A)
		=\lim_{\epsilon\rightarrow 0}\frac{1}{\epsilon}\P{
			x-\frac{\epsilon}{2}\leq X\leq x+\frac{\epsilon}{x}},
	\end{equation*}
	où $\epsilon$ est positif.
\end{definition}

\begin{theoreme}
	$f_X(x|A)=\dfrac{\d{}}{\d{x}}F_X(x|A)$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}
	$p_X(x_k|A)=
	\left\{
		\begin{matrix}
			\dfrac{p_X(x_k)}{\P{A}} & \text{si} & x_k\in A,\\
			0                       & \text{si} & x_k\notin A,\\
		\end{matrix}
	\right.$
	si $X$ est discrète.
\end{theoreme}

\subsection{Médiane et quantile}
\begin{definition}
	La médiane d'une variable aléatoire $X$ continue est le nombre réel
	$x_{1/2}$ qui permet de couper l'ensemble des valeurs en deux parties
	égales, c'est-à-dire
	\begin{equation*}
		F_X(x_{1/2})=\nicefrac{1}{2}.
	\end{equation*}
\end{definition}

\begin{definition}
	Le \textit{quantile} d'ordre $p$ d'une variable aléatoire $X$ continue est
	le nombre réel $x_p$ tel que
	\begin{equation*}
		F_X(x_p)=p.
	\end{equation*}
\end{definition}

\begin{remark}
	La médiane est le quantile d'ordre $\nicefrac{1}{2}$.
\end{remark}

\begin{exemple}
	Dans une certaine population, la taille $X$ d'un adulte choisi au hasard
	possède la fonction de répartition 
	\begin{equation*}
		F_X(x)=\left\{
			\begin{matrix}
				0        & \text{si} & x<1.2,\\
				1.5x-1.8 & \text{si} & 1.2\leq x<1.7,\\
				0.5x-0.1 & \text{si} & 1.7\leq x<2.2,\\
				1        & \text{si} & 2.2\leq x.\\
			\end{matrix}
		\right.
	\end{equation*}
	Calculer la médiane et le quantile d'ordre 95.

	Puisque $F_X(1.7)=0.75$, alors le quantile d'ordre 95 est dans la tranche
	$1.7\leq x<2.2$. Il suffit de résoudre $0.95=0.5x_{0.95}-0.1$ et on obtient
	que $x_{0.95}=2.1$.
\end{exemple}

\subsection{Lois de probabilités discrètes}
\subsubsection{Loi de Bernoulli}
\begin{definition}
	Une variable aléatoire $X$ suivant une \textit{loi de Bernoulli}, dénotée
	\begin{equation*}
		X\sim\Bin{1}{p},
	\end{equation*}
	est le résultat d'une expérience aléatoire pouvant être soit un <<échec>>,
	dénotée $X=0$, ou un <<succès>>, dénotée $X=1$. La probabilité du succès
	est donnée par $p$.
\end{definition}

\begin{property}
	$S_X=\left\{0,1\right\}$.
\end{property}

\begin{property}
	$p_X(k)=\left\{
		\begin{matrix}
			q & \text{si} & k=0,\\
			p & \text{si} & k=1,\\
			0 & \text{sinon}.\\
		\end{matrix}
	\right.$
\end{property}

\subsubsection{Loi binomiale}
\begin{definition}
	Une variable aléatoire $X$ suivant une\textit{loi binomiale}, dénotée
	\begin{equation*}
		X\sim\Bin{n}{p},
	\end{equation*}
	est le nombre de succès obtenu de $n$ expériences de Bernoulli
	indépendantes où la probabilité d'un succès individuel est donnée par $p$.
\end{definition}

\begin{property}
	$S_X=\left\{0,1,\dots,n\right\}$.
\end{property}

\begin{property}
	$p_X(k)=\left\{
		\begin{matrix}
			\comb{k}{n}p^kq^{n-k} & \text{si} & k=0,1,\dots,n,\\
			0                     & \text{sinon}.\\
		\end{matrix}
	\right.$
\end{property}

\begin{proof}
	Soit $A_i$ le $i$-ème succès et $A_i^c$ le $i$-ème échec dans une
	expérience aléatoire à $n$ essai. La probabilité d'obtenir $k$ succès est
	donnée par
	\begin{equation*}
		\P{
			\underbrace{A_1\cap A_2\cap\cdots\cap A_k}_\text{$k$ fois}
			\cap
			\underbrace{A_{k+1}^c\cap A_{k+1}^c\cap\cdots\cap A_n^c}_
			\text{$n-k$ fois}
		},
	\end{equation*}
	où $k=0,1,2,3,\dots,n$.

	Hors, la probabilité d'un succès est $\P{A_i}=p$ et celle d'un échec est
	$\P{A_i^c}=q$, où $q=1-p$. Puisque chaque essai est indépendant des autres,
	la probabilité peut s'écrire
	\begin{equation*}
		\underbrace{
			\P{A_1}\P{A_2}\cdots P(A_k)
		}_\text{$k$ fois}
		\cdot
		\underbrace{
			\P{A_{k+1}^c}\P{A_{k+2}^c}\cdots P(A_n^c)
		}_\text{$n-k$ fois}
		=p^kq^{n-k}.
	\end{equation*}

	De plus, les succès peuvent être à n'importe quel essai. Par conséquent,
	il faut choisir $k$ essaies parmis les $n$ essaies de sorte que la
	probabilité d'obtenir $k$ succès en tout est
	$p_X(k)=\P{X=k}=\comb{k}{n}p^kq^{n-k}$.
\end{proof}

\begin{proposition}
	$\P{S_X}=1$.
\end{proposition}

\begin{proof}
	Par définition, on a
	\begin{equation*}
		\P{S_X}=\sum_{k=0}^n\comb{k}{n}p^kq^{n-k}
	\end{equation*}
	Selon le théorème binomial, soit
	\begin{equation*}
		(a+b)^n=\sum_{k=0}^n\comb{k}{n}p^kq^{n-k},
	\end{equation*}
	et que $q=1-p$, on peut simplifier de sorte à obtenir
	\begin{equation*}
		\P{S_X}=(p+q)^k=1^k=1.\qedhere
	\end{equation*}
\end{proof}

\begin{exemple}
	Vous achetez un billet de 6/49 à chaque semaine depuis vos 18 ans. Quelle
	est la probabilité de gagner le gros lot au plus tard à votre $98^e$
	anniversaire?
	
	Ce problème se décrit à l'aide d'une loi binomiale, soit
	\begin{equation*}
		X\sim\Bin{n}{\frac{1}{\SI{13983816}{}}},
	\end{equation*}
	où $n$ est le nombre d'essaies et $X$ le nombre de gros lots gagnés.
	\vspace{-1mm}

	Puisqu'on joue chaque semaine de 18 ans à 98 ans, un total de
	$n=52\cdot(98-18)=4160$ billets sont achetés. La probabilité d'obtenir que
	des échecs est données par
	\begin{equation*}
		\left(1-\frac{1}{\SI{13983816}{}}\right)^{4160}=
		\left(\frac{\SI{13983815}{}}{\SI{13983816}{}}\right)^{4160},
	\end{equation*}
	de sorte que la probabilité d'obtenir au moins 1 billet gagnant est donnée
	par le complément, soit
	\begin{equation*}
		\P{X\geq 1}
		=1-\left(\frac{\SI{13983815}{}}{\SI{13983816}{}}\right)^{4160}
		\approx\SI{0.000297}{}.
	\end{equation*}
\end{exemple}

\subsubsection{Loi géométrique}
\begin{definition}
	Une variable aléatoire $X$ suivant une \textit{loi géométrique}, dénotée
	\begin{equation*}
		X\sim\Geo{p},
	\end{equation*}
	est le nombre d'essais nécessaires avant d'obtenir un succès. La
	probabilité d'un succès est donnée par $p$.
\end{definition}

\begin{property}
	$S_X=\left\{1,2,\dots\right\}$.
\end{property}

\begin{property}
	$p_X(k)=\left\{
		\begin{matrix}
			q^{k-1}p & \text{si} & k=1,2,\dots,\\
			0        & \text{si} & k=0.\\
		\end{matrix}
	\right.$
\end{property}

\begin{proof}
	Soit $A_i$ le $i$-ième succès et $A_i^c$ le $i$-ème échec dans un
	expérience aléatoire où il faut $k$ essai pour obtenir un succès. La
	probabilité d'obtenir le succès au $k$-ième essai est donnée par
	\begin{equation*}
		\P{
			\underbrace{A_1^c\cap A_2^c\cap\cdots\cap A_{k-1}^c}_
			\text{$k-1$ fois}\cap A_k
		},
	\end{equation*}
	où $k=1,2,\dots$

	Hors, la probabilité d'un succès est $\P{A_i}=p$ et celle d'un échec
	est $\P{A_i^c}=q$, où $q=1-p$. Puisque chaque essai est indépendant des
	autres, la probabilité peut s'écrire
	\begin{equation*}
		p_X(k)=
		\underbrace{\P{A_1^c}\P{A_2^c}\cdots\P{A_{k-1}}}_\text{$k-1$ fois}
		\cdot\P{A_k}=q^{k-1}p.\qedhere
	\end{equation*}
\end{proof}

\begin{property}\label{pr:repartition_geo}
	$F_X(n)=1-q^n$ où $n=1,2,\dots$.
\end{property}

\begin{proof}
	Selon une variante du théorème \ref{th:masse_repartition}, on a
	\begin{equation*}
		F_X(n)=\sum_{k=1}^np_X(k)=\sum_{k=1}^nq^{k-1}p=p\sum_{k=1}^nq^{k-1}.
	\end{equation*}
	On remarque que la somme est une série géométrique, alors
	\begin{equation*}
		\sum_{k=1}^nq^{k-1}=\frac{1-q^n}{1-q}.
	\end{equation*}
	Puisque $q=1-p\Leftrightarrow p=1-q$, on obtient
	\begin{equation*}
		F_X(n)=(1-q)\frac{1-q^n}{1-q}=1-q^n.\qedhere
	\end{equation*}
\end{proof}

\begin{property}[absence de mémoire]
	$\Pg{X>k+j}{X>j}=\P{X>k}$ où $k,j\in\mathbb{Z}$.
\end{property}

\begin{proof}
	On sait que
	\begin{equation*}
		\Pg{X>k+j}{X>j}=\frac{\P{
			\left\{X>k+j\right\}\cap
	   		\left\{X>j\right\}}}{\P{X>j}}.
	\end{equation*}
	Puisque $k\in\mathbb{Z}$, alors $\left\{X>k+j\right\}\cap\left\{X>j\right\}
	=\left\{X>k+1\right\}$ de sorte que
	\begin{equation*}
		\Pg{X>k+j}{X>j}=\frac{\P{X>k+j}}{\P{X>j}}.
	\end{equation*}
	Avec le complément de la propriété \ref{pr:repartition_geo}, on a
	\begin{equation*}
		\Pg{X>k+j}{X>j}=\frac{q^{k+j}}{q^{j}}=q^{k}=\P{X>k}.\qedhere
	\end{equation*}
\end{proof}

\begin{proposition}
	$\P{S_X}=1$.
\end{proposition}

\begin{proof}
	En utilisant à nouveau la série géométrique, on a
	\begin{equation*}
		\P{S_X}
		=\sum_{k=1}^\infty p_X(k)
		=p\sum_{k=1}^\infty q^{k-1}
		=p\frac{1}{1-q}
		=(1-q)\frac{1}{1-q}
		=1,
	\end{equation*}
	puisque $q=1-p\Leftrightarrow p=1-q$.
\end{proof}

\subsubsection{Loi de Poisson}
\begin{definition}
	Une variable aléatoire suivant une \textit{loi de Poisson} de paramètre
	$\alpha$, dénotée
	\begin{equation*}
		X\sim\Poi{\alpha},
	\end{equation*}
	est équivalente à suivre une loi Binomiale telle que
	\begin{equation*}
		X\sim\lim_{n\rightarrow\infty}\Bin{n}{\frac{\alpha}{n}},
	\end{equation*}
	où $\alpha$ est un nombre positif.
\end{definition}

\begin{property}
	$S_X=\left\{0,1,\dots\right\}$.
\end{property}

\begin{property}
	$p_X(k)=\left\{
		\begin{matrix}
			\dfrac{\alpha^k}{k!}\e^{-\alpha} & \text{si} & k=0,1,\dots,\\
			0                                & \text{sinon}.\\
		\end{matrix}
	\right.$
\end{property}

\begin{proposition}
	$\P{S_X}=1$.
\end{proposition}

\begin{proof}
	En utilisant le développement en série de la fonction exponentielle, on a
	\begin{equation*}
		\P{S_X}
		=\sum_{k=0}^\infty p_X(k)
		=\e^{-\alpha}\sum_{k=0}^\infty\frac{\alpha^k}{k!}
		=\e^{-\alpha}\e^\alpha
		=1.\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Approximation par une loi de Poisson}
\begin{theoreme}
	Soit une variable aléatoire $X\sim\Bin{n}{p}$. Si $p$ est près de 0, alors
	\begin{equation*}
		X\approx\Poi{np}.
	\end{equation*}
\end{theoreme}

% TODO: approximation lorsque $p$ est près de 1

\begin{remark}
	En général, l'approximation est bonne si $n\geq 30$ et $p\leq 0.05$.
\end{remark}

\subsection{Loi des probabilités continues}
\subsubsection{Loi uniforme continue}
\begin{definition}
	Une variable aléatoire $X$ suivant une \text{loi uniforme continue},
	dénotée
	\begin{equation*}
		X\sim\Uni{a}{b},
	\end{equation*}
	est un nombre réel choisie avec équiprobabilité dans l'intervalle $[a,b]$. 
\end{definition}

\begin{property}
	$S_X=[a,b]$.
\end{property}

\begin{property}
	$f_X(x)=\left\{
		\begin{matrix}
			\dfrac{1}{b-a} & \text{si} & a\leq x\leq b,\\
			0              & \text{sinon}.\\
		\end{matrix}
	\right.$
\end{property}

\begin{property}
	$F_X(x)=\left\{
		\begin{matrix}
			0                & \text{si} & x<a,\\
			\dfrac{x-a}{b-a} & \text{si} & a\leq x\leq b,\\
			1                & \text{si} & b<x.\\
		\end{matrix}
	\right.$
\end{property}

\subsubsection{Loi exponentielle}
\begin{definition}
	Une variable aléatoire $X$ suivant une \textit{loi exponentielle} de
	paramètre $\lambda>0$, dénotée
	\begin{equation*}
		X\sim\Exp{\lambda},
	\end{equation*}
	décrit le temps entre les événements d'un \textit{processus de Poisson}.
\end{definition}

\begin{property}
	$S_X=[0,\infty[$.
\end{property}

\begin{property}
	$f_X(x)=\left\{
		\begin{matrix}
			\lambda\e^{-\lambda x} & \text{si} & x\geq 0,\\
			0                      & \text{si} & x<0.\\
		\end{matrix}
	\right.$
\end{property}

\begin{property}
	$F_X(x)=\left\{
		\begin{matrix}
			1-\e^{-\lambda x} & \text{si} & x\geq 0,\\
			0                 & \text{si} & x<0.\\
		\end{matrix}
	\right.$
\end{property}

\begin{property}[absence de mémoire]
	$\Pg{X>s+t}{X>t}=\P{X>t}$ où $s,t\in[0,\infty[$.
\end{property}

\subsubsection{Loi gamma}
\begin{definition}
	Une variable aléatoire $X$ suivant une \textit{loi gamma}, dénotée
	\begin{equation*}
		X\sim\Gam{\alpha}{\lambda},
	\end{equation*}
	où $\alpha>0$ et $\lambda>0$, est continue et positive.
\end{definition}

\begin{property}
	$S_X=[0,\infty[$.
\end{property}

\begin{property}
	$f_X(x)=\left\{
		\begin{matrix}
			\dfrac{(\lambda x)^{\alpha-1}\lambda\e^{-\lambda x}}
				{\Gamma(\alpha)} & \text{si} & x\geq 0,\\
			0                    & \text{si} & x<0.\\
		\end{matrix}
	\right.$
\end{property}

\begin{property}
	$F_X(x)=1-\displaystyle\sum_{k=0}^{n-1}\frac{(\lambda x)^k\e^{-\lambda x}}
		{(n-1)!}$ si $\alpha=n=1,2,\dots$.
\end{property}

\begin{proposition}
	$\P{S_X}=1$.
\end{proposition}

\begin{proof}
	Il suffit d'intégrer la fonction de densité de probabilité, soit
	\begin{equation*}
		\int_{-\infty}^\infty f_X(x)\d{x}
		=\int_0^\infty\frac{(\lambda x)^{\alpha-1}\lambda\e^{-\lambda x}}
			{\Gamma(\alpha)}\d{x}
		=\frac{1}{\Gamma(\alpha)}\int_0^\infty(\lambda x)^{\alpha-1}
			\lambda\e^{-\lambda x}\d{x},
	\end{equation*}
	et en posant $y=\lambda x$, on obtient
	\begin{equation*}
		\frac{1}{\Gamma(\alpha)}\int_0^\infty y^{\alpha-1}\e^{-y}\d{y}
		=\frac{1}{\Gamma(\alpha)}\Gamma{(\alpha)}=1.\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Loi normale}
\begin{definition}
	Une variable aléatoire suivant une \textit{loi normale} $X$ de moyenne
	$\mu$ et de variance $\sigma^2$, dénotée
	\begin{equation*}
		X\sim\Norm{\mu}{\sigma^2},
	\end{equation*}
	suit une distribution décrite par une fonction gaussienne.
\end{definition}

\begin{property}
	$S_X=\mathbb{R}$.
\end{property}

\begin{property}
	$f_X(x)=\dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{
		-\dfrac{1}{2\sigma^2}(x-\mu)^2
	\right\}$.
\end{property}

\begin{property}
	$F_X(x)=\dfrac{1}{\sqrt{2\pi\sigma^2}}\displaystyle\int_{-\infty}^x
		\exp\left\{-\dfrac{1}{2\sigma^2}(x-\mu)^2\right\}\d{x}$.
\end{property}


\subsubsection{Loi normale centrée réduite}
\begin{definition}
	Une variable aléatoire $X$ suivant une \textit{loi normale centrée
	réduite}, est une variable aléatoire suivant une loi normale de moyenne
	$\mu=0$ et variance $\sigma^2=1$, c'est-à-dire
	\begin{equation*}
		X\sim\Norm{0}{1}.
	\end{equation*}
\end{definition}

\begin{property}
	$S_X=\mathbb{R}$.
\end{property}

\begin{property}
	$f_X(x)=\dfrac{1}{\sqrt{2\pi}}\exp\left\{-\dfrac{1}{2}x^2\right\}$.
\end{property}

\begin{property}
	$F_X(x)=\dfrac{1}{\sqrt{2\pi}}\displaystyle\int_{-\infty}^x
		\exp\left\{-\frac{1}{2}x^2\right\}\d{x}$.
\end{property}

\begin{proposition}
	$\P{S_X}=1$.
\end{proposition}

\begin{proof}
	Par définition, on a
	\begin{equation*}
		I
		=\P{S_X}
		=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty\exp
			\left\{-\frac{1}{2}x^2\right\}\d{x},
	\end{equation*}
	donc
	\begin{equation*}
		I^2
		=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty\exp
			\left\{-\frac{1}{2}x^2\right\}\d{x}\cdot
			\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty\exp
			\left\{-\frac{1}{2}y^2\right\}\d{y}
	\end{equation*}

	Puisque $x$ et $y$ sont des variables différentes, on a
	\begin{equation*}
		I^2
		=\frac{1}{2\pi}\int_{-\infty}^\infty\int_{-\infty}^\infty
			\exp\left\{-\frac{1}{2}\left(x^2+y^2\right)\right\}\d{x}\d{y},
	\end{equation*}
	et coordonnées polaires, 
	\begin{equation*}
		I^2
		=\frac{1}{2\pi}\int_0^{2\pi}\int_0^\infty
			\exp\left\{-\frac{1}{2}r^2\right\}r\d{r}\d{\theta},
	\end{equation*}
	de sorte qu'en posant posant $u=r^2$, on peut obtenir
	\begin{equation*}
		I^2
		=\frac{1}{2\pi}\int_0^{2\pi}
			\exp\left\{-\frac{1}{2}\e^{-u}\right\}\bigg\rvert_0^\infty
			\d{\theta}
		=\frac{1}{2\pi}\int_0^{2\pi}\d{\theta}
		=1
	\end{equation*}
	
	Par conséquent, on a $I=1$.
\end{proof}

\begin{theoreme}
	Si $Y=\dfrac{X-\mu}{\sigma^2}$, où $X\sim\Norm{\mu}{\sigma^2}$, alors
	$Y\sim\Norm{0}{1}$.
\end{theoreme}

\subsection{Fonction d'une variable aléatoire}
\begin{definition}
	Une fonction d'une variable aléatoire $X$ est une transformation $g(X)$ sur
	toutes les valeurs de $X$.
\end{definition}

\begin{exemple}
	Soit $X$ est la valeur d'ampliture d'un signal an temps $t$. Le signal
	numérisé $Y$ peut s'écrire\vspace{-2mm}
	\begin{equation*}
		Y
		=\underbrace{\mathrm{signe}(X)\cdot\Delta\cdot\mathrm{part}\left(
			\frac{|X|}{\Delta}+\frac{1}{2}
		\right)}_{g(X)},
	\end{equation*}
	où $\Delta$ est le pas de quantification.
\end{exemple}

\subsubsection
	[$X$ et $Y$ sont des variables aléatoires discrètes]
	{\boldmath $X$ et $Y$ sont des variables aléatoires discrètes}
\begin{exemple}
	Soit $X\sim\Bin{2}{\nicefrac{1}{4}}$ et $Y=g(X)$, où $g(x)=(x-1)^2$. Quelle
	est la fonction de masse de $Y$?

	L'espace échantillion de $X$ est
	\begin{equation*}
		S_X=\left\{0,1,2\right\}
	\end{equation*}
	de sorte qu'en appliquant $g(x)$ sur tout $x\in S_X$, on obtient
	\begin{equation*}
		S_Y=\left\{1,0,1\right\}.
	\end{equation*}

	À la lumière de ce résultat, on peut facilement définir la fonction de
	masse de $Y$ en partie, soit
	\begin{equation*}
		f_Y(k)=\left\{
			\begin{matrix}
				f_X(1)        & \text{si} & k=0,\\
				f_X(0)+f_X(2) & \text{si} & k=1.\\
			\end{matrix}
		\right.
	\end{equation*}
\end{exemple}

\pagebreak
\subsubsection
	[$X$ et $Y$ sont des variables aléatoires mixtes]
	{\boldmath $X$ et $Y$ sont des variables aléatoires mixtes}
\begin{exemple}
	Soit $X\sim\Norm{0}{1}$ et
	\begin{equation*}
		Y=\left\{
			\begin{matrix}
				-1 &\text{si} & X<-\nicefrac{1}{2},\\
				 0 &\text{si} & -\nicefrac{1}{2}\leq X\leq\nicefrac{1}{2},\\
				 1 &\text{si} & \nicefrac{1}{2}X.\\
			\end{matrix}
		\right.
	\end{equation*}
	Quelle est la fonction de masse de $Y$?

	Il suffit de calculer les probabilités des conditions de la fonction par
	partie, c'est-à-dire
	\begin{equation*}
		p_X(k)=\left\{
	 		\begin{matrix}
				F_X(-\nicefrac{1}{2})                      & \text{si} &k=-1,\\
				F_X(\nicefrac{1}{2})-F_X(-\nicefrac{1}{2}) & \text{si} &k=0,\\
				1-F_X(\nicefrac{1}{2})                     & \text{si} &k=1,\\
				0                                          & \text{sinon}.\\
			\end{matrix}
		\right.
	\end{equation*}
\end{exemple}

\subsubsection
	[$X$ et $Y$ sont des variables aléatoires continues]
	{\boldmath $X$ et $Y$ sont des variables aléatoires continues}
\begin{exemple}
	Soit $X\sim\Uni{-1}{2}$ et $Y=X^2$. Quelle est la fonction de répartition de
	$Y$?

	Par définition, on a
	\begin{equation*}
		F_Y(Y)
		=\P{Y\leq y}
		=\P{X^2\leq y}
		=\P{-\sqrt{y}\leq X\leq\sqrt{y}}
		=F_X(\sqrt{y})-F_X(-\sqrt{y}),
	\end{equation*}
	où $0\leq \sqrt{y}\leq 2$. Puisque l'uniforme n'est pas symétrique, on a
	\begin{equation*}
		F_Y(y)=\left\{
			\begin{matrix}
				0                            & \text{si} & y<0,\\
				F_X(\sqrt{y})-F_X(-\sqrt{y}) & \text{si} & 0\leq y\leq 1,\\
				F_X(\sqrt{y})-F_X(1)         & \text{si} & 1\leq y\leq 4,\\
				1                            & \text{si} & 4<y.\\
			\end{matrix}
		\right.
	\end{equation*}
\end{exemple}

\subsection{Espérance mathématique}
\begin{definition}
	L'\textit{espérance} d'une variable aléatoire $X$, dénotée $\Esp{X}$, est
	la somme des valeur possibles de $X$ pondérées par leur probabilité,
	c'est-à-dire
	\begin{equation*}
		\Esp{X}=\sum_{k=1}^\infty x_kp_X(x_k)
	\end{equation*}
	si $X$ est discrète, et
	\begin{equation*}
		\Esp{X}=\int_{-\infty}^\infty xf_X(x)\d{x}
	\end{equation*}
	si $X$ est continue.
\end{definition}

\begin{exemple}
	Quelle est l'espérance d'un lancer d'un dé?

	On calcule l'espérance d'une variable discrète, soit
	\begin{equation*}
		\Esp{X}=
			1\cdot\frac{1}{6}+
			2\cdot\frac{1}{6}+
			3\cdot\frac{1}{6}+
			4\cdot\frac{1}{6}+
			5\cdot\frac{1}{6}+
			6\cdot\frac{1}{6}
		=3.5.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit $X\sim\Poi{\alpha}$. Quelle est l'espérance de $X$?

	Par définition, on calcule l'espérance avec
	\begin{equation*}
		\Esp{X}
		=\sum_{k=0}^\infty k\cdot p_X(k)
		=\sum_{k=0}^\infty k\cdot\frac{\e^{-\alpha}\alpha^k}{k!}
		=\sum_{k=1}^\infty k\cdot\frac{\e^{-\alpha}\alpha^k}{k!},
	\end{equation*}
	car le premier terme à $k=0$ est nul. Par conséquent,
	\begin{equation*}
		\Esp{X}
		=\sum_{k=1}^\infty\frac{\e^{-\alpha}\alpha^k}{(k-1)!}
		=\e^{-\alpha}\sum_{k=1}^\infty\frac{\alpha^k}{(k-1)!}
		=\e^{-\alpha}\sum_{i=0}^\infty\frac{\alpha^{i+1}}{i!}
		=\e^{-\alpha}\alpha\sum_{i=0}^\infty\frac{\alpha^i}{i!}.
	\end{equation*}
	Hors, la somme est le développement en séries de la fonction exponentielle,
	alors
	\begin{equation*}
		\Esp{X}
		=\e^{-\alpha}\alpha\e^{\alpha}
		=\alpha.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit $X\sim\Exp{\lambda}$. Quelle est l'espérance de $X$?

	Par définition, on calcule l'espérance avec
	\begin{equation*}
		\Esp{X}
		=\int_{-\infty}^\infty xf_X(x)\d{x}
		=\int_0^\infty x\lambda\e^{-\lambda x}\d{x}.
	\end{equation*}
	En posant $u=x$ et $\d{v}=\lambda\e^{-\lambda x}\d{x}$, on obtient
	\begin{equation*}
		\Esp{X}
		=-x\e^{-\lambda x}\bigg\rvert_0^\infty
		+\int_0^\infty\e^{-\lambda x}\d{x}
		=\frac{1}{\lambda}\int_0^\infty\lambda\e^{-\lambda x}\d{x}
		=\frac{1}{\lambda},
	\end{equation*}
	car l'aire sous la fonction de densité de probabilités est égal à 1.
\end{exemple}

\begin{property}
	$\Esp{c}=c$, où $c$ est une constante.
\end{property}

\begin{property}
	$\Esp{aX+b}=a\Esp{X}+b$, où $a$ et $b$ des constantes.
\end{property}

\begin{theoreme}
	Si $Y=g(X)$, où $X$ est une variable aléatoire et $g(X)$ une fonction,
	alors
	\begin{equation*}
		\Esp{Y}=\sum_{k=1}^\infty g(x_k)p_X(x_k)
	\end{equation*}
	si $X$ est discrète, et
	\begin{equation*}
		\Esp{Y}=\int_{-\infty}^\infty g(x)f_X(x)\d{x}
	\end{equation*}
	si $X$ est continue.
\end{theoreme}

\subsection{Espérance conditionelle}
\begin{definition}
	L'\textit{espérance conditionelle} d'une variable aléatoire $X$ sachant un
	événement $A$, dénotée $\Espg{X}{A}$, est la somme des valeurs possibles de
	$X$ pondérées par leur probabilité conditionnelle, c'est-à-dire
	\begin{equation*}
		\Espg{X}{A}=\sum_{k=1}^\infty x_kp_X(x_k|A)
	\end{equation*}
	si $X$ est discrète, et
	\begin{equation*}
		\Espg{X}{A}=\int_{-\infty}^\infty xf_X(x|A)\d{x}
	\end{equation*}
	si $X$ est continue.
\end{definition}

\begin{theoreme}
	Soit $X$ une variable aléatoire. Si $B_1,B_2,\dots,B_n$ forment une
	partition de $S_X$, alors
	\begin{equation*}
		\Esp{X}=\sum_{i=1}^n\Espg{X}{B_i}\P{B_i}.
	\end{equation*}
\end{theoreme}

\begin{exemple}
	Soit $X$ une variable aléatoire mixte. Quelle est la forme de l'espérance
	de $X$?

	On définie $C$ comme l'événement où $X$ prend une valeur continue et $D$
	lorsque $X$ prend une valeur discrète. Par conséquent, on a une partition
	de $S_X$ de sorte à avoir
	\begin{equation*}
		\Esp{X}
		=\underbrace{\Espg{X}{C}}_{\int}\P{C}
		+\underbrace{\Espg{X}{D}}_{\sum}\P{D}.
	\end{equation*}
\end{exemple}

\subsection{Variance}
\begin{definition}
	La \textit{variance} d'une variable aléatoire $X$, dénotée $\Var{X}$, est
	une mesure de la dispersion des valeurs de $X$ par rapport à la moyenne.
	Elle est définie comme
	\begin{equation*}
		\Var{X}=\Esp{\left(X-\Esp{X}\right)^2}.
	\end{equation*}
\end{definition}

\begin{property}
	$\Var{c}=c$, où $c$ est une constante.
\end{property}

\begin{property}
	$\Var{aX+b}=a^2\Var{X}$, où $a$ et $b$ des constantes.
\end{property}

\begin{property}
	$\Std{X}=\sqrt{\Var{X}}$.
\end{property}

\begin{property}
	$\Varg{X}{A}=\Espg{X^2}{A}-\Espg{X}{A}^2$.
\end{property}

\begin{theoreme}
	$\Var{X}=\Esp{X}^2-\Esp{X^2}$.
\end{theoreme}

\begin{proof}
	Par définition, on a
	\begin{equation*}
		\Var{X}
		=\Esp{\left(X-\Esp{X}\right)^2}
		=\Esp{X^2-2X\Esp{X}+\Esp{X}^2}.
	\end{equation*}
	Puisque l'espérance est un opérateur linéaire, on a
	\begin{equation*}
		\Var{X}
		=\Esp{X^2}-2\Esp{X\Esp{X}}+\Esp{\Esp{X}^2}.
	\end{equation*}
	Hors, $\Esp{X}$ et $\Esp{X}^2$ sont des constantes, alors
	\begin{equation*}
		\Var{X}
		=\Esp{X^2}-2\Esp{X}\Esp{X}+\Esp{X}^2
		=\Esp{X}^2-\Esp{X^2}.\qedhere
	\end{equation*}
\end{proof}

\begin{exemple}
	Soit $X\sim\Bin{1}{p}$. Quelle est la variance de $X$?

	On sait que l'espérance de $X$ est
	\begin{equation*}
		\Esp{X}=0\cdot(1-p)+1\cdot p=p,
	\end{equation*}
	et celle de $X^2$ est
	\begin{equation*}
		\Esp{X^2}=0^2\cdot(1-p)+1^2\cdot p=p.
	\end{equation*}
	Par conséquent, la variance est
	\begin{equation*}
		\Var{X}=p-p^2=p(1-p).
	\end{equation*}
\end{exemple}

\subsection{Inégalité de Markov}
\begin{theoreme}
	Soit $X$ une variable aléatoire prenant des valeurs non négatives, alors
	\begin{equation*}
		\P{X\geq a}\leq\frac{\Esp{X}}{a},
	\end{equation*}
	pour tout $a>0$.
\end{theoreme}

\subsection{Inégalité de Bienaymé-Tchebychev}
\begin{theoreme}
	Soit $X$ une variable aléatoire de moyenne $\mu=\Esp{X}$ et variance
	$\sigma^2=\Var{X}$ définies, alors
	\begin{equation*}
		\P{\middle|X-\mu\middle|\geq a}\leq\frac{\sigma^2}{a^2},
	\end{equation*}
	pour tout $a>0$.
\end{theoreme}

\begin{exemple}
	Soit le lancer d'une pièce de monnaie avec $X\sim\Bin{n}{\nicefrac{1}{2}}$.
	On calcule la moyenne avec
	\begin{equation*}
		\Esp{\frac{X}{n}}
		=\frac{1}{n}\Esp{X}
		=\frac{1}{n}\cdot n\cdot\frac{1}{2}
		=\frac{1}{2}
	\end{equation*}
	et la variance avec
	\begin{equation*}
		\Var{\frac{X}{n}}
		=\frac{1}{n^2}\Var{X}
		=\frac{1}{n^2}\cdot n\cdot\frac{1}{2}\cdot\frac{1}{2}
		=\frac{1}{4n}.
	\end{equation*}

	Selon l'inégalité de Bienaymé-Tchebychev, on a
	\begin{equation*}
		\P{\left|\frac{X}{n}-\frac{1}{2}\right|\geq 0.01}
		\geq\frac{1/4n}{(0.01)^2}
		=\frac{10000}{4n}
		=\frac{2500}{n}.
	\end{equation*}
\end{exemple}

\subsection{Fonction caractéristique}
\begin{definition}
	Une \textit{fonction caractéristique}, dénoté $\phi_X(\omega)$, est
	l'espérance d'une variable aléatoire $X$ tel que
	\begin{equation*}
		\phi_X(\omega)=\Esp{\e^{j\omega X}},
	\end{equation*}
	où $j$ est le nombre imaginaire tel que $j^2=-1$.
\end{definition}

\begin{property}
	$\phi_X(0)=1$.
\end{property}

\begin{property}
	$\Esp{X^n}=(-j)^n\left[
	\dfrac{\d{}^n}{\d{\omega}^n}\phi_X(\omega)\right]_{\omega=0}$.
\end{property}

\begin{exemple}
	Soit $X\sim\Norm{\mu}{\sigma^2}$. On peut montrer que
	\begin{equation*}
		\phi_X(\omega)=\exp\left(j\omega\mu-\frac{1}{2}\omega^2\sigma^2\right).
	\end{equation*}
	Soit $Y=aX+b$. Quelle est la fonction caractéristique de $Y$?
	Par définition, on a
	\begin{equation*}
		\phi_Y(\omega)
		=\Esp{\e^{j\omega Y}}
		=\Esp{\e^{j\omega(aX+b)}}
		=\e^{j\omega b}\Esp{\e^{j\omega aX}}
		=\e^{j\omega b}\cdot\exp\left(j\omega a\mu+\frac{1}{2}
			\omega^2a^2\sigma^2
		\right)
	\end{equation*}
	de sorte à obtenir
	\begin{equation*}
		\phi_Y(\omega)=
		\exp\bigg[
			j\omega\underbrace{(a\mu+b)}_{\mu_Y}-
			\frac{1}{2}\omega^2\underbrace{(a^2\sigma)^2}_{\sigma_Y^2}
		\bigg].
	\end{equation*}
	La fonction $\phi_Y(\omega)$ est de la même forme que $\phi_X(\omega)$,
	alors on peut en déduire que $Y$ suit aussi une loi normale.
\end{exemple}

\begin{exemple}
	Soit $X\sim\Bin{n}{p}$. Quelle est la fonction caractéristique de $X$?

	La fonction caractéristique de la loi binomiale est donnée par
	\begin{equation*}
		\phi_X(\omega)
		=\sum_{k=0}^n\e^{j\omega k}\cdot\comb{n}{k}p^kq^{n-k}
		=\sum_{k=0}^n\comb{n}{k}\left(p\e^{j\omega}\right)^kq^{n-k}
		=(p\e^{j\omega}+q)^n,
	\end{equation*}
	selon le binôme de Newton.
\end{exemple}

\subsection{Fiabilité}
\begin{definition}
	La \textit{fonction de fiabilité} d'un système, dénotée $R(t)$, est la
	probabilité que le temps de vie $T$ du système, une variable aléatoire
	continue et positive, soit supérieur à une valeur $t$, c'est-à-dire
	\begin{equation*}
		R(t)=\P{T>t}=1-F_T(t).
	\end{equation*}
\end{definition}

\begin{definition}
	Le \textit{taux de défaillance} d'un système, dénotée $r(t)$, est la
	probabilité qu'un système tombe subitement en panne, sachant qu'il
	fonctionnait au moment précédent, c'est-à-dire
	\begin{equation*}
		r(t)
		=\lim_{s\rightarrow t}f_T(s|T>t)
		=\frac{f_T(t)}{1-F_T(t)}
		=-\frac{R'(t)}{R(t)}.
	\end{equation*}
\end{definition}

\begin{theoreme}
	$R(t)=\exp\left\{-\displaystyle\int_0^tr(s)\d{s}\right\}$.
\end{theoreme}

\begin{proof}
	Il suffit de résoudre l'équation différentielle
	\begin{equation*}
		r(t)=-\frac{R'(t)}{R(t)},
	\end{equation*}
	avec $R(0)=1$, pour obtenir
	\begin{equation*}
		R(t)=\exp\left\{-\int_0^tr(s)\d{s}\right\}.\qedhere
	\end{equation*}
\end{proof}

\begin{exemple}
	Soit $T\sim\Exp{\lambda}$. Quel est le taux de défaillance?

	On calcule la fonction de fiabilité avec
	\begin{equation*}
		R(t)
		=1-F_T(t)
		=1-(1-\e^{-\lambda t})
		=\e^{-\lambda t}
	\end{equation*}
	de sorte à obtenir le taux de défaillance
	\begin{equation*}
		r(t)
		=\frac{\lambda\e^{-\lambda t}}{\e^{-\lambda t}}
		=\lambda.
	\end{equation*}
\end{exemple}

\subsubsection{Durée de vie moyenne}
\begin{definition}
	La \textit{durée de vie moyenne} d'un système est l'espérance de $T$, le
	temps de vie du système.
\end{definition}

\begin{theoreme}
	$\Esp{T}=\displaystyle\int_0^\infty R(t)\d{t}$.
\end{theoreme}

\begin{proof}
	Par définition, on a
	\begin{equation*}
		\Esp{T}
		=\int_0^\infty tf_T(t)\d{t}.
		=\int_0^\infty\left(\int_0^t\d{s}\right)f_T(t)\d{t}
		=\int_0^\infty\int_0^tf_T(t)\d{s}\d{t}.
	\end{equation*}
	En inversant l'ordre d'intégration, on obtient
	\begin{equation*}
		\Esp{T}
		=\int_0^\infty\underbrace{\int_s^\infty f_T(t)\d{t}}_{\P{T>s}}\d{s}
		=\int_0^\infty R(s)\d{s}.\qedhere
	\end{equation*}
\end{proof}

\begin{exemple}
	Soit un système en série à $n$ composantes indépendantes. Quelle est la
	fonction de fiabilité du système?

	La probabilité que le système fonctionne après un temps $t$ est équivalent
	à la probabilité que tous les composantes fonctionnents après un temps $t$,
	soit
	\begin{equation*}
		\P{T>t}
		=\P{\left\{T_1>t\right\}\cap\cdots\cap\left\{T_N>t\right\}}
		=\P{T_1>t}\cdots\P{T_n>t},
	\end{equation*}
	car les événements sont indépendants. Par conséquent, on obtient
	\begin{equation*}
		R(t)=\prod_{k=1}^n R_k(t).
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit un système en parrallèle à $n$ composantes indépendantes. Quelle est
	la fonction de fiablité dus système?

	La probabilité que le système soit en panne est équivalent à la probabilité
	que tous les composantes soit en panne après un temps $t$, soit
	\begin{equation*}
		\P{T\leq t}
		=\P{\left\{T_1\leq t\right\}\cap\cdots\cap\left\{T_n\leq t\right\}}
		=\P{T_1\leq t}\cdots\P{T_n\leq t},
	\end{equation*}
	car les événements sont indépendants. Par conséquent, on obtient
	\begin{equation*}
		R(t)=1-\prod_{k=1}^n\bigg[1-R_k(t)\bigg].
	\end{equation*}
\end{exemple}

\section{Vecteurs aléatoires}
\begin{definition}
	Un \textit{vecteur aléatoire} $\vec{X}=\invec{X_1&X_2&\cdots&X_n}$ est une
	généralisation à $n$ dimensions d'une variable aléatoire. Il est formé de
	plusieurs variables aléatoires observées lors d'une même expérience.
\end{definition}

\begin{exemple}
	Soit le lancer de 2 dés. On pose $X$ comme étant le résultat du premier dé
	et $Y$ comme étant le résultat du deuxième dé. Quelle est la probabilité
	d'avoir $(X,Y)=(j,k)$?

	L'espace échantillion est donné par
	\begin{equation*}
		S_{X,Y}=\left\{\
			\begin{matrix}
				(1,1), & (2,1), & \cdots & (6,1),\\
				(1,2), & (2,2), & \cdots & (6,2),\\
				\vdots & \vdots & \ddots & \vdots\\
				(1,6), & (2,6), & \cdots & (6,6) \\
			\end{matrix}
		\right\},
	\end{equation*}
	de sorte la probabilité d'avoir l'événement voulu est
	\begin{equation*}
		\P{\left\{X=j\right\}\cap\left\{Y=k\right\}}=\frac{1}{36},\quad\forall
		(j,k)\in\left\{1,\dots,6\right\}\times\left\{1,\dots,6\right\}.
	\end{equation*}
\end{exemple}

\begin{exemple}
	On génère un point au hasard dans le triangle $T$ de coordonnées $(0,0)$,
	$(1,1)$ et $(0,1)$. Soit $X$ l'abscisse du point et $Y$ l'ordonnée. Sachant
	qu'il y a équiprobabilité, quelle est la fonction de densité conjointe?

	Puisqu'il y a équiprobabilité, tous les points dans le triangle ont la même
	probabilité d'être généré. Par conséquent, on peut définir la fonction de
	densité comme
	\begin{equation*}
		f_{X,Y}(x,y)=\left\{
			\begin{array}{rl}
				c&\text{si $(x,y)$ est dans le triangle}\\
				0&\text{sinon}\\
			\end{array}
		\right.,
	\end{equation*}
	où la constante $c$ est donnée par
	\begin{equation*}
		\iint_Tf_{X,Y}(x,y)\d{A}=1\Leftrightarrow c=2.
	\end{equation*}
\end{exemple}

\subsection{Vecteur aléatoire discret}
\subsubsection{Fonction de masse conjointe}
\begin{definition}
	La \textit{fonction de masse conjointe}, dénotée $p_{\vec{X}}(\vec{x})$,
	est la probabilité qu'un vecteur aléatoire discret $\vec{X}$ prenne une
	certaine valeur, c'est-à-dire
	\begin{equation*}
		p_{\vec{X}}(\vec{x})
		=\P{\vec{X}=\vec{x}}
		=\P{\bigcap_{k=1}^n\left\{X_k=x_k\right\}}.
	\end{equation*}
\end{definition}

\begin{property}
	$\displaystyle\sum_{\forall\vec{x}\in S_{\vec{X}}}p_{\vec{X}}(\vec{x})=1$.
\end{property}

\begin{notation}
	Soit un vecteur aléatoire $\vec{X}$ à $n$ composantes. Alors la notation de
	la fonction du vecteur aléatoire $g_{\vec{X}}(\vec{x})$ est équivalente à
	$g_{X_1,X_2,\dots,X_n}(x_1,x_2,\dots,x_n)$.
\end{notation}

\subsubsection{Fonction de masse marginale}
\begin{definition}
	Soit $\vec{X}$, $\vec{Y}$ et $\vec{Z}$ des vecteurs aléatoires discrets
	tels que $\vec{X}=\invec{\vec{Y}&\vec{Z}}$. La \textit{fonction de masse
	marginale}, dénotée $p_{\vec{Z}}(\vec{z})$, est la probabilité que
	$\vec{Z}$ prend une valeur $\vec{z}$. On a alors
	\begin{equation*}
		p_{\vec{Z}}(\vec{z})
		=\sum_{\forall\vec{y}\in S_{\vec{Y}}}
			p_{\vec{X}}\left(\invec{\vec{y}&\vec{z}}\right).
	\end{equation*}
\end{definition}

\subsubsection{Fonction de masse conditionnelle}
\begin{definition}
	Soit $\vec{X}$, $\vec{Y}$ et $\vec{Z}$ des vecteurs aléatoires discrets 
	tels que $\vec{X}=\invec{\vec{Y}&\vec{Z}}$. La \textit{fonction de masse
	conditionnelle}, dénotée $p_{\vec{Z}|\vec{Y}}(\vec{z}|\vec{y})$, est la
	probabilité que $\vec{Z}$ prend une valeur $\vec{z}$ sachant que $\vec{Y}$
	prend une valeur $\vec{y}$. On a alors
	\begin{equation*}
		p_{\vec{Z}|\vec{Y}}(\vec{z}|\vec{y})
		=\frac{p_{\vec{X}}\left(\invec{\vec{y}&\vec{z}}\right)}
			{p_{\vec{Y}}\left(\vec{y}\right)}.
	\end{equation*}
\end{definition}

\subsection{Vecteur aléatoire continu}
\begin{remark}
	Les fonctions de densité ne sont pas des probabilités de prendre une
	valeur précise, mais bien une valeur au alentour.
\end{remark}

\subsubsection{Fonction de densité conjointe}
\begin{definition}
	La \textit{fonction de densité conjointe}, dénotée $f_{\vec{X}}(\vec{x})$,
	est la probabilité qu'un vecteur aléatoire continu $\vec{X}$ prenne une
	certaine valeur $\vec{x}$, c'est-à-dire
	\begin{equation*}
		f_{\vec{X}}(\vec{x})
		=\frac{1}{\epsilon_1\epsilon_2\cdots\epsilon_n}\P{
			\bigcap_{k=1}^n\left\{
				x-\frac{\epsilon_k}{2}\leq X\leq x+\frac{\epsilon_k}{2}
			\right\}},
	\end{equation*}
	où $\epsilon_i\rightarrow 0^+$ pour tout $i\in\left\{1,2,\dots,n\right\}$.
\end{definition}

\begin{property}
	$\displaystyle\idotsint\limits_{\vec{x}\in S_{\vec{X}}}f_{\vec{X}}(\vec{x})
	\d{\vec{x}}=1$.
\end{property}

\subsubsection{Fonction de densité marginale}
\begin{definition}
	Soit $\vec{X}$, $\vec{Y}$ et $\vec{Z}$ des vecteurs aléatoires continus
	tels que $\vec{X}=\invec{\vec{Y}&\vec{Z}}$. La \textit{fonction de densité
	marginale}, dénotée $f_{\vec{Z}}(\vec{z})$, est la probabilité que
	$\vec{Z}$ prend une valeur $\vec{z}$. On a alors
	\begin{equation*}
		f_{\vec{Z}}(\vec{z})=\idotsint\limits_{\vec{y}\in S_{\vec{Y}}}
		f_{\vec{X}}\left(\invec{\vec{y}&\vec{z}}\right)\d{\vec{y}}.
	\end{equation*}
\end{definition}

\subsubsection{Fonction de densité conditionnelle}
\begin{definition}
	Soit $\vec{X}$, $\vec{Y}$ et $\vec{Z}$ des vecteurs aléatoires continus
	tels que $\vec{X}=\invec{\vec{Y}&\vec{Z}}$. La \textit{fonction de densité
	conditionnelle}, dénotée $f_{\vec{Z}|\vec{Y}}(\vec{z}|\vec{y})$, est la
	probabilité que $\vec{Z}$ prend une valeur $\vec{z}$ sachant que $\vec{Y}$
	prend une valeur $\vec{y}$. On a alors
	\begin{equation*}
		f_{\vec{Z}|\vec{Y}}(\vec{z}|\vec{y})
		=\frac{f_{\vec{X}}\left(\invec{\vec{y}&\vec{z}}\right)}
			{f_{\vec{Y}}(\vec{y})}.
	\end{equation*}
\end{definition}

\subsection{Fonction de répartition conjointe}
\begin{definition}
	La \textit{fonction de répartition conjointe}, dénotée
	$F_{\vec{X}}(\vec{x})$, est la probabilité que les composantes d'un vecteur
	$\vec{X}$ soient plus petites ou égales à celles de $\vec{x}$, c'est-à-dire
	\begin{equation*}
		F_{\vec{X}}(\vec{x})=\P{\bigcap_{k=1}^n\left\{X_k\leq x_k\right\}}.
	\end{equation*}
\end{definition}

\begin{theoreme}
	$F_{\vec{X}}(\vec{x})=\displaystyle\sum_{\forall\vec{x\prime}\leq\vec{x}}
	p_{\vec{X}}(\vec{x}\prime)$ dans le cas discret.
\end{theoreme}

\begin{theoreme}
	$F_{\vec{X}}(\vec{x})=\displaystyle\idotsint\limits_{\forall\vec{x}\prime
	\leq\vec{x}}f_{\vec{X}}(\vec{x}\prime)\d{\vec{x}\prime}$ dans le cas
	continu.
\end{theoreme}

\begin{theoreme}
	$f_{\vec{X}}(\vec{x})=\dfrac{\p[n]{}}{\p{x_1}\p{x_2}\cdots\p{x_n}}
	F_{\vec{X}}(\vec{x})$ dans le cas continu.
\end{theoreme}

\subsection{Fonction de répartition marginale}
\begin{definition}
	Soit $\vec{X}$, $\vec{Y}$ et $\vec{Z}$ des vecteurs aléatoires tels que
	$\vec{X}=\invec{\vec{Y}&\vec{Z}}$. La \textit{fonction de répartition
	marginale}, dénotée $F_{\vec{Z}}(\vec{z})$, est la probabilité que
	$\vec{Z}$ prend une valeur $\vec{z}$. On a alors
	\begin{equation*}
		F_{\vec{Z}}(\vec{z})=\lim_{\vec{y}\rightarrow\infty}F_{\vec{X}}\left(
		\invec{\vec{y}&\vec{z}}\right).
	\end{equation*}
\end{definition}

\begin{notation}
	Soit un vecteur $\vec{x}\in\mathbb{R}^n$. La limite lorsque
	$\vec{x}\rightarrow\infty$ dénote la limite lorque $x_i\rightarrow\infty$
	pour tout $i\in\left\{1,2,\dots,n\right\}$.
\end{notation}

\begin{exemple}
	Quelle est la probabilité que $\invec{X&Y}$ soit dans un rectangle $R=[\,a,
	b\,]\times[\,c,d\,]$? On suppose que la fonction de répartition conjointe
	est
	\begin{equation*}
		F_{X,Y}(x,y)=\P{\left\{X\leq x\right\}\cap\left\{Y\leq y\right\}}.
	\end{equation*}

	En utilisant seulement la fonction de répartition conjointe, on peut
	montrer que
	\begin{equation*}
		\P{R}
		=F_{X,Y}(b,d)-F_{X,Y}(b,c)-F_{X,Y}(a,d)+4F_{X,Y}(a,c).
	\end{equation*}
\end{exemple}

\subsection{Indépendance dans un vecteur}
\begin{definition}
	Soit les variables aléatoires $X$ et $Y$. Ces variables sont
	\textit{indépendantes} si et seulement si
	\begin{enumerate}
		\item $S_{X,Y}=S_X\times S_Y$
		\item $F_{X,Y}(x,y)=F_X(x)F_Y(y)$
		\begin{itemize}
			\item[$\Rightarrow$] $p_{X,Y}(x_j,y_k)=p_X(x_j)p_Y(y_k)$
			\item[$\Rightarrow$] $f_{X,Y}(x,y)=f_X(x)f_Y(y)$
		\end{itemize}
	\end{enumerate}
\end{definition}

\begin{exemple}
	La veille d'un examen, un professeur estime qu'il recevra $X$ questions par
	courriel, où $X\sim\Poi{\alpha}$. Le professeur répond à chaque question
	avec une probabilité $p$, et ce indépendamment d'une question à l'autre.
	Soit $Y$ le nombre de réponses du professeur. Déterminer $p_Y(y)$.

	L'espace échantillion du vecteur aléatoire discrète $(X,Y)$ est
	\begin{equation*}
		S_{X,Y}=\left\{
			\begin{matrix}
				(0,0), & (1,0), & \cdots & (j,0), & \cdots\\
				       & (1,1), & \cdots & (j,1), & \cdots\\
				       &        & \ddots & \vdots &       \\
				       &        &        & (j,j), & \cdots\\
					   &        &        &        & \ddots\\
			\end{matrix}
		\right\}.
	\end{equation*}
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	On peut montrer que
	\begin{equation*}
		p_{X,Y}(j,k)
		=\underbrace{p_{Y|X}(k,j)}_{\Bin{j}{p}}
		 \underbrace{p_X(j)}_{\Poi{\alpha}}
		=\comb{k}{j}p^kq^{j-k}\cdot\frac{\e^{-\alpha}}{j!}\alpha^j,
	\end{equation*}
	de sorte que
	\begin{equation*}
		p_Y(k)
		=\sum_{j=k}^\infty p_{X,Y}(j,k)
		=\sum_{j=k}^\infty\frac{j!}{k!(j-k)!}p^kq^{j-k}\frac{\e^{-\alpha}}{j!}
			\alpha^j
		=\frac{p^k}{k!}\e^{-\alpha}\sum_{j=k}^\infty\frac{1}{(j-k)!}q^{j-k}
			\alpha^j.
	\end{equation*}
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	En appliquant une translation à la série, on obtient le développement en
	série de l'exponentielle, soit
	\begin{equation*}
		p_Y(k)
		=\frac{p^k}{k!}\e^{-\alpha}\sum_{j=0}^\infty\frac{(q\alpha)^j}{j!}
			\alpha^k
		=\frac{p^k}{k!}\e^{-\alpha}\e^{q\alpha}\alpha^k
		=\frac{(p\alpha)^k}{k!}\e^{-(p\alpha)}.
	\end{equation*}

	Par conséquent, $Y\sim\Poi{p\alpha}$.
\end{exemple}

\subsection{Espérance conditionnelle}
\begin{theoreme}
	Soit $X$ et $Y$ des variables aléatoires, alors
	\begin{equation*}
		\Espg{Y}{X=x_j}=\sum_{k=1}^\infty y_kp_{Y|X}(x_k,y_k)
	\end{equation*}
	si les variables sont discrètes, et
	\begin{equation*}
		\Espg{Y}{X=x}=\int_{-\infty}^\infty yf_{Y|X}(x,y)\d{y}
	\end{equation*}
	si $X$ est continue.
\end{theoreme}

\begin{remark}
	On peut définir l'espérance comme une fonction, c'est-à-dire $g(x)=
	\Espg{Y}{X=x}$ de sorte que $g(X)=\Espg{Y}{X}$ est une variable aléatoire.
\end{remark}

\begin{theoreme}\label{th:esp_cond}
	$\Esp{Y}=\Esp{\Espg{Y}{X}}$.
\end{theoreme}

\begin{proof}[Démonstration (cas continu).]
	On sait que
	\begin{equation*}
		\Esp{\Espg{Y}{X}}
		=\int_{-\infty}^\infty\Espg{Y}{X=x}f_X(x)\d{x}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty
         yf_{Y|X}(y|x)\d{y}\cdot f_X(x)\d{x}.
	\end{equation*}
	On remarque que
	\begin{equation*}
		\Esp{\Espg{Y}{X}}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty
		 y\cdot\underbrace{f_{Y|X}(y|x)f_X(x)}_{f_{X,Y}(x,y)}\d{y}\d{x}
		=\int_{-\infty}^\infty y\underbrace{\int_{-\infty}^\infty
         f_{X,Y}(x,y)\d{x}}_{f_Y(y)}\d{y}
	\end{equation*}
	de sorte que
	\begin{equation*}
		\Esp{\Espg{Y}{X}}
		=\int_{-\infty}^\infty yf_Y(y)\d{y}
		=\Esp{Y}.\qedhere
	\end{equation*}
\end{proof}

\subsection{Variance conditionnelle}
\begin{theoreme}
	$\Varg{Y}{X}=\Espg{Y^2}{X}-\left(\Espg{Y}{X}\right)^2$.
\end{theoreme}

\begin{theoreme}
	$\Var{Y}=\Esp{\Varg{Y}{X}}+\Var{\Espg{Y}{X}}$.
\end{theoreme}

\subsection{Espérance d'une transformation}
\begin{theoreme}
	Soit $\vec{X}$ un vecteur aléatoire et $g(\vec{x})$ une transformation,
	alors
	\begin{equation*}
		\Esp{g(\vec{X})}=\sum_{\forall\vec{x}\in S_{\vec{X}}}
		g(\vec{x})p_{\vec{X}}(\vec{x})
	\end{equation*}
	dans le cas discret, et
	\begin{equation*}
		\Esp{g(\vec{X})}=\idotsint\limits_{\forall\vec{x}\in S_X}
		g(\vec{x})f_{\vec{X}}(\vec{x})\d{\vec{x}}
	\end{equation*}
	dans le cas continu.
\end{theoreme}

\begin{exemple}
	Soit $X$ et $Y$ des variables aléatoires dont la fonction de masse
	conjointe est donnée par la table suivante.
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccc}
			\backslashbox{Y}{X}  & 0 & 1 & 2\\
			\hline
			0 & $\nicefrac{1}{6}$ & $\nicefrac{1}{6}$ & $\nicefrac{1}{6}$\\
			1 & 0                 & $\nicefrac{1}{6}$ & $\nicefrac{1}{6}$\\
			2 & 0                 & 0                 & $\nicefrac{1}{6}$\\
		\end{tabular}
	\end{table}
	\noindent Calculer $\Esp{X^2Y}$.

	Il suffit d'utiliser le théorème précèdent, soit
	\begin{equation*}
		\Esp{X^2Y}
		=\sum_{j=0}^2\sum_{k=0}^2(j^2k)p_{X,Y}(j,k)
		=\frac{13}{6}.
	\end{equation*}
\end{exemple}

\pagebreak
\begin{exemple}
	Soit $X\sim\Norm{1}{1}$ et $Y\sim\Norm{2}{1}$ des variables aléatoires
	indépendantes. Calculer $\Esp{X+Y}$ et $\Esp{XY}$.

	On sait que
	\begin{equation*}
		\Esp{X+Y}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty(x+y)f_{X,Y}(x,y)\d{x}\d{y}.
	\end{equation*}
	Puisque l'espérance est un opérateur linéaire, on a
	\begin{equation*}
		\Esp{X+Y}
		=\underbrace{\int_{-\infty}^\infty\int_{-\infty}^\infty
		 xf_{X,Y}(x,y)\d{x}\d{y}}_{\Esp{X}}
		+\underbrace{\int_{-\infty}^\infty
		 y\int_{-\infty}^\infty f_{X,Y}(x,y)\d{x}\d{y}}_{\Esp{Y}}
	\end{equation*}
	de sorte à obtenir que $\Esp{X+Y}=3$.
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	On sait que
	\begin{equation*}
		\Esp{XY}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty(xy)f_{X,Y}(x,y)\d{x}\d{y}.
	\end{equation*}
	Puisque les variables aléatoires sont indépendantes, on a
	\begin{equation*}
		\Esp{XY}
		=\int_{-\infty}^\infty xf_X(x)\d{x}\int_{-\infty}^\infty yf_Y(y)\d{y}
		=\Esp{X}\Esp{Y}
	\end{equation*}
	de sorte à obtenir $\Esp{XY}=2$.
\end{exemple}

\begin{theoreme}
	Soit un vecteur aléatoire $\vec{X}$ à $n$ variables indépendantes et une
	transformation $g(\vec{x})$, alors
	\begin{equation*}
		\Esp{g(\vec{X})}=\Esp{g_1(X_1)}\Esp{g_2(X_2)}\cdots\Esp{g_n(X_n)},
	\end{equation*}
	si $g(X)=g_1(X_1)g_2(X_2)\cdots g_n(X_n)$.
\end{theoreme}

\subsection{Corrélation et covariance}
\begin{definition}
	La \textit{corrélation} de deux variables aléatoires $X$ et $Y$ est
	l'espérance de leur produit, soit $\Esp{XY}$.
\end{definition}

\begin{remark}
	Lorsque $\Esp{XY}=0$, on dit que $X$ et $Y$ sont \textit{orthogonales}.
\end{remark}

\begin{definition}
	La \textit{covariance} de deux variables aléatoires $X$ et $Y$, dénotée
	$\Cov{X}{Y}$, est une mesure permettant de quantifier leurs écarts
	conjoints par rapport à leurs expérances respectives, c'est-à-dire
	\begin{equation*}
		\Cov{X}{Y}=\Esp{\left(X-\Esp{X}\right)\left(Y-\Esp{Y}\right)}.
	\end{equation*}
\end{definition}

\begin{property}
	$\Cov{X}{Y}=\Esp{XY}-\Esp{X}\Esp{Y}$.
\end{property}

\begin{property}
	$\Cov{X}{X}=\Var{X}$.
\end{property}

\begin{definition}
	Le \textit{coefficient de corrélation} $\rho_{X,Y}$ est une mesure de
	corrélation linéaire entre deux variables aléatoires $X$ et $Y$. Il est
	défini comme
	\begin{equation*}
		\rho_{X,Y}=\frac{\Cov{X}{Y}}{\Std{X}\Std{Y}}.
	\end{equation*}
\end{definition}

\begin{property}
	$-1\leq\rho_{X,Y}\leq 1$.
\end{property}

\begin{property}
	$\rho_{X,Y}=1\Leftrightarrow Y=aX+b$ où $a>0$.
\end{property}

\begin{property}
	$\rho_{X,Y}=-1\Leftrightarrow Y=aX+b$ où $a<0$.
\end{property}

\begin{property}
	$\rho_{X,Y}=0$ si $X$ et $Y$ sont indépendants.
\end{property}

\begin{remark}
	Si $\rho_{X,Y}=0$, alors $X$ et $Y$ ne sont pas nécessairement
	indépendantes. On dit que ce sont des \textit{variables non-corrélées}.
\end{remark}

\begin{theoreme}
	Soit $X\sim\Norm{\mu_X}{\sigma_X^2}$ et $Y\sim\Norm{\mu_Y}{\sigma_Y^2}$ des
	variables aléatoires telles que $\rho_{X,Y}=0$, alors $X$ et $Y$ sont
	indépendantes.
\end{theoreme}

\subsection{Loi binormale}
\begin{definition}
	Soit un vecteur aléatoire $\invec{X&Y}$. Si $X\sim\Norm{\mu_X}{\sigma_X^2}$
	et $Y\sim\Norm{\mu_Y}{\sigma_Y^2}$ alors on dit que le vecteur suit une
	\textit{loi binormale}, dénotée
	\begin{equation*}
		\left[\begin{matrix}X\\Y\end{matrix}\right]\sim\BiNorm{\mu_X}{\mu_Y}
		{\sigma_X^2}{\sigma_Y^2}{\rho},
	\end{equation*}
	où $\rho\equiv\rho_{X,Y}$. On peut alors montrer que
	\newsavebox{\overlongequation}
	\begin{displaymath}
		\begin{lrbox}{\overlongequation}
			$\displaystyle
			f_{X,Y}(x,y)=\frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}
			\exp\left\{-\frac{1}{2(1-\rho)^2}\left[
				\left(\frac{x-\mu_X}{\sigma_X}\right)^2+
				\left(\frac{x-\mu_Y}{\sigma_Y}\right)^2+
				2\rho\frac{(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}
			\right]\right\}.
			$
		\end{lrbox}
		\makebox[0pt]{\usebox{\overlongequation}}
	\end{displaymath}
\end{definition}

\begin{property}
	$X|\left\{Y=y\right\}\sim\Norm
	{\mu_X+\rho\dfrac{\sigma_X}{\sigma_Y}(y-\mu_Y)}
	{\sigma_X^2(1-\rho)^2}.$
\end{property}

\subsection{Estimation}
\begin{definition}
	Un \textit{estimateur} $g(X)$ est une fonction cherchant à prévoir la
	valeur de $Y$ à partir de $X$. Elle est définie comme étant la solution
	minimisant l'erreur quadratique moyenne, c'est-à-dire
	\begin{equation*}
		\Min\Esp{\left(Y-g(X)\right)^2}.
	\end{equation*}
\end{definition}

\subsubsection{Estimateur constant}
\begin{theoreme}
	Si $g(X)$ est un estimateur constant de $Y$, alors $g(X)=\Esp{Y}$.
\end{theoreme}

\begin{proof}
	Dans un cas où $g(X)=c$, le problème se résume à
	\begin{equation*}
		\Min\Esp{\left(Y-c\right)^2}\equiv
		\Min\Esp{Y^2}-2c\Esp{Y}+c^2.
	\end{equation*}
	En dérivant, on obtient la constante optimale
	\begin{equation*}
		\frac{\d{}}{\d{c}}\left(\Esp{Y^2}-2c\Esp{Y}+c^2\right)
		=-2\Esp{Y}+2c=0
		\Leftrightarrow c=\Esp{Y}
	\end{equation*}
	de sorte que
	\begin{equation*}
		g(X)=\Esp{Y}.\qedhere
	\end{equation*}
\end{proof}

\begin{corollary}
	$\Esp{\left(Y-g(Y)\right)^2}=\Var{Y}$.
\end{corollary}

\subsubsection{Estimateur linéaire}
\begin{theoreme}
	Si $g(X)$ est un estimateur linéaire de $Y$, alors $g(X)=\hat{a}X+\hat{b}$,
	où
	\begin{equation*}
		\hat{a}=\frac{\Std{Y}}{\Std{X}}\rho_{X,Y}
		\quad\text{et}\quad
		\hat{b}=\Esp{Y}-\hat{a}\Esp{X}.
	\end{equation*}
\end{theoreme}

\begin{corollary}
	$\Esp{\left(Y-g(X)\right)^2}=\Var{Y}(1-\rho_{X,Y}^2)$.
\end{corollary}

\subsubsection{Estimateur non linéaire}
\begin{theoreme}
	Si $g(X)$ est un estimateur non-linéaire de $Y$, alors $g(X)=\Espg{Y}{X}$.
\end{theoreme}

\begin{proof}[Démonstration (cas discret).]
	Selon le théorème \ref{th:esp_cond}, on a
	\begin{equation*}
		\Min\Esp{\left(Y-g(X)\right)^2}\equiv
		\Min\Esp{\Espg{\left(Y-g(X)\right)^2}{X}},
	\end{equation*}
	de sorte que problème peut s'écrire sous la forme
	\begin{equation*}
		\Min\sum_{j=1}^\infty\underbrace{\Espg{Y-g(X)}{X=x_j}}_{\text{fonction
		positive à min. $\forall x_j$}}p_X(x_j).
	\end{equation*}

	On retrouve alors plusieurs cas d'estimateurs constants, mais conditionnés
	pour $X=x_j$. Par conséquent,
	\begin{equation*}
		g(X)=\Espg{Y}{X}.\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Estimateur d'une binormale}
\begin{theoreme}
	Soit un vecteur aléatoire $\invec{X&Y}\sim\BiNorm{\mu_X}{\mu_Y}{\sigma_X^2}
	{\sigma_Y^2}{\rho}$, alors le meilleur estimateur $g(X)$ de $Y$ est un
	estimateur linéaire.
\end{theoreme}

\begin{exemple}
	Soit $\invec{X&Y}\sim\BiNorm{\mu_X=3}{\mu_Y=1}{\sigma_X^2=4}{\sigma_Y^2=9}
	{\rho=\nicefrac{1}{4}}$. Quel est le meilleur estimateur de $Y$ en fonction
	de $X$?

	Avec le théorème précédent, on a que le meilleur estimateur est linéaire,
	alors
	\begin{equation*}
		\hat{a}=\frac{\Std{Y}}{\Std{X}}\rho=\frac{3}{8}
	\end{equation*}
	et
	\begin{equation*}
		\hat{b}=\Esp{Y}-\hat{a}\Esp{X}=-\frac{1}{8},
	\end{equation*}
	de sorte que
	\begin{equation*}
		g(X)=\frac{3}{8}X-\frac{1}{8}.
	\end{equation*}
\end{exemple}

\subsection{Combinaison linéaire}
\begin{definition}
	Soit un vecteur aléatoire $\invec{X_1&X_2&\cdots&X_n}$, alors la
	\textit{combinaison linéaire} $Z$ des variables aléatoires du vecteur est
	\begin{equation*}
		Z=a_0+a_1X_1+a_2X_2+\cdots+a_nX_n=a_0+\sum_{i=1}^na_iX_i,
	\end{equation*}
	où $a_0,a_1,\dots,a_n$ sont des constantes.
\end{definition}

\begin{property}
	$\Esp{Z}=a_0+\displaystyle\sum_{i=1}^na_i\Esp{X_i}$.
\end{property}

\begin{property}
	$\Var{Z}=\displaystyle\sum_{i=1}^n\displaystyle\sum_{j=1}^na_ia_j
	\Cov{X_i}{X_j}$.
\end{property}

\begin{proof}
	Soit $Y_i=a_iX_i$. Par définition, on a
	\begin{equation*}
		\Var{Z}
		=\Var{a_0+\sum_{i=1}^nY_i}
		=\Var{\sum_{i=1}^nY_i}
	\end{equation*}
	et
	\begin{equation*}
		\Var{Z}
		=\Esp{\left(\sum_{i=1}^nY_i\right)^2}
		-\Esp{\sum_{i=1}^nY_i}^2
		=\Esp{\left(\sum_{i=1}^nY_i\right)^2}
		-\left(\sum_{i=1}^n\Esp{Y_i}\right)^2.
	\end{equation*}

	En écrivant explicitement les carrés, on a
	\begin{equation*}
		\Var{Z}
		=\Esp{\left(\sum_{i=1}^nY_i\right)\left(\sum_{j=1}^nY_j\right)}
		-\left(\sum_{i=1}^n\Esp{Y_i}\right)\left(\sum_{j=1}^n\Esp{Y_j}\right)
	\end{equation*}
	et
	\begin{equation*}
		\Var{Z}
		=\Esp{\sum_{i=1}^n\sum_{j=1}^nY_iY_j}
		-\sum_{i=1}^n\sum_{j=1}^n\Esp{Y_i}\Esp{Y_j}
	\end{equation*}

	On peut écrire les sommes comme
	\begin{equation*}
		\Var{Z}
		=\sum_{i=1}^n\sum_{j=1}^n\Esp{Y_iY_j}
		-\sum_{i=1}^n\sum_{j=1}^n\Esp{Y_i}\Esp{Y_j}
		=\sum_{i=1}^n\sum_{j=1}^n\bigg(\Esp{Y_iY_j}-\Esp{Y_i}\Esp{Y_j}\bigg).
	\end{equation*}
	Avec la définition de la covariance, on obtient
	\begin{equation*}
		\Var{Z}
		=\sum_{i=1}^n\sum_{j=1}^n\Cov{Y_i}{Y_j}
		=\sum_{i=1}^n\sum_{j=1}^na_ia_j\Cov{X_i}{X_j}\qedhere
	\end{equation*}
\end{proof}

\subsection{Variables aléatoires indépendantes et identiquement distribuées}
\begin{definition}
	Les variables aléatoires $X_1,X_2,\dots,X_n$ sont indépendantes et
	identiquement distribuées si elles sont tous indépendantes entre elles et
	qu'elles suivent tous la même distribution.
\end{definition}

\begin{notation}
	On dénote $S_n=X_1+X_2+\cdots+X_n$ la somme de $n$ variables aléatoires
	indépendantes et identiquement distribuées.
\end{notation}

\begin{property}
	$\Esp{S_n}=n\Esp{X}$.
\end{property}

\begin{property}
	$\Var{S_n}=n\Var{X}$.
\end{property}

\begin{exemple}
	Si $X_1\sim\Bin{n_1}{p}$ et $X_2\sim\Bin{n_2}{p}$ sont des variables
	aléatoires indépendantes, alors on a
	\begin{equation*}
		X_1+X_2\sim\Bin{n_1+n_2}{p}.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Si $X_i\sim\Poi{\alpha_i}$ sont des variables aléatoires indépendantes,
	alors on a
	\begin{equation*}
		X_1+X_2+\dots+X_n\sim\Poi{\alpha},
	\end{equation*}
	où $\alpha=\alpha_1+\alpha_2+\dots+\alpha_n$.
\end{exemple}

\begin{exemple}
	Si $X_i\sim\Exp{\lambda}$ sont des variables aléatoires indépendantes,
	alors on a
	\begin{equation*}
		X_1+X_2+\dots+X_n\sim\Gam{n}{\lambda}.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Si $X_i\sim\Gam{\alpha_i}{\lambda}$ sont des variables aléatoires
	indépendantes, alors on a
	\begin{equation*}
		X_1+X_2+\dots+X_n\sim\Gam{\alpha}{\lambda},
	\end{equation*}
	où $\alpha=\alpha_1+\alpha_2+\dots+\alpha_n$.
\end{exemple}

\begin{exemple}
	Si $X_i\sim\Norm{\mu_i}{\sigma_i^2}$ sont des variables aléatoires
	indépendantes, alors on a
	\begin{equation*}
		a_0+\sum_{i=1}^na_iX_i\sim\Norm{\mu}{\sigma^2},
	\end{equation*}
	où
	\begin{equation*}
		\mu=a_0+\sum_{i=1}^na_i\mu_i
		\quad\text{et}\quad
		\sigma^2=\sum_{i=1}^n\sum_{j=1}^na_ia_j\Cov{X_i}{X_j}.
	\end{equation*}
\end{exemple}

\subsection{Loi faible des grands nombres}
\begin{theoreme}
	Soit une variable aléatoire $S_n=X_1+X_2+\cdots+X_n$, où $X_1,X_2,\dots,
	X_n$ sont des variables aléatoires indépendantes et identiquement
	distribuées de moyenne $\mu$, alors
	\begin{equation*}
		\lim_{n\rightarrow\infty}\P{\left\rvert
			\frac{S_n}{n}-\mu
		\right\rvert<c}=1,
	\end{equation*}
	pour tout $c>0$.
\end{theoreme}

\subsection{Loi forte des grands nombres}
\begin{theoreme}
	Soit une variable aléatoire $S_n=X_1+X_2+\cdots+X_n$, où $X_1,X_2,\dots,
	X_n$ sont des variables aléatoires indépendantes et identiquement
	distribuées de moyenne $\mu$ et de variance $\sigma^2$, alors
	\begin{equation*}
		\P{\lim_{n\rightarrow\infty}\frac{S_n}{n}=\mu}=1.
	\end{equation*}
\end{theoreme}

\subsection{Théorème central limite}
\begin{theoreme}
	Soit une variable aléatoire $S_n=X_1+X_2+\cdots+X_n$, où $X_1,X_2,\dots,
	X_n$ sont des variables aléatoire indépendantes et identiquement
	distribuées de moyenne $\mu$ et de variance $\sigma^2$. Si $n$ est
	\textit{grand}, alors
	\begin{equation*}
		S_n\approx\Norm{n\mu}{n\sigma^2}
		\Leftrightarrow
		\frac{S_n}{n}\approx\Norm{\mu}{\frac{\sigma^2}{n}}
		\Leftrightarrow
		\frac{S_n-n\mu}{\sqrt{n\sigma^2}}\approx\Norm{0}{1}.
	\end{equation*}
\end{theoreme}

\begin{remark}
	En général, le théorème central limite est applicable si $n\geq 30$.
\end{remark}

\begin{exemple}
	Soit $X\sim\Bin{2000}{\nicefrac{1}{2}}$. Donnez une approximation de $X$.

	Une loi binomiale est équivalente à une somme de lois de Bernoulli
	indépendantes et identiquement distribuées de paramètre $p$. On approxime
	donc la loi binomiale à l'aide du théorème central limite, soit
	\begin{equation*}
		X\approx\Norm{np}{npq}.
	\end{equation*}
	
	On a alors que
	\begin{equation*}
		\P{X=k}\approx\frac{1}{\sqrt{2\pi\cdot npq}}\exp\left\{
			-\frac{(k-np)^2}{2npq}
		\right\}
	\end{equation*}

	Puisqu'on approxime une loi discrète par une loi continue, on applique
	une \textit{correction de continuité}, c'est-à-dire
	\begin{equation*}
		\P{a\leq X\leq b}
		\approx\Phi\left(
			\frac{b+\nicefrac{1}{2}-np}{\sqrt{npq}}
		\right)-\Phi\left(
			\frac{a-\nicefrac{1}{2}-np}{\sqrt{npq}}
		\right)
	\end{equation*}
	et
	\begin{equation*}
		\P{a<X\leq b}
		\approx\Phi\left(
			\frac{b+\nicefrac{1}{2}-np}{\sqrt{npq}}
		\right)-\Phi\left(
			\frac{a+\nicefrac{1}{2}-np}{\sqrt{npq}}
		\right).
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit les variables aléatoires $X$ et $Y$ avec la fonction densité conjointe
	suivante
	\begin{equation*}
		f_{X,Y}(x,y)=\frac{3}{4},
	\end{equation*}
	si $0<y<1$ et $x^2<y$. Calculez $f_X(x)$ et $f_Y(y)$, ainsi que
	$\P{Y\geq X}$.

	Pour un $x$, on a
	\begin{equation*}
		f_X(x)
		=\int_{x^2}^1f_{X,Y}(x,y)\d{y}
		=\frac{3}{4}(1-x^2),
	\end{equation*}
	si $-1<x<1$. Pour un $y$, on a
	\begin{equation*}
		f_Y(y)
		=\int_{-\sqrt{y}}^{\sqrt{y}}f_{X,Y}(x,y)\d{x}
		=\frac{3}{2}\sqrt{y},
	\end{equation*}
	si $0<y<1$.

	Par définition, on a
	\begin{equation*}
		\P{Y\geq X}
		=\int_0^1\int_{-\sqrt{y}}^{\sqrt{y}}f_{X,Y}(x,y)\d{x}\d{y}
		=\frac{7}{8}.
	\end{equation*}
\end{exemple}

\pagebreak
\section{Processus stochastique}
\begin{definition}
	Un \textit{processus stochastique} est une expérience aléatoire qui ce
	déroule dans le temps. Il est défini par son \textit{espace des états}
	$\left\{X(t):t\in T\right\}$, où $X(t)$ est la variable aléatoire
	correspondant à l'observation du processus au temps $t\leq 0$.
\end{definition}

\begin{definition}
	Une \textit{trajectoire} d'un processus stochastique est une événement
	élémentaire de l'espace des états.
\end{definition}

Il existe 4 types de processus stochastique:
\begin{enumerate}
	\item processus stochastique à temps discret et état discret (PSTDED)
	\begin{exemple*}
		Le nombre de personnes en classe à chaque jour.
	\end{exemple*}
	\item processus stochastique à temps discret et état continu (PSTDEC)
	\begin{exemple*}
		La valeur d'une action à la fermmeture de la bourse.
	\end{exemple*}
	\item processus stochastique à temps continu et état discret (PSTCED)
	\begin{exemple*}
		La longueur de la file d'attente à la cafétéria.
	\end{exemple*}
	\item processus stochastique à temps discret et état discret (PSTCEC)
	\begin{exemple*}
		La valeur d'une action en temps réel.
	\end{exemple*}
\end{enumerate}

\begin{definition}
	Soit $\left\{X(t):t\in T\right\}$, où $T=[0,\infty[$, un processus
	stochastique à temps continu et état continu. La \textit{fonction de
	répartition d'ordre $n$} est la probabilité que $n$ états soient plus
	petite ou égal à $n$ valeurs, c'est-à-dire
	\begin{equation*}
		F(x_1,\dots,x_n;t_1,\dots,t_n)
		=\P{\left\{X(t_1)\leq x_1\right\}
		 \cap\cdots\cap
		 \left\{X(t_n)\leq x_n\right\}}.
	\end{equation*}
\end{definition}

\begin{definition}
	Soit $\left\{X(t):t\in T\right\}$, où $T=[0,\infty[$, un processus
	stochastique à temps continu et état continu. La \textit{fonction de
	densité d'ordre $n$} est la probabilité que $n$ états soient autour de $n$
	valeurs, c'est-à-dire
	\begin{equation*}
		f(x_1,\dots,x_n;t_1,\dots,t_n)
		=\P{\left\{X(t_1)=x_1\right\}
		 \cap\cdots\cap
		 \left\{X(t_n)=x_n\right\}}.
	\end{equation*}
\end{definition}

\begin{theoreme}
	$f(x_1,\dots,x_n;t_1,\dots,t_n)=\dfrac{\p[n]{}}{\p{x_1}\cdots\p{x_n}}
	F(x_1,\dots,x_n;t_1,\dots,t_n)$.
\end{theoreme}

\begin{definition}
	Un \textit{accroissement} $X(t_1,t_2)$, où $t_1<t_2$, est la différence
	les états à $t_1$ et $t_2$, c'est-à-dire
	\begin{equation*}
		X(t_1,t_2)=X(t_2)-X(t_1).
	\end{equation*}
\end{definition}

\pagebreak
\begin{exemple}
	Soit un patient qui recoit une dose de médicament $Y$. Au temps $t=0$, on
	a $Y\sim\Uni{0}{1}$. La quantité active $X$ après $t$ unités de temps est
	définie par $X(t)=Y\e^{-t}$. Quel est la fonction de répartition du
	premier ordre, soit $F(x;t)$?

	Par définition, on a $F(x;t)=\P{X(t)\leq x}=\P{Y}\leq x\e^t$. Pour une loi
	uniforme, on a
	\begin{equation*}
		F(x;t)=\left\{
			\begin{matrix}
				0     &\text{si}& x\e^t<0\\
				x\e^t &\text{si}& 0\leq x\leq e^{-t}\\
				1     &\text{si}& \e^{-t}<x\\
			\end{matrix}
		\right..
	\end{equation*}
\end{exemple}

\subsection{Moyenne d'un processus stochastique}
\begin{definition}
	La \textit{moyenne} d'un processus stochastique, dénotée $m_X(t)$, est
	donnée par
	\begin{equation*}
		m_X(t)=\Esp{X(t)}=\int_{-\infty}^\infty xf(x;t)\d{x}.
	\end{equation*}
\end{definition}

\begin{exemple}
	Quel est la moyenne de $X(t)=Y\e^{-t}$, où $Y\sim\Uni{0}{1}$?

	Avec l'exemple précèdent, on obtient que la fonction de densité de
	probabilité est donnée par
	\begin{equation*}
		f(x;t)=\frac{\p{}}{\p{x}}F(x;t)=\e^t,
	\end{equation*}
	pour $0\leq x\leq e^{-t}$. La moyenne est donc
	\begin{equation*}
		m_X(t)=\int_0^{e^{-t}}x\e^t\d{x}=\frac{e^{-t}}{2}.
	\end{equation*}
\end{exemple}

\subsection{Fonction d'autocorrélation}
\begin{definition}
	La \textit{fonction d'autocorrélation}, dénotée $R_X(t_1,t_2)$, est la
	corrélation d'un processus stochastique $X(t)$ avec lui-même à deux temps
	différents. Par conséquent, on a
	\begin{equation*}
		R_X(t_1,t_2)
		=\Esp{X(t_1)X(t_2)}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty
			x_1x_2f(x_1,x_2;t_1,t_2)\d{x_1}\d{x_2}.
	\end{equation*}
\end{definition}

\subsection{Fonction d'autocovariance}
\begin{definition}
	La \textit{fonction d'autocovariance}, dénotée $C_X(t_1,t_2)$, est la
	covariance d'un processus stochastique $X(t)$ avec lui-même à deux temps
	différents. Par conséquent, on a
	\begin{equation*}
		C_X(t_2,t_2)
		=R_X(t_1,t_2)-m_X(t_1)m_X(t_2).
	\end{equation*}
\end{definition}

\subsection{Processus stochastique stationnaire au sens large}
\begin{definition}
	Si $m_X(t)=c$, où $c$ est une constante, et si $R_X(t_1,t_2)=h(t_2-t_1)$, où
	$h$ est une fonction, alors on dit que le processus stochastique est
	\textit{stationnaire au sens large} (SSL).
\end{definition}

\begin{exemple}
	Soit un signal aléatoire $X(t)=Y\sin{(t+Z)}$, où $Y$ et $Z$ sont des
	variables aléatoires indépendantes et $Z\sim\Uni{0}{2\pi}$. Montrer que ce
	signal est SSL.

	Par définition, la moyenne est donnée par
	\begin{equation*}
		m_X(t)
		=\Esp{Y\sin{(t+Z)}}
		=\Esp{Y}\hspace{-8mm}\underbrace{\Esp{\sin{(t+Z)}}}_{\displaystyle
			\int_0^{2\pi}\sin{(t+z)}\frac{1}{2\pi}\d{z}=0}\hspace{-8mm}
		=0,
	\end{equation*}
	ce qui est une constante. De plus, la fonction d'autocorrélation est donnée
	par
	\begin{equation*}
		R_X(t_1,t_2)
		=\Esp{Y\sin{(t_1+Z)}Y\sin{(t_2+Z)}}
		=\Esp{Y^2}\Esp{\sin{(t_1+Z)}\sin{(t_2+Z)}},
	\end{equation*}
	et avec une identité trigonométrique, on obtient
	\begin{equation*}
		\begin{split}
			R_X(t_1,t_2)
			&=\frac{1}{2}\Esp{Y^2}\left(
				\Esp{\cos{(t_1-t_2)}}-\underbrace{\Esp{\cos{(t_1-t_2+2Z)}}}_0
			\right)\\
			&=\frac{1}{2}\Esp{Y^2}\cos{(t_1-t_2)}
			=h(t_2-t_1).
		\end{split}
	\end{equation*}

	Par conséquent, ce signal est stationnaire au sens large.
\end{exemple}

\subsection{Chaîne de Markov}
\begin{definition}
	Une \textit{chaîne de Markov} est un processus stochastique sans mémoire à
	temps discret et état discret dont l'espace des états $\left\{X_n:n=0,1,
	\dots\right\}$ est constitué de nombres entiers.
\end{definition}

\begin{notation}
	On dénote $p_{i,j}$ la probabilité de passer de l'état $i$ vers l'état $j$,
	où $i$ et $j$ sont des nombres entiers.
\end{notation}

\begin{theoreme}
	$p_{i,j}=\Pg{X_{n+1}=j}{X_n=i}$.
\end{theoreme}

\subsubsection{Représentation graphique}
Une chaîne de Markov peut se représenter comme un graphe, où les noeuds sont
les états et les segments sont les probabilité $p_{i,j}$. La figure
\ref{fig:markov_graph} représente le graphe d'une chaîne de Markov ayant 4
états.

\begin{figure}[H]
	\centering
	\input{figure/markov_chain}
	\caption{Chaîne de Markov à 4 états}
	\label{fig:markov_graph}
\end{figure}

\subsubsection{Représentation matricielle}
\begin{definition}
	Soit $P$ la \textit{matrice des probabilités de transition} en une étape où
	chaque cellule $p_{i,j}$ sont les probabilités de passer de l'état $i$ vers
	l'état $j$ telles que
	\begin{equation*}
		P=\left[
			\begin{matrix}
				p_{0,0} & p_{0,1} & \cdots & p_{0,n}\\
				p_{1,0} & p_{1,1} & \cdots & p_{1,n}\\
				\vdots  & \vdots  & \ddots & \vdots \\
				p_{n,0} & p_{n,1} & \cdots & p_{n,n}\\
			\end{matrix}
		\right].
	\end{equation*}
\end{definition}

\begin{exemple}
	On établie des prévisions météorologiques. On défini 3 états suivants : une
	journée ensoleillée (état 0), une journée nuageuse (état 1) et une journée
	pluvieuse (état 2). Les probabilités de transition sont définies par la
	matrice suivante :
	\begin{equation*}
		P=\left[
			\begin{matrix}
				0.40 & 0.35 & 0.25\\
				0.30 & 0.20 & 0.50\\
				0.75 & 0.10 & 0.15\\
			\end{matrix}
		\right]
	\end{equation*}
	Quelle est la probabilité qu'il pleuve le lendemain d'une journée
	ensoleillée? Quelle est la probabilité que trois journées ensoleillées
	suivent une journée nuageuse? Quelle est la probabilité qu'il ne pleuve pas
	pendant les deux jours suivant une journée pluvieuse?

	On cherche $p_{0,2}$ dans la matrice, alors la probabilité est de 25\%.

	On cherche $p_{1,0}p_{0,0}p_{0,0}$, alors la probabilité est de 4.8\%.

	Il faut calculer toutes les trajectoires possibles. Il y a 4 cas possibles
	On cherche $p_{2,0}p_{0,0}+p_{2,0}p_{0,1}+p_{2,1}p_{1,0}+p_{2,1}p_{1,1}$,
	alors la probabilité est 61.25\%.
\end{exemple}

\subsubsection{Probabilité de transition en \boldmath{$n$} étapes}
\begin{notation}
	On dénote $p_{i,j}^n$ la probabilité de passer de l'état $i$ vers l'état
	$j$ en $n$ étapes, où $i$ et $j$ sont des nombres entiers.
\end{notation}

\begin{theoreme}
	$p_{i,j}^n=\Pg{X_{m+n}=j}{X_m=i}$.
\end{theoreme}

\begin{theoreme}
	Si $P^n=\underbrace{P\cdot P\cdots P}_{\text{$n$ fois}}$, alors $p_{i,j}^n
	=P_{i,j}^n$.
\end{theoreme}

\subsection{Processus de comptage}
\begin{definition}
	Un \textit{processus de comptage} est un processus stochastique à temps
	continu et état discret dont l'espace des états est $\left\{N(t):t\geq 0
	\right\}$ avec $N(t)$ le nombre d'événements jusqu'à un temps $t$ tel que
	\begin{enumerate}
		\item $N(t)\geq 0$ pour tout $t\geq 0$
		\item $N(t)\in\mathbb{N}$ pour tout $t\geq 0$
		\item $N(t_1)\leq N(t_2)$ si $t_1\leq t_2$
	\end{enumerate}
\end{definition}

\subsection{Processus de Poisson}
\begin{definition}
	Un \textit{processus de Poisson} est un processus de comptage où
	\begin{enumerate}
		\item $N(0)=0$
		\item $N(t_1,t_2)$ et $N(t_3,t_4)$ sont indépendants si $t_1<t_2\leq
		      t_3<t_4$
		\item $N(\tau,\tau+t)\sim\Poi{\lambda t}$
	\end{enumerate}
\end{definition}

\begin{theoreme}
	$C_N(t_1,t_2)=\lambda\cdot\mathrm{min}(t_1,t_2)$.
\end{theoreme}

\subsubsection{Temps d'arrivé}
\begin{notation}
	On dénote $T_n$ le temps entre le $n$-ième et celui qui le précède.
\end{notation}

\begin{theoreme}
	$T_n\sim\Exp{\lambda}$.
\end{theoreme}

\begin{proof}
	Soit $M_n(t)=N(T_{n-1}+t)-N(T_{n-1})$ le nombre d'événements compris entre
	l'événement $n-1$ et un temps $t$ plus tard. On peut alors définir que
	\begin{equation*}
		\P{T_n>t}
		=\P{M_n(t)=0}
		=\P{N(T_{n-1}+t)-N(T_{n-1})=0}
	\end{equation*}

	Par définition, on a que $N(T_{n-1})=n-1$ de sorte que
	\begin{equation*}
		\P{T_n>t}
		=\P{N(T_{n-1}+t)=n-1}
		=\P{\left\{N(T_{n-1})=n-1\right\}\cap\left\{N(t)=0\right\}}
	\end{equation*}
	Puisque les accroissements dans un processus de Poisson sont indépendants,
	on a
	\begin{equation*}
		\P{T_n>t}
		=\underbrace{\P{N(T_{n-1})=n-1}}_{1}\P{N(t)=0}
		=\P{N(t)=0}
		=\e^{-\lambda t},
	\end{equation*}
	car $N(t)=N(t)-N(0)\sim\Poi{\lambda t}$.
	
	Par conséquent, on a que
	\begin{equation*}
		\P{T_n\leq t}=1-\e^{-\lambda t},
	\end{equation*}
	ce qui est la fonction de répartition d'une distribution exponentielle de
	paramètre $\lambda$.
\end{proof}

\begin{definition}
	Le \textit{temps d'arrivé} $S_n$ d'un événement $n$ est la somme de la
	différence de tous les événements consécutifs antérieurs, c'est-à-dire
	\begin{equation*}
		S_n=\sum_{k=1}^nT_k.
	\end{equation*}
\end{definition}

\begin{theoreme}
	$S_n\sim\Gam{n}{\lambda}$.
\end{theoreme}

\begin{exemple}
	Soit un processus de Poisson de taux $\lambda=2$ avec $t_1=3$, $t_2=7$,
	$t_3=8$ et $t_4=10$. Quelle est la probabilité d'avoir 1 événement dans
	l'intervalle $]t_1,t_2]$ et plus de 1 événement dans l'intervalle
	$]t_3,t_4]$?
	
	Soit $X=N(t_1,t_2)\sim\Poi{\lambda}$ et $Y=N(t_3,t_4)\sim\Poi{\lambda}$. On
	cherche
	\begin{equation*}
		\P{\left\{X=1\right\}\cap\left\{Y\geq 1\right\}}
		=\P{X=1}\P{Y\geq 1}
		=8\e^{-8}\left(1-\e^{-4}\right).
	\end{equation*}
\end{exemple}

\subsection{Marche aléatoire}
\begin{definition}
	Une \textit{marche aléatoire} est un processus stochastique sans mémoire à
	temps discret et état discret dont l'espace des états est $\left\{X_n:n\in
	\mathbb{N}\right\}$ avec $X_n$ la position du processus tel que
	\begin{enumerate}
		\item $\left\rvert X_{n}-X_{n-1}\right\rvert=1$ pour tout
		      $n\in\mathbb{N}^*$
		\item $X_n\in\mathbb{Z}$ pour tout $n\in\mathbb{N}$
	\end{enumerate}
\end{definition}

\begin{remark}
	On définit souvent ce processus comme le mouvement discret d'une particule
	en une dimension.
\end{remark}

\begin{exemple}
	Soit une particule se déplaçant aléatoirement en une dimension. La
	probabilité qu'elle se dirige vers la droite peut importe sa position est
	$\nicefrac{1}{2}$ tandis que celle qu'elle se dirige vers la gauche est
	$\nicefrac{1}{2}$.

	On définit les variables aléatoires $D$ et $G$ comme étant le nombre de
	déplacements vers la droite et la gauche respectivement. On peut montrer
	que
	\begin{equation*}
		D\sim\Bin{n}{\nicefrac{1}{2}}
		\quad\text{et}\quad
		G\sim\Bin{n}{\nicefrac{1}{2}},
	\end{equation*}
	où $n$ est le nombre d'étapes.
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Si un déplacement correspond à une unité entière sur $\mathbb{Z}$, alors la
	position $X_n$ à l'étape $n$ est
	\begin{equation*}
		X_n=D-G=D-(n-D)=2D-n,
	\end{equation*}
	si $X_0=0$, de sorte que
	\begin{equation*}
		p_{0,i}
		=\Pg{X_n=i}{X_0=0}
		=\P{2D-n=i}
		=\P{D=\frac{n+i}{2}},
	\end{equation*}
	soit la probabilité d'être à la position $i$ en $n$ étapes à partir de $0$.
\end{exemple}

\subsection{Processus de Wiener}
\begin{definition}
	Un \textit{processus de Wiener} est un processus stochastique à temps
	continu et état continu dont l'espace des états est $\left\{W(t):t\geq 0
	\right\}$, où
	\begin{enumerate}
		\item $W(0)=0$
		\item $W(t_1,t_2)$ et $W(t_3,t_4)$ sont indépendants si $t_1<t_2\leq
		      t_3<t_4$
		\item $W(t_1,t_2)$ et $W(t_1+\tau,t_2+\tau)$ suivent la même loi pour
		      tout $\tau$ et $t_1<t_2$
		\item $W(t)\sim\Norm{0}{\sigma^2 t}$
	\end{enumerate}
\end{definition}

\begin{remark}
	On peut construire ce processus à partir d'une marche aléatoire de pas
	$\pm\epsilon$ et d'unité de temps $\delta$. On pose alors $\epsilon=\sigma
	\sqrt{\delta}$ afin d'obtenir
	\begin{equation*}
		W(t)=\lim_{\delta\rightarrow 0^+}X(t).
	\end{equation*}
\end{remark}

\begin{remark}
	Les trajectoires de ce processus sont des fonctions continues, mais
	dérivables nulle part. La \textit{dérivée généralisée} de ce processus 
	s'appele le \textit{bruit blanc gaussien}.
\end{remark}

\begin{definition}
	Un \textit{mouvement Brownien standard} est un processus de Wiener où le
	\textit{coefficient de diffusion} est $\sigma^2=1$.
\end{definition}

\begin{exemple}
	Calculer la fonction de densité du 2-ième ordre $f(w_1,w_2;t_1,t_2)$ du
	processus de Wiener. On suppose que $t_1<t_2$.

	Par définition, on a
	\begin{equation*}
		f(w_1,w_2;t_1,t_2)
		=\P{\left\{W(t_1)=w_1\right\}\cap\left\{W(t_2)=w_2\right\}}.
	\end{equation*}
	En écrivant l'expression en terme d'accroissements indépendants, on a
	\begin{equation*}
		\begin{split}
		f(w_1,w_2;t_1,t_2)
		&=\P{\left\{W(t_1)=w_1\right\}\cap\left\{W(t_2)-W(t_1)=w_2-w_1\right\}}
		\\
		&=\P{W(t_1)=w_1}\P{W(t_2)-W(t_1)=w_2-w_1}.
		\end{split}
	\end{equation*}
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Hors, $X=W(t_1)\sim\Norm{0}{\sigma^2t_1}$ et $Y=W(t_2)-W(t_1)\sim\Norm{0}{
	\sigma^2(t_2-t_1)}$ de sorte que
	\begin{equation*}
		f(w_1,w_2;t_1,t_2)
		=f_X(w_1)f_Y(w_2-w_1)
	\end{equation*}
	et
	\begin{equation*}
		f(w_1,w_2;t_1,t_2)
		=\frac{1}{\sqrt{2\pi\sigma^2t_1}}\exp\left\{-
			\frac{(w_1^2)}{2\sigma^2t_1}
		\right\}\cdot\frac{1}{\sqrt{2\pi\sigma^2(t_2-t_1)}}\exp\left\{-
			\frac{(w_2-w_1)^2}{2\sigma^2(t_2-t_1)}
		\right\}.
	\end{equation*}
\end{exemple}

\subsubsection{Fonction d'autocovariance}
\begin{theoreme}
	$C_w(t_1,t_2)=\sigma^2\cdot\min(t_1,t_2)$.
\end{theoreme}

\pagebreak
\section{Statistique descriptive}
\begin{definition}
	Un \textit{échantillion} est un ensemble de données, dénoté $x_1,x_2,\dots,
	x_n$, représentant les résultats d'une expérience repétée $n$ fois.
\end{definition}

\begin{definition}
	Un \textit{échantillion ordonnée} est un ensemble de données, dénoté
	$x_{(1)},x_{(2)},\dots,x_{(n)}$, tel que $x_{(1)}\leq x_{(2)}\leq\cdots\leq
	x_{(n)}$, représentant les résultats d'une expérience répétée $n$ fois.
\end{definition}

\begin{definition}
	Une \textit{population} est l'ensemble des résultats possibles, ou encore
	un modèle théorique, de l'expérience.
\end{definition}

\begin{definition}
	Une \textit{classe} est un sous-ensemble de la population.
\end{definition}

\begin{definition}
	L'\textit{effectif} est le nombre de résultats de l'échantillion observés
	dans chaque classe.
\end{definition}

% TODO: représentation graphique
% - diagramme à points
% - tableau d'effectif
% - histogramme
% - diagramme en boite
% - diagramme chronologique

\subsection{Représentation numérique}
\begin{definition}
	Une \textit{statistique} est une fonction de l'échantillion, soit $g(x_1,
	x_2,\dots,x_n$).
\end{definition}

\subsubsection{Moyenne de l'échantillion}
\begin{definition}
	La \textit{moyenne} $\bar{x}$ de l'échantillion $x_1,x_2,\dots,x_n$ est une
	statistique écrivant la valeur moyenne de l'échantillion. Elle est définie
	par
	\begin{equation*}
		\bar{x}=\frac{1}{n}\sum_{k=1}^nx_k.
	\end{equation*}
\end{definition}

\subsubsection{Médianne de l'échantillion}
\begin{definition}
	La \textit{médiane} $\widetilde{x}$ d'un échantillion ordonnée $x_{(1)},
	x_{(2)},\dots,x_{(n)}$ est une statistique séparant l'échantillion
	ordonnée en deux sous-échantillions ordonnées de même cardinalité. Elle 
	est définie par
	\begin{equation*}
		\widetilde{x}=\left\{
			\begin{matrix}
				x_{(\nicefrac{n+1}{2})} & \text{si} & \text{$n$ est impair},\\
				\dfrac{
					x_{(\nicefrac{n}{2})}+
					x_{(\nicefrac{n}{2}+1)}
				}{2} & \text{si} & \text{$n$ est pair}.\\
			\end{matrix}
		\right.
	\end{equation*}
\end{definition}

\subsubsection{Variance de l'échantillion}
\begin{definition}
	La \textit{variance} $s^2$ d'un échantillion $x_1,x_2,\dots,x_n$ est une
	statistique décrivant la dispersion des données. Elle est définie par
	\begin{equation*}
		s^2
		=\frac{1}{n-1}\sum_{k=1}^n\left(x_k-\bar{x}\right)^2
		=\frac{1}{n-1}\left(\sum_{k=1}^nx_k^2-n\bar{x}^2\right).
	\end{equation*}
\end{definition}

\subsubsection{Étendue de l'échantillion}
\begin{definition}
	L'\textit{étendue} $R$ d'un échantillion ordonnée $x_{(1)},x_{(2)},\dots,
	x_{(n)}$ est une statistique décrivant l'étendue des données. Elle est
	définie par
	\begin{equation*}
		R=x_{(n)}-x_{(1)}.
	\end{equation*}
\end{definition}

\pagebreak
\section{Inférence statistique}
On suppose que la population est décrite par la variable aléatoire $X$ idéale.
On a alors un échantillion aléatoire $X_1,X_2,\dots,X_n$ telle que $X_k$ sont
des variables aléatoires indépendantes et identiquements distribuées.

On cherche à \textit{estimer} les paramètre inconnues $\theta_1,\theta_2,\dots,
\theta_m$ de la loi de $X$. La statistique $T=g(X_1,X_2,\dots,x_n)$ est aussi
une variable aléatoire. En général, sa loi dépend de $\theta_1,\theta_2,\dots,\theta_m$.

\begin{exemple}
	On suppose que $X$ suit une loi quelconque de moyenne $\mu=\Esp{X}$ et
	$\sigma^2=\Var{X}$. Les paramètres de la loi sont $\mu$ et $\sigma^2$.

	On cherche un estimateur de $\mu$ tel que
	\begin{equation*}
		\overline{X}
		=\frac{1}{n}\sum_{k=1}^nX_k.
	\end{equation*}
	Par le théorème central limite, on a que
	\begin{equation*}
		\overline{X}
		\approx\Norm{\mu}{\frac{\sigma^2}{n}}.
	\end{equation*}
	Pour un échantillion particulier $x_1,x_2,\dots,x_n$, $\bar{x}$ est une
	estimation partielle de $\mu$.

	La loi $\Norm{\mu}{\sigma^2/n}$ pour $\overline{X}$ est dite exacte si on
	suppose que $X\sim\Norm{\mu}{\sigma^2/n}$.
\end{exemple}

\begin{exemple}
	Soit $X\sim\Uni{0}{\theta}$.

	On peut estimer $\theta$ avec la statistique $X_{(n)}=\max{X_1,X_2,\dots,
	X_n}$.
	\begin{equation*}
		\P{X_{(n)}\leq x}
		=\P{
			\left\{X_1\leq x\right\}\cap
			\left\{X_2\leq x\right\}\cap
			\cdots\cap
			\left\{X_n\leq x\right\}\cap}
		=\left(\P{X\leq x}\right)^n
	\end{equation*}

	\begin{equation*}
		\P{X_{(n)}\leq x}
		=\left\{
			\begin{matrix}
				0 & \text{si} & x<0,\\
				\left(\dfrac{x}{\theta}\right)^n 
				  & \text{si} & 0\leq x\leq\theta,\\
				1 & \text{si} & \theta<x.\\
			\end{matrix}
		\right.
	\end{equation*}
\end{exemple}

\begin{definition}
	Un \textit{estimateur} d'un paramètre inconnu $\theta$ est une statistique
	qui correspond à $\theta$.
\end{definition}

\begin{remark}
	Si $X\sim\Norm{\mu}{\sigma^2}$, alors $\bar{X}$ est un estimateur de $\mu$
	et $S^2$ est un estimateur de $\sigma^2$.

	Pour une somme de $X$ $n$ fois, on trouve que
	\begin{equation*}
		\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\sim\Norm{0}{1}.
	\end{equation*}
	et aussi
	\begin{equation*}
		\frac{(n-1)S^2}{\sigma^2}\sim\Gam{\frac{n-1}{2}}{\frac{1}{2}},
	\end{equation*}
	une loi du \textit{khi-deux}. On peut montrer que ces deux variables
	aléatoires sont indépendantes.
\end{remark}

\subsection{Biais d'un estimateur}
\begin{definition}
	Le biais d'un estimateur $T$, dénoté $\Biais{T}$, est la différence entre
	l'espérance de $T$ et la valeur $\theta$ qu'il est sensé estimer,
	c'est-à-dire
	\begin{equation*}
		\Biais{T}=\Esp{T}-\theta.
	\end{equation*}
\end{definition}

\begin{property}
	Si $\Esp{T}=\theta$, alors $T$ est sans biais.
\end{property}

\subsection{Erreur quadratique moyenne}
\begin{definition}
	L'\textit{erreur quadratique moyenne} d'un estimateur $T$ de $\theta$,
	dénotée $\EQM{T}$ est une mesure de la précision de l'estimateur, définie
	par
	\begin{equation*}
		\EQM{T}=\Esp{\left(T-\theta\right)^2}.
	\end{equation*}
\end{definition}

\begin{property}
	$\EQM{T}=\Var{T}+\left(\Biais{T}\right)^2$.
\end{property}

\begin{definition}
	Soient deux estimateurs $T_1$ et $T_2$ de $\theta$. On dit que $T_1$ est
	\textit{meilleur} que $T_2$ si
	\begin{equation*}
		\EQM{T_1}<\EQM{T_2}.
	\end{equation*}
\end{definition}

\begin{exemple}
	Soit $X\sim\Uni{0}{\theta}$ et $X_{(n)}$ est un estimateur de $\theta$. On
	a montré que
	\begin{equation*}
		\P{X_{(n)}\leq x}=\left(\frac{x}{\theta}\right)^n
	\end{equation*}
	pour $0\leq x\leq\theta$.

	On calcul le biais. On a premièrement
	\begin{equation*}
		f_{X_{(n)}}=\frac{nx^{n-1}}{\theta^n}
		\quad\text{pour}\quad 0\leq x\leq\theta.
	\end{equation*}
	Par conséquent,
	\begin{equation*}
		\Esp{X_{(n)}}
		=\int_0^\theta x\cdot\frac{nx^{n-1}}{\theta^n}\d{x}
		=\frac{n}{\theta^n}\cdot\frac{\theta^{n+1}}{n+1}
		=\frac{n}{n+1}\theta.
	\end{equation*}
	et
	\begin{equation*}
		\Biais{X_{(n)}}
		=\frac{n}{n+1}\theta-\theta
		=-\frac{1}{n+1}\theta.
	\end{equation*}

	On peut définir
	\begin{equation*}
		\hat{\theta}=\frac{n+1}{\theta}X_{(n)}
	\end{equation*}
	comme étant un estimateur sans biais de $\theta$, car 
	$\Biais{\hat{\theta}}=0$.
\end{exemple}

\begin{exemple}
	Soit $X\sim\Norm{\mu}{\sigma^2}$. Comparer l'erreur quadratique moyenne
	des estimateurs de $\sigma^2$ suivant
	\begin{equation*}
		T_1
		=S^2
		=\frac{1}{n-1}\sum_{k=1}^n\left(X_k-\bar{X}\right)^2
	\end{equation*}
	et
	\begin{equation*}
		T_2
		=\frac{n-1}{n}S^2
		=\frac{1}{n}\sum_{k=1}^n\left(X_k-\bar{X}\right)^2.
	\end{equation*}
	On sait que
	\begin{equation*}
		\frac{(n-1)S^2}{\sigma^2}\sim\Gam{\frac{n-1}{2}}{\frac{1}{2}}.
	\end{equation*}

	Premièrement, on a
	\begin{equation*}
		\Esp{S^2}
		=\frac{\sigma^2}{n-1}\Esp{\frac{(n-1)S^2}{\sigma^2}}
		=\frac{\sigma^2}{n-1}\cdot\frac{(n-1)/2}{1/2}
		=\sigma^2
	\end{equation*}
	et
	\begin{equation*}
		\Var{S^2}
		=\frac{\sigma^2}{(n-1)^2}\Var{\frac{(n-1)S^2}{\sigma^2}}
		=\frac{\sigma^4}{(n-1)^2}\cdot\frac{(n-1)/2}{(1/2)^2}
		=\frac{2\sigma^4}{n-1},
	\end{equation*}
	de sorte que
	\begin{equation*}
		\EQM{T_1}
		=\frac{2\sigma^4}{n-1}+\left(\sigma^2-\sigma^2\right)^2
		=\frac{2\sigma^4}{n-1}.
	\end{equation*}

	Deuxièment, on a
	\begin{equation*}
		\Esp{\frac{n-1}{n}S^2}
		=\frac{n-1}{n}\sigma^2
	\end{equation*}
	et
	\begin{equation*}
		\Var{\frac{n-1}{n}S^2}
		=\left(\frac{n-1}{n}\right)^2\Var{S^2}
		=\left(\frac{n-1}{n}\right)^2\cdot\frac{2\sigma^4}{n-1},
	\end{equation*}
	de sorte que
	\begin{equation*}
		\EQM{T_2}
		=\frac{n-1}{n}\cdot\frac{\sigma^4}{n}+\left(\frac{n-1}{n}\sigma^2
			-\sigma^2\right)^2
		=\frac{2n-1}{2n}\cdot\frac{2\sigma^4}{n}.
	\end{equation*}

	On trouve que $\EQM{T_2}<\EQM{T_1}$.
\end{exemple}

\subsection{Recherche d'un estimateur}
Soit une population $X$ avec $f_X(x_j;\theta)$ et l'échantillion $X_1,X_2,\dots
X_n$.

\subsubsection{Méthode du maximum de vraisemblance}
\begin{enumerate}
	\item On calcul la \textit{fonction de vraisemblance} $L(\theta)$, soit
	\begin{equation*}
		L(\theta)=\prod_{k=1}^nf_X(x_k;\theta).
	\end{equation*}

	\item Pour maximiser $\L(\theta)$, on utilise
	\begin{equation*}
		\ln{L(\theta)}.
	\end{equation*}

	\item On pose
	\begin{equation*}
		\frac{\d{}}{\d{\theta}}\ln{L(\theta)}=0.
	\end{equation*}

	\item On isole $\theta$.
\end{enumerate}
On note l'expression trouvée $\theta_{\text{VM}}$ comme étant l'\textit{
estimateur à vraisemblance maximale} de $\theta$.

\begin{exemple}
	Soit $X\sim\Exp{\theta}$, où $\theta\equiv\lambda$.
	\begin{enumerate}
		\item $
			L(\theta)
			=\displaystyle\prod_{k=1}^nf_X(x_k;\theta)
			=\theta^n\exp\left\{-\theta\displaystyle\sum_{k=1}^nx_k\right\}
			=\theta^n\e^{-\theta n\bar{X}}.
		$

		\item $
			\ln{L(\theta)}
			=n\ln{\theta}-\theta n\bar{X}.
		$

		\item $
			\dfrac{\d{}}{\d{\theta}}\ln{L(\theta)}
			=\dfrac{n}{\theta}-n\bar{X}
			=0
		$

		\item $
			\theta_{\text{VM}}=\dfrac{1}{\bar{X}}
		$
	\end{enumerate}
\end{exemple}

\begin{exemple}
	Soit $X\sim\Poi{\theta}$ où $\theta\equiv\alpha$.

	\begin{enumerate}
		\item $
			L(\theta)
			=\displaystyle\prod_{k=1}^np_X(x_k;\theta)
			=\dfrac{\e^{-\theta}\e^{x_1}}{x_1!}\cdot
			 \dfrac{\e^{-\theta}\e^{x_2}}{x_2!}\cdots
			 \dfrac{\e^{-\theta}\e^{x_n}}{x_n!}
			=\dfrac{\e^{n\theta}\theta^{n\bar{X}}}{x_1!x_2!\cdots x_n!}
		$
		\item $
			\ln{L(\theta)}
			=-n\theta+n\bar{X}\ln{\theta}-\displaystyle\sum_{k=1}^n\ln{x_k!}
		$
		\item $
			\dfrac{\d{}}{\d{\theta}}\ln{L(\theta)}
			=-n+\dfrac{n\bar{X}}{\theta}
			=0
		$
		\item $
			\theta_{\text{VM}}=\bar{X}
		$
	\end{enumerate}
\end{exemple}

\begin{exemple}
	Soit $X\sim\Uni{0}{\theta}$.

	\begin{enumerate}
		\item $
			L(\theta)
			=\left\{
				\begin{matrix}
					\dfrac{1}{\theta^n} & \text{si} & 0\leq X_k\leq\theta,\\
					0                   & \text{sinon}.
				\end{matrix}
			\right.
		$

		\item $
			\theta_{\text{VM}}=X_{(n)}
		$
	\end{enumerate}
\end{exemple}

\subsubsection{Méthode des moments}
\begin{enumerate}
	\item On pose que
	\begin{equation*}
		\Esp{X^m}=\frac{1}{n}\sum_{k=1}^nX_k^m.
	\end{equation*}

	\item En commençant par $m=1$, on cherche la première équation où
	$\Esp{X^m}=h(\theta)$.

	\item On isole $\theta$. On note $\theta_{\text{N}}$ l'expression trouvée.
\end{enumerate}

\begin{exemple}
	Soit $X\sim\Poi{\theta}$, où $\theta\equiv\alpha$.

	\begin{enumerate}
		\item $
			\Esp{X}=\bar{X}
		$

		\item $
			\theta_{\text{M}}=\bar{X}
		$
	\end{enumerate}
\end{exemple}

\begin{exemple}
	Soit $X\sim\Norm{0}{\theta^2}$, où $\theta\equiv\sigma$.

	\begin{enumerate}
		\item $
			\Esp{X^2}
			=\dfrac{1}{n}\displaystyle\sum_{k=1}^nX_k^2
			=\theta^2
		$

		\item $
			\theta_{\text{M}}
			=\sqrt{\dfrac{1}{n}\displaystyle\sum_{k=1}^nX_k^2}
		$
	\end{enumerate}
\end{exemple}

\begin{exemple}
	Soit $X\sim\Uni{0}{\theta}$.

	\begin{enumerate}
		\item $
			\Esp{X}
			=\bar{X}
			=\dfrac{\theta}{2}
		$

		\item $
			\theta_{\text{M}}
			=2\bar{X}
		$
	\end{enumerate}
\end{exemple}

\subsection{Intervalle de confiance}
\begin{exemple}
	Un premier sondage révèle que $p_1=45\pm 3\%$ des personnes sont en faveur
	d'un événement, tandis qu'un deuxième sondage révèle $p_2=51\pm 10\%$ des
	personnes sont en faveur.
\end{exemple}

\begin{exemple}
	Soit $X$ une population avec un paramètre inconnu $\theta$. On définit 
	$[\mathrm{LI},\mathrm{LS}]$, où $\mathrm{LI}$ et $\mathrm{LS}$ sont les
	limites inférieure et supérieure respectivement, comme un intervalle de
	confiance à $100(1-\alpha)\%$ pour $\theta$ si
	\begin{equation*}
		\P{\mathrm{LI}\leq\theta\leq\mathrm{LS}}=1-\alpha
	\end{equation*}
	On a que $\mathrm{LI}$ et $\mathrm{LS}$ sont des probabilités.
	
	On cherche l'intervalle de confiance pour une moyenne $\mu$ d'un loi
	$\Norm{\mu}{\sigma^2}$ avec $\sigma^2$ connue. Par conséquent, on sait que
	\begin{equation*}
		\bar{X}\sim\Norm{\mu}{\frac{\sigma^2}{n}}
	\end{equation*}
	et
	\begin{equation*}
		Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\sim\Norm{0}{1}.
	\end{equation*}

	On définit $z_\alpha=\P{Z\leq z_\alpha}=\alpha$. On a alors 
	\begin{equation*}
	\begin{split}
		\P{-z_{\alpha/2}\leq Z\leq z_{\alpha/2}}
		&=\P{z_{\alpha/2}\leq\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2}}\\
		&=\P{
		 \underbrace{\bar{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}}_{\mathrm{LI}}
		 \leq\mu\leq
		 \underbrace{\bar{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}}_{\mathrm{LS}}
		 }\\
		&=1-\alpha
	\end{split}
	\end{equation*}
	Par conséquent, la formule pour l'intervalle de confiance à
	$100(1-\alpha)\%$ est
	\begin{equation*}
		\bar{X}\pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}.
	\end{equation*}
\end{exemple}

\begin{remark}
	Si $X$ ne suit pas une loi normale, mais que $\Var{X}$ est tout de même
	connue, alors la formule trouvée reste valide si $n$ est grand (T.C.L.).
\end{remark}

\begin{remark}
	Si $n$ est grand et si $\mu$ est le seul paramètre de $X$, alors on obtient
	l'intervalle de confiance \textit{approximatif} suivant
	\begin{equation*}
		\bar{X}\pm z_{\alpha/2}\frac{\sqrt{h(\bar{X})}}{\sqrt{n}},
	\end{equation*}
	où $h(\bar{X})=\Var{X}=h(\mu)$.
\end{remark}

\begin{exemple}
	Soit $X\sim\Norm{\mu}{1}$ avec $x_n\in\left\{2.3,1.8,2.0,1.7,1.4,2.2
	\right\}$. Calculer l'intervalle de confiance pour $\mu$ avec un niveau
	de confiance de $95\%$.

	On a que $\bar{x}=1.9$ et $\alpha=0.025$. Par conséquent, les limites sont
	données par
	\begin{equation*}
		1.9\pm z_{0.025}\frac{1}{\sqrt{6}}
	\end{equation*}
	de sorte que $\mathrm{LI}=1.0998$ et $\mathrm{LS}=2.7002$.
	
	La moyenne $\mu$ est comprise dans l'intervalle $[1.0998,2.7002]$ au niveau
	de confiance de $95\%$.
\end{exemple}

\subsection{\boldmath moyenne $\mu$ si $\Norm{\mu}{\sigma^2}$ et $\sigma^2$ inconnue}
Si $n\geq 30$ (grand), alors on remplace $\sigma$ par $S$. Les bornes sont donc
données par
\begin{equation*}
	\bar{X}\pm z_{\alpha/2}\frac{S}{\sqrt{n}}
\end{equation*}
(aussi valide si $X$ n'est pas normale)

Si $n<30$ (petit), on a
\begin{equation*}
	T=\frac{\bar{X}-\mu}{S/\sqrt{n}}\sim t_{n-1}
\end{equation*}
(loi de Student à $n-1$ degrés de liberté)

On définit $t_{\alpha,n}$ par $\P{T>t_{\alpha,n}}=\alpha$. L'intervalle de
confiance à $100(a-\alpha)\%$ pour $\mu$ est
\begin{equation*}
	\bar{X}\pm t_{\alpha/2,n-1}\frac{S}{\sqrt{n}}.
\end{equation*}

\begin{exemple}
	Soit $X\sim\Norm{\mu}{\sigma^2}$ avec $x_n\in\left\{2.3,1.8,2.0,1.7,1.4,2.2
	\right\}$. Calculer l'intervalle de confiance pour $mu$.

	On a que $\bar{x}=1.9$ et $S=0.3347$. Par conséquent, l'intervalle de
	confiance à $95\%$ est
	\begin{equation*}
		1.9\pm t_{0.025,5}\frac{0.3347}{\sqrt{6}}
	\end{equation*}
	et $\mathrm{LI}=1.5487$ et $\mathrm{LS}=2.2513$.
\end{exemple}

\begin{remark}
	Un intervalle de confiance unilatérale est un intervalle de la forme
	$[\mathrm{LI},\infty[$ et $]-\infty,\mathrm{LS}]$. On réutilise les
	formules de $\mathrm{LI}$ et $\mathrm{LS}$ en remplçant $z_{\alpha/2}$
	par $z_\alpha$ ou $t_{\alpha/2,n-1}$ par $t_{\alpha,n-1}$.
\end{remark}

\subsection{Test d'ajustement du \boldmath $\chi^2$}
On cherche à valider ou rejeter un modèle proposé pour $X$. On teste
l'\textit{hypothèse nulle} $H_0:f_X(x)=f_0(x)$, où $f_X(x)$ est la densité
inconnue et $f_0(x)$ la densité proposée, versus la \textit{contre-hypothèse}
$H_1:f_X(x)\neq f_0(x)$.

\begin{enumerate}
	\item On divise $S_X$ en $k$ classes.
	\item On prélève un échantillon de taille $n$ de $X$.
	\item On calcule la statistique 
		  \begin{equation*}
		  	  D^2=\displaystyle\sum_{j=1}^k\dfrac{\left(n_j-m_j\right)^2}{m_j},
		  \end{equation*}
	      où $n_j$ est l'effectif observé dans la classe $j$ et $m_j$ est
		  l'effectif espéré dans la classe $j$ en supposant $H_0$ comme étant
		  vraie.
	\item On rejète $H_0$ au seuil de signification $\alpha$ si et seulement si
	      \begin{equation*}
	          D^2>\chi^2_{\alpha,k-r-1},
		  \end{equation*}
		  où $r$ est le nombre de paramètres estimés pour $f_0(x)$. Dans le cas
		  contraire, on dit qu'on \textit{ne rejette pas} $H_0$.
\end{enumerate}

\begin{remark}
	On a $m_j=n\underbrace{\Pg{X\in C_j}{H_o\text{ vraie}}}_{\text{calcul fait
	avec $f_0$}}$, où $C_j$ est la classe $j$.
\end{remark}

\begin{remark}
	Il faut que $m_j\geq 5$ pour tout $j$. Si ce n'est pas le cas, on peut
	regrouper des classes adjacentes ($k$ diminue).
\end{remark}

\begin{exemple}
	Soit un test de $H_0:X\sim\Uni{0}{5}$ contre $H_1:X\nsim\Uni{0}{5}$ avec le
	tableau d'effectifs suivant et $n=80$.
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccc}
			\toprule
			$j$   & $1$     & $2$     & $3$\\
                  & $[0,1]$ & $]1,2]$ & $]2,5]$\\
			\midrule
			$n_j$ & $20$    & $23$    & $37$\\
			\bottomrule
		\end{tabular}
	\end{table}

	On a $m_1=16$, $m_2=16$ et $m_3=48$. Par conséquent, $D^2\approx 6.58$. On
	compare $D^2$ à $\chi^2_{0.05,3-0-1}=\chi^2_{0.05,2}\approx 5.99$. Par
	conséquent, on a $D^2>\chi^2$ et on rejette $H_0$.
\end{exemple}

\begin{exemple}
	Soit un test de $H_0:X\sim\Bin{3}{p}$ contre $H_1:X\nsim\Bin{3}{p}$ au
	seuil $\alpha=0.05$ à partir du tableau d'effectif suivant et $n=100$.
	\begin{table}[H]
		\centering
		\begin{tabular}{c|cccc}
			\toprule
			$j$   & $0$  & $1$  & $2$  & $3$\\
			\midrule
			$n_j$ & $33$ & $36$ & $24$ & $7$\\
			\bottomrule
		\end{tabular}
	\end{table}

	On estime $p$ par la méthode des moments, soit
	\begin{equation*}
		\Esp{X}=3p=\bar{X}\Leftrightarrow p=0.35.
	\end{equation*}
	On a $m_0\approx 27.46$, $m_1\approx 44.36$, $m_2\approx 23.89$ et
	$m_3\approx 4.29$. Puisque $m_3<5$, alors on regroupe les classes $2$ et
	$3$. On obtient alors le tableau d'effectif suivant.
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccc}
			\toprule
			$j$   & $0$     & $1$     & $\left\{2,3\right\}$\\
			\midrule
			$n_j$ & $33$    & $36$    & $31$\\
			$m_j$ & $27.46$ & $44.36$ & $28.18$\\
			\bottomrule
		\end{tabular}
	\end{table}

	Par conséquent, $D^2\approx 2.97$. On compare $D^2$ à $\chi^2_{0.05,3-1-1}=
	\chi^2_{0.05,1}\approx 3.84$. Puisque $D^2<\chi^2_{0.05,1}$, alors on ne
	rejette pas $H_0$.
\end{exemple}

\subsection{Tests d'hypothèses}
Soit $H_0$ et $H_1$ des hypothèses au sujet de $\mu/\mu_1/\mu_2$ ou $\sigma^2/
\sigma_1^2/\sigma_2^2$. En général, on pose un énoncé que l'on croit \textit{%
faux} comme $H_0$.

\begin{definition}
	L'\textit{erreur de première espèce} $\alpha$ est la probabilité de
	rejeter $H_0$ sachant que $H_0$ est vrai, c'est-à-dire
	\begin{equation*}
		\alpha=\Pg{\text{rejeter $H_0$}}{\text{$H_0$ est vrai}}.
	\end{equation*}
\end{definition}

\begin{remark}
	En général, $\alpha$ est une valeur fixée par l'expérimentateur.
\end{remark}

\begin{definition}
	L'\textit{erreur de deuxième espèce} $\beta$ est la probabilité d'accepter
	$H_0$ sachant que $H_0$ est fausse, c'est-à-dire
	\begin{equation*}
		\beta=\Pg{\text{accepter $H_0$}}{\text{$H_0$ est fausse}}.
	\end{equation*}
\end{definition}

\begin{remark}
	En général, $\beta$ dépend de $\alpha$, de $n$ et d'une contre-hypothèse
	particulière.
\end{remark}

\begin{definition}
	On définit la \textit{puissance du test} comme $1-\beta$.
\end{definition}

Puisque $\alpha$ est fixée à l'avance, on dit que <<rejeter $H_0$>> est une
\textit{conclusion forte}. D'une manière similaire, puisque $\beta$ peut être
élevée, on dit que <<ne pas rejeter $H_0$>> est une \textit{conclusion faible}.

\begin{remark}
	Dans le cas d'une seule population, on suppose que $X\sim\Norm{\mu}
	{\sigma^2}$. Dans le cas de deux populations, on suppose que $X_1\sim\Norm
	{\mu_1}{\sigma_1^2}$ et $X_2\sim\Norm{\mu_2}{\sigma_2^2}$.
\end{remark}

\begin{exemple}
	Soit le test d'une moyenne et variance inconnue avec l'échantillon de $n=8$
	notes d'examens :
	\begin{equation*}
		\left\{75\%,72\%,64\%,81\%,79\%,65\%,82\%,85\%\right\}
	\end{equation*}
	On suppose que $X\sim\Norm{\mu}{\sigma^2}$ comme population. Peut-on
	affirmer que la moyenne $\mu$ est au-dessus de $75\%$? ($\alpha=0.05$)

	On remarque que l'unité de mesure est sans conséquent pour le test. Soit
	$H_0:\mu=75$ et $H_1:\mu>75$, une conclusion forte. On a donc
	\begin{equation*}
		t_0=\frac{\bar{X}-75}{S/\sqrt{8}}\approx 0.135
	\end{equation*}
	et
	\begin{equation*}
		t_{\alpha,n-1}=t_{0.05,7}\approx 1.835.
	\end{equation*}

	Par conséquent, on rejette si $t_0>t_{\alpha,n-1}$. Puisque
	$0.135\leq 1.835$, on ne rejette pas $H_0$, car on ne peut pas affirmer que
	$\mu>75$.
\end{exemple}

\end{document}

