\documentclass[11pt]{article}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{standalone}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{siunitx}
\usepackage{gphys}
\usepackage{thmtools}
\usepackage{mdframed}
\usepackage{float}
\usepackage{tikz}
\usepackage{contour}
\usepackage{scrextend}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{pgfplots}

\usetikzlibrary{babel}
\usetikzlibrary{calc}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{decorations.markings}

%%%%%%%%%%%%%%%%%%
% Configurations %
%%%%%%%%%%%%%%%%%%

\geometry{
	top=34mm,
	bottom=34mm
}

%%%%%%%%%%
% Macros %
%%%%%%%%%%

\input{eq}

\newcommand\card{%
	\ensuremath{%
		\mathrm{card}\,%
	}%
}%

\newcommand\comb[2]{%
	\ensuremath{%
		\mathcal{C}_{#2}^{#1}%
	}%
}%

\newcommand\perm[2]{%
	\ensuremath{%
		\mathcal{P}_{#2}^{#1}%
	}%
}%

\renewcommand\P{%
	\ensuremath{%
		\mathbb{P}%
	}%
}%

\newcommand\e{%
	\ensuremath{%
		\mathrm{e}%
	}%
}%

\newcommand\bin{%
	\ensuremath{%
		\mathcal{B}%
	}%
}%

\newcommand\geo{%
	\ensuremath{%
		\mathcal{G}%
	}%
}%

\newcommand\poi{%
	\ensuremath{%
		\mathrm{Poi}%
	}%
}%

\newcommand\uni{%
	\ensuremath{%
		\mathcal{U}%
	}%
}%

\newcommand\Exp{%
	\ensuremath{%
		\mathrm{Exp}%
	}%
}%

\newcommand\Gam{%
	\ensuremath{%
		\mathrm{Gam}%
	}%
}%

\newcommand\Norm{%
	\ensuremath{%
		\mathcal{N}%
	}%
}%

\newcommand\Esp{%
	\ensuremath{%
		\mathrm{E}%
	}%
}%

\newcommand\Var{%
	\ensuremath{%
		\mathrm{Var}%
	}%
}%

\newcommand\Std{%
	\ensuremath{%
		\mathrm{Std}%
	}%
}%

\newtheorem{axiome}{Axiome}
\newtheorem{theoreme}{Théoreme}[section]

\declaretheoremstyle[
	notefont=\bfseries,
	notebraces={}{},
	bodyfont=\normalfont,
	postheadspace=0.5em,
	numbered=no,
]{basicstyle}
\declaretheorem[name=Définition,style=basicstyle]{definition}

\newmdtheoremenv[
	linecolor=black,
	backgroundcolor=gray!40,
	ntheorem
]{exemple}{Exemple}[section]

%%%%%%%%%%%%%
% Documents %
%%%%%%%%%%%%%

\begin{document}
\tableofcontents
\pagebreak

\section{Élèments de probabilités}
\begin{definition}
	Une expérience est \textit{aléatoire} si un observateur peut la répéter
	dans les mêmes conditions, mais sans pouvoir en prédire le résultat.
\end{definition}

\begin{definition}
	Un \textit{espace échantilion} est un ensemble $S$ des résultats possibles.
\end{definition}

Un espace d'échantilion peut être \textit{qualitatif} ou \textit{quantitatif},
ainsi que \textit{discret}, \textit{continu} ou \textit{mixte}. Il peut aussi
être \textit{dénombrable} ou \textit{non-dénombrable}.

\begin{definition}
	Un \textit{évènement} $A$ est un sous-ensemble de $S$ d'intêret à
	l'observateur.
\end{definition}

\begin{definition}
	Un \textit{évènement élémentaire} $A$ est un résultat particulier,
	c'est-à-dire, un élèment de $S$.
\end{definition}

La différence entre les deux dernières définitions est que la $\card(A)=1$ pour
un évènement élémentaire tandis que $\card(A)\geq 1$ pour un évènement.

\begin{exemple}
	On observe le résultat du lancer de deux pièces de monnaie. On note $P$
	comme un lancer pile et $F$ comme un lancer face. L'ensemble est donc
	\begin{equation*}
		S=\left\{PP, FF, PF, FP\right\}
	\end{equation*}
	avec chaque résultat ayant \SI{25}{\percent} d'arriver. L'ensemble $S$ est
	qualitatif, soit pile ou face, et discret.
\end{exemple}

\begin{exemple}
	On observer la somme obtenue lors du lancer de deux dés à 6 faces.
	L'ensemble de résultat possible est
	\begin{equation*}
		S=\left\{2,3,4,5,6,7,8,9,10,11,12\right\}.
	\end{equation*}
	Il est quantitatif et discret.
\end{exemple}

\begin{exemple}
	On compte le nombre de lancers d'une pièce pour obtenir une première fois
	un pile. L'espace échantilion est
	\begin{equation*}
		S=\left\{1,2,3,4,5,\dots\right\},
	\end{equation*}
	car il est possible qu'une grande quantité de lancer est effectuée avant
	d'obtenir un pile. L'ensemble $S$ est quantitatif, discret et
	infini dénombrable.
\end{exemple}

\pagebreak
\begin{exemple}
	On mesure le temps d'attente à l'arrêt d'autobus. L'espace échantilion est
	\begin{equation*}
		S=\left[0,\infty\right[.
	\end{equation*}
	L'ensemble $S$ est quantitatif, continu et infini non-dénombrable.
\end{exemple}

\begin{exemple}
	On mesure le temps d'attente à l'arrêt d'autobus ainsi que le nombre de
	personnes en file à l'arrivée de l'autobus. L'espace échantilion est
	\begin{equation*}
		S=T\times U,
	\end{equation*}
	avec
	\begin{equation*}
		T=\left[0,\infty\right[
	\end{equation*}
	et
	\begin{equation*}
		U=\left\{1,2,3,4,5,\dots\right\}.
	\end{equation*}
	L'ensemble $S$ est quantitatif et mixte. Un exemple d'évènement élèmentaire
	peut être un couple tel que $\left(\SI{4.25}{\second}, 4\right)$.
\end{exemple}

\subsection{Lien entre l'expérience aléatoire et son modèle}
\begin{definition}
	La \textit{fréquence relative} $f_A$ d'un évènement $A$ est le rapport
	entre le nombre d'observations $n_A$ de l'évèmenent et le nombre $n$ de
	répétition de l'expérience, c'est-à-dire
	\begin{equation*}
		f_A=\frac{n_A}{n}.
	\end{equation*}
	La limite lorsque l'expérience est répétée infiniment est la probabilité de
	l'évènement $A$, dénotée
	\begin{equation*}
		\P(A)=\lim_{n\rightarrow\infty}f_A.
	\end{equation*}
\end{definition}

\subsection{Opérations sur les ensembles}
Soit deux ensembles $A$ et $B$ tel que $A,B\subset S$. La figure
\ref{fig:venn_intersection} montre une intersection tandis que la figure
\ref{fig:venn_union} montre une union entre $A$ et $B$. La figure
\ref{fig:venn_complement} montre le complémenet de $A$ et la figure
\ref{fig:venn_exclusion} montre l'exclusion de deux ensembles.

\begin{figure}[H]
	\centering
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/intersection}
		\caption{$A\cap B$}
		\label{fig:venn_intersection}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/union}
		\caption{$A\cup B$}
		\label{fig:venn_union}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/complement}
		\caption{$A^c$}
		\label{fig:venn_complement}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/combinaison}
		\caption{$\left(A\cup B\right)\setminus\left(A\cap B\right)$}
		\label{fig:venn_exclusion}
	\end{subfigure}
	\caption{Opérations d'ensembles démontrées sur des diagrammes de Venn}
\end{figure}

\subsection{Axiomes fondamentales de la probabilité}
\begin{axiome}
	La probabilité d'un évèment $A$ est plus grand ou égal à 0, c'est-à-dire
	\begin{equation*}
		\P(A)\geq 0,
	\end{equation*}
	pour tout $A\in S$.
\end{axiome}

\begin{axiome}
	La probabilité de l'espace d'échantilion $S$ est 1, c'est-à-dire
	\begin{equation*}
		\P\left(S\right)=1.
	\end{equation*}
\end{axiome}

\begin{axiome}
	La probabilité d'un évènement $A$ ou d'un évènement $B$ est équivalent à la
	somme de leur probabilité, c'est-à-dire
	\begin{equation*}
		\P(A\cup B)=\P(A)+\P(B),
	\end{equation*}
	si $A\cup B=\emptyset$.

	En général, 
	\begin{equation*}
		\P\left(\bigcup_{k=1}^\infty A_k\right)=\sum_{k=1}^\infty \P\left(A_k\right),
	\end{equation*}
	si $A_i\cup A_j=\emptyset$, $\forall i,j$.
\end{axiome}

\begin{theoreme}
	$\P(A^c)=1-\P(A)$.
\end{theoreme}

\begin{proof}
	On sait que $A\cup A^c=S$ et $A\cap A^c=\emptyset$. Hors,
	$\P(A\cup A^c)=\P(S)\Leftrightarrow \P(A)+\P(A^c)=1$, car
	$A\cup A^c=\emptyset$.
	En réarrangeant, on obtient que $\P(A^c)=1-\P(A).$
\end{proof}

\begin{theoreme}
	$\P(A)\leq 1$.
\end{theoreme}

\begin{proof}
	On sait que $\P(A^c)\geq 0$ et $\P(A^c)=1-\P(A)$. En réarrangeant, on
	obtient que $\P(A)\leq 1$.
\end{proof}

\begin{theoreme}
	$\P(\emptyset)=0$.
\end{theoreme}

\begin{proof}
	On sait que $S^c=\emptyset$. Par conséquent, $\P(\emptyset)=1-\P(S)=1-1=0$.
\end{proof}

\begin{theoreme}
	$A\subset B\Rightarrow \P(A)\leq \P(B)$.
\end{theoreme}

\begin{proof}
	La différence entre $A$ et $B$ est $A^c\cap B$ de sorte qu'on peut écrire
	$B=A\cup(A^c\cap B)$. Par conséquent, $\P(B)=\P(A)+\P(A^c\cap B)
	\Leftrightarrow \P(B)-\P(A)=\P(A^c\cap B)\geq 0$, car
	$A\cup(A^c\cap B)=\emptyset$. En réarrangeant, on obtient $\P(A)\leq \P(B)$.
\end{proof}

\begin{theoreme}
	$\P(A\cup B)=\P(A)+\P(B)-\P(A\cap B)$.
\end{theoreme}

\subsection{Principe d'équiprobabilité}
\subsubsection{Ensemble fini}
On suppose que $S$ est fini, soit $S=\left\{e_1,e_2,\dots,e_n\right\}$. On dit
que les résultats d'une expérience aléatoire sont \textit{équiprobables} si
\begin{equation*}
	\P(e_1)=\P(e_2)=\cdots=\P(e_n)=\frac{1}{n}.
\end{equation*}
Dans ce cas, on a que la probabilité d'un évènement $A\subset S$ est
\begin{equation*}
	\P(A)=\frac{n_A}{n},
\end{equation*}
où $n_A$ est le nombre d'élément dans $A$ et $n$ celui dans $S$.

\subsubsection{Ensemble infini dénombrable}
On suppose que $S$ est infini dénombrable, alors l'équiprobabilité est
impossible. Si la probabilité d'un évènement élémentaire est $\P(e_i)=\epsilon$,
on obtient que $\P(S)=\P(e_1)+\P(e_2)+\cdots=\infty$, Ce qui est en contradiction
avec les axiomes. D'une manière similaire, si $\P(e_i)=0$, alors $\P(S)=0$, ce
qui est aussi en contradiction avec les axiomes.

\subsubsection{Ensemble infini non-dénombrable}
On suppose que $S=[a,b]$ est infini non-dénombrable. Soit un évènement
$A=[c,d]\subset S$. La probabilité de l'évènement est
\begin{equation*}
	\P(A)=\frac{l_A}{l}=\frac{d-c}{b-a},
\end{equation*}
où $l_A$ est la longueur de $A$ et $l$ la longueur de $S$. Par conséquent, la
probabilité d'un évènement élémentaire $e\in S$ est
\begin{equation*}
	\P(e)=0,
\end{equation*}
car $l_A=0$.

\begin{exemple}
	On calcule la somme de deux dés lancés. Quelle est la probabilité d'obtenir
	chaque somme possible?
	
	L'espace échantilion est $S=L\times L$, où $L=\left\{1,2,3,4,5,6\right\}$
	est le résultat possible d'un dé. On peut écrire
	$S=\left\{(1,1),(1,2),\dots,(6,6)\right\}$. On suppose qu'il y a
	équiprobabilité de sorte que la probabilité d'obtenir un évènement
	élémentaire est $\nicefrac{1}{36}$.

	Hors, certaines des sommes sont dupliquées de sorte que la probabilité
	d'obtenir une somme particulière est donnée par la table suivante.
	\begin{table}[H]
		\centering
		\begin{tabular}{r|ccccccccccc}
			$\phantom{\P(}A\phantom{)}$ &
			$2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$ & $11$ & $12$\\
			$\P(A)$ &
			$\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ &
			$\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ &
			$\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\\
		\end{tabular}
	\end{table}
\end{exemple}

\section{Analyse combinatoire}
\subsection{Diagramme en arbre}

\begin{exemple}
	On lance un dé, puis une pièce de monnaie. Combien de résultats est-il
	possible?
	
	On peut représenté la situation par l'arbre suivant.
	\begin{figure}[H]
		\centering
		\input{figure/diagramme_arbre.tex}
	\end{figure}
	Par le principe de multiplication, il y a $6\cdot 2=12$ possibilités.
\end{exemple}

\subsection{Permutations}
\begin{definition}
	Une \textit{permutation} correspond à un choix de $k$ objets parmi $n$
	objets distincts. Le choix se fait sans remise et dans un ordre spécifique.
\end{definition}

La table \ref{tb:permutation} résume le principe d'une permutation à l'aide
d'une pige d'objet. À chaque pige, la quantité d'objets diminue de 1.

\begin{table}[H]
	\centering
	\caption{Pige d'objets}
	\begin{tabular}{r|cccc}
		\toprule
		Choix d'objet   &   1 &   2   & $\dots$ & k\\
		Objets restants & $n$ & $n-1$ & $\dots$ & $n-(n-k)$\\
		\bottomrule
	\end{tabular}
	\label{tb:permutation}
\end{table}

À l'aide du principe de multiplication, il y a $n\cdot(n-1)\cdots(n-k+1)$
combinaisons. On dénote le nombre de permutations sans remise de $n$ éléments
par
\begin{equation*}
	\perm{k}{n}=\frac{n!}{(n-k)!}.
\end{equation*}
Lorsqu'il y a remise, le nombre de permutations de $n$ éléments est $n^k$.

\begin{exemple}
	On dispose de 10 composantes dont 4 défectueuses. On pige 3 composantes au
	hasard et sans remise. Quelle est la probabilité d'obtenir uniquement des
	composantes non défectueuses?

	On suppose qu'il y a équiprobabilité du choix des 3 composantes. La
	probabilité d'obtenir 3 composantes non défecteuses $F$ est
	\begin{equation*}
		\P(F)
		=\frac{6\cdot 5\cdot 4}{10\cdot 9\cdot 8}
		=\frac{\perm{3}{6}}{\perm{3}{10}}
		=\frac{1}{6}.
	\end{equation*}
\end{exemple}

\subsection{Combinaisons}
\begin{definition}
	Une \textit{combinaison} correspond au choix de $k$ objets parmis $n$
	objets distincts. Le choix se fait sans remise et sans ordre spécifique.
\end{definition}

Soit $C_k^n$ le nombre de combinaisons. Le nombre de permutations est égal au
nombre de combinaisons multiplié par le nombre d'arrangement possible $k!$,
c'est-à-dire 
\begin{equation*}
	\perm{k}{n}=\comb{k}{n}\cdot k!,
\end{equation*}
et en réarrangeant, on obtient
\begin{equation*}
	\comb{k}{n}={{n}\choose{k}}=\frac{n!}{k!(n-k)!}.
\end{equation*}

\begin{exemple}
	Combien de codes de deux lettres peut-on former à partir du mot
	\texttt{OUI}?
	
	Avec ordre, il y a $\perm{2}{3}=6$ permutations et sans ordre, il y a
	$\comb{2}{3}=3$ combinaisons.
\end{exemple}

\begin{exemple}
	Quel est la probabilité de gagner le gros lot à la $6/49$?
	
	Sans ordre, il y a 1 seul cas favorable et $\comb{6}{49}$ cas possibles.
	Par conséquent, la probabilité de gagner $G$ est
	\begin{equation*}
		\P(G)=\frac{1}{\comb{6}{49}}=\frac{1}{\SI{13983816}{}}.
	\end{equation*}
	Avec ordre, il y a $6!$ cas favorables et $\perm{6}{49}$ cas possibles de
	sorte que la probabilité est
	\begin{equation*}
		\P(G)=\frac{6!}{\perm{6}{49}}=\frac{720}{\SI{10068347520}{}}.
	\end{equation*}
\end{exemple}

\subsubsection{Triangle de Pascal}
Une combinaison peut se représenter avec le triangle de Pascal comme le montre
la figure \ref{fig:pascal}.

\begin{figure}[H]
	\centering
	\input{figure/pascal}
	\caption{représentation du triangle de Pascal}
	\label{fig:pascal}
\end{figure}

\begin{theoreme}
	$\comb{k}{n}=\comb{k-1}{n-1}+\comb{k}{n-1}$
\end{theoreme}

\begin{proof}
	La démonstration est triviale et est laissée au lecteur.
\end{proof}

\subsubsection{Binôme de Newton}
La combinaison est souvent appliquée dans le cas de la puissance d'un binôme
tel que
\begin{equation*}
	\left(a+b\right)^n
	=\sum_{k=0}^n\comb{k}{n}\cdot a^k\cdot b^{n-k}
	=\sum_{k=0}^n{{k}\choose{n}}\cdot a^k\cdot b^{n-k},
\end{equation*}
mais elle est plus souvent utilisée avec la deuxième notation.

\subsection{Permutation d'objets semblables}
\begin{exemple}
	Combien y a-t-il d'ordres possibles des lettres <<ppfff>>?

	Soit 5 cases distinctes représentant l'ordre d'une pige dans les lettres.
	Il faut choisir les cases où mettre les <<p>>, soit
	\begin{equation*}
		\comb{2}{5}=\frac{5!}{2!3!}=10
	\end{equation*}
	ordres possibles.
	
	Si on échange un <<p>> et un <<f>> dans une séquence $A$, on obtient une
	séquence $B$ différente de $A$. Si on échange un <<p>> avec une autre <<p>>
	dans une séquence $A$, on retrouve la même séquence $A$.

	En analysant la formule, le facteur $5!$ représente le nombre d'ordres si
	toutes les lettres étaient différentes. Le facteur $2!$ représente les
	<<p>> s'ils étaient différents et le facteur $3!$ représente le nombres des
	<<f>> s'ils étaient différents.
\end{exemple}

En général, avec $n$ objets objets comprenant $n_1$ objets de classes $1$,
$n_2$ objets de classe $2$, $\dots$, $n_k$ objets à la classe $k$, on a
\begin{equation*}
	\frac{n!}{n_1!n_2!\cdots n_k!}
\end{equation*}
ordres possibles.

\begin{exemple}\label{ex:network}
	Soit le graphe $n\times n$ suivant.
	\begin{figure}[H]
		\vspace{-3mm}
		\centering
		\input{figure/network}
		\vspace{-3mm}
	\end{figure}

	Combien de chemins existe entre $A$ et $B$ suivant, s'il est seulement
	permis d'aller vers la droite ou vert le haut?
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	On suppose que tous les chemins possibles sont équiprobables. Peu importe
	le chemin, il faut avancer $n-1$ vers la droite et $n-1$ vers le haut pour
	un total de $2n-2$ mouvement.
	
	Il suffit de calculer le nombre de permutations de ces mouvements sachant
	qu'il y des objets semblables. Le nombre chemins est
	\begin{equation*}
		\frac{\left(2n-2\right)!}{\left(n-1\right)!\left(n-1\right)!},
	\end{equation*}
	ce qui est équivalent à $\comb{n-1}{2n-2}$.
\end{exemple}


\subsection{Équivalence}
\begin{exemple}\label{ex:network_broken}
	Soit le graphe $n\times n$ suivant.
	\begin{figure}[H]
		\vspace{-3mm}
		\centering
		\input{figure/network_broken}
		\vspace{-3mm}
	\end{figure}

	Quel est la probabilité qu'un chemin choisi au hasard, allant seulement
	vers la droite ou le haut, ne traverse pas les noeuds au-dessus de la
	diagonale entre $A$ et $B$?

	Les <<bons>> chemins ne passent pas par la ligne critique en pointillée et
	les <<mauvais>> chemins passent par la ligne critique. La probabilité d'un
	bon chemin $B$ peut s'écrire en fonction de son complément, soit
	\begin{equation*}
		\P(B)=1-\P(M)=1-\frac{n_m}{C_{n-1}^{2n-2}},
	\end{equation*}
	où $M$ est un mauvais chemin et $n_m$ est le nombre de mauvais chemins.

	Soit un mauvais chemin $M$. On définit le point $X$ comme étant le premier
	point au-dessus de la diagonale que $M$ atteint. On applique une
	transformation mirroir au chemin suivant $X$ comme la prochaine figure.

	\noindent
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/network_bad}
		\end{figure}
	\end{minipage}%
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/network_transformed}
		\end{figure}
	\end{minipage}
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Il s'avère que tous les mauvais chemins dans le graphe $n\times n$
	correspond à un chemin unique dans le graphe $(n-1)\times(n+1)$.
	C'est-à-dire la transformation est bijective.
	
	Par conséquent, le problème est équivalent à trouver le nombre de chemins
	dans un graphe $(n-1)\times(n+1)$. D'une manière similaire à l'exemple
	\ref{ex:network}, ce nombre de chemins est donné par
	\begin{equation*}
		\frac{(2n-2)!}{(n-2)!n!},
	\end{equation*}
	ce qui est équivalent à $\comb{2n-2}{n-2}$.

	Par conséquent, la probabilité d'avoir un bon chemin au hasard est
	\begin{equation*}
		\P(B)
		=1-\frac{\comb{n-2}{2n-2}}{\comb{n-1}{2n-2}}
		=\frac{1}{n}.
	\end{equation*}
\end{exemple}

\subsection{Réccurence}
\begin{exemple}
	Il est possible de résoudre le problème à l'exemple \ref{ex:network_broken}
	à l'aide de la réccursion.
	
	En effet, le nombre de <<bons>> chemins $b_{i,j}$ à partir d'un noeud est
	la somme des <<bons>> chemins des noeuds à droite et en haut, c'est-à-dire
	\begin{equation*}
		b_{i,j}=b_{i-1,j}+b_{i,j-1},
	\end{equation*}
	où $i$ est le nombre de noeuds restants vers la droite et $j$ la nombre de
	noeuds restants vers le haut.

	On sait que $b_{0,0}=1$, car il ne reste plus de noeud à parcourir fois
	rendu à $B$. Aussi, $b_{0,j}=1$, car il est seulement possible de se rendre
	à $B$ en allant vers le haut. Sur la diagonale, on a $b_{i,i}=b_{i-1,j}$,
	car on ne peut pas aller vers le haut. Par conséquent, on obtient le
	système d'équations à reccurence suivant,
	\begin{equation*}
		b_{i,j}=\left\{
			\begin{matrix}
				0,                 &\text{si}&i=0,j=0\\
				1,                 &\text{si}&i=0\\
				b_{i-1,j},         &\text{si}&i=j\\
				b_{i-1,j}+b_{i,j-1}&\text{sinon}
			\end{matrix}
		\right.
	\end{equation*}

	Pour une grille $4\times 4$, on peut calculer le nombre de <<bons>> chemins
	en développant la récurrence afin d'obtenir $b_{3,3}=5$ chemins.
\end{exemple}

\section{Probabilité conditionnelles}
\begin{definition}
	Une \textit{probabilité conditionnelle} est la probabilité qu'un évènement
	$A$ se réalise, si $B$ s'est réalisé.
\end{definition}

Mathématiquement, on dénote une probabilité conditionnelle avec
\begin{equation*}
	\P(A|B)=\frac{\P(A\cap B)}{\P(B)}.
\end{equation*}
où $A$ et $B$ sont des évènements.

\begin{exemple}
	Soit le lancement d'un dé avec les évènements $A=\left\{5,6\right\}$ et
	$B=\left\{2,4,6\right\}$, alors $\P(A)=\nicefrac{1}{3}$ et 
	\begin{equation*}
		\P(A|B)=\frac{\P(\left\{6\right\})}{\P(B)}=\frac{1}{3}.
	\end{equation*}

	Si $A=\left\{6\right\}$, alors $\P(A)=\nicefrac{1}{6}$ et
	\begin{equation*}
		\P(A|B)=\frac{\P(\left\{6\right\})}{\P(B)}=\frac{1}{3}.
	\end{equation*}
\end{exemple}

\subsection{Propriétés}
\begin{theoreme}
	$\P(A\cap B)=\P(A|B)\cdot \P(B)$.
\end{theoreme}

\begin{theoreme}
	$\P(A|B)=\P(B|A)=0$, si $A\cap B=\emptyset$.
\end{theoreme}

\begin{theoreme}
	$\P(A|B)\neq \P(B|A)$, en général.
\end{theoreme}

\begin{theoreme}
	$\P(A|S)=\P(A)$, $A\in S$.
\end{theoreme}

Le dernier théorème résulte que toutes probabilités peut s'exprimer sous la
forme d'une probabilité conditionnelle.

\begin{exemple}
	On pige sans remise 3 composantes non défecteuses parmis 10 composantes, donc 4 sont
	défecteuses. Soit $A_i$ le $i^\text{e}$ composante non défecteuses.
	\begin{equation*}
		\P(A_1\cap A_2\cap A_3)=\P(A_3|A_1\cap A_2)\P(A_2|A_1)\P(A_1)
		=\frac{4}{8}\cdot\frac{5}{9}\cdot\frac{6}{10}
	\end{equation*}
\end{exemple}

\subsection{Probabilités totales}
\begin{definition}
	Les évènements $B_1,B_2,\dots,B_n$ forment une \textit{partition} si les
	évènements $B_i\cap B_j=\emptyset$, $\forall i\neq j$, et
	$B_1\cup B_2\cup\cdots\cup B_n=S$.
\end{definition}

Avec une partition, on a la règle de probabilités totales, soit
\begin{equation*}
	\P(A)=\sum_{i=1}^n\P(A\cap B_i)=\sum_{i=1}^n\P(A|B_i)\P(B_i).
\end{equation*}

\begin{exemple}
	Soit la partition $B_1$, $B_2$ et $B_3$ représenté dans l'arbre suivant.
	\begin{figure}[H]
		\centering
		\input{figure/partition_arbre}
	\end{figure}

	À partir de l'arbre, il facile de déterminer les probabilités
	conditionnelles de $A$ et $A^c$. Par exemple,
	$\P(A|B_1)=\nicefrac{3}{6}\cdot\nicefrac{1}{10}=\nicefrac{1}{30}$ et
	$\P(A^c|B_2)=\nicefrac{2}{6}\cdot\nicefrac{9}{10}=\nicefrac{3}{10}$.
\end{exemple}

\begin{theoreme}[Règle d'inversion]
	$\P(B|A)=\dfrac{\P(A|B)\P(B)}{\P(A)}$
\end{theoreme}

\begin{theoreme}[Règles de Bayes]
	$\P(B_j|A)=\dfrac{\P(A|B_j)\P(B_j)}{\sum_{i=1}^n\P(A|B_i)\P(B_i)}$, où $B_i$ et
	$B_j$ sont des partitions.
\end{theoreme}

\begin{exemple}
	On dépiste le cancer du poumon dans une clinique. On sait que 
	\begin{itemize}
		\item\SI{25}{\percent} des individus sont fumeurs
		\item\SI{75}{\percent} des individus sont non fumeurs
		\item\SI{10}{\percent} des fumeurs développent un cancer
		\item\SI{1 }{\percent} des non fumeurs développent un cancer
	\end{itemize}

	On détecte un cancer des poumons chez un individu sélectionné au hasard
	pour dépistage. Quelle est la probabilité que ça soit un fumeur?

	Soit $B$ les fumeurs, $B^c$ les non fumeurs, $A$ la présence de cancer du
	poumon et $A^c$ son absence. Par conséquent, on cherche $\P(B|A)$. Avec les
	données, on sait que $\P(A|B)=\SI{0.10}{}$, $\P(A|B^c)=\SI{0.01}{}$,
	$\P(B)=\SI{0.25}{}$ et $\P(B^c)=\SI{0.75}{}$, de sorte que le théorème de
	Bayes nous donne
	\begin{equation*}
		\P(B|A)=\frac{\P(A|B)\P(B)}{\P(A|B)\P(B)+\P(A|B^c)\P(B^c)}\approx\SI{0.769}{}.
	\end{equation*}
\end{exemple}

\subsection{Notion d'indépendance}
\begin{definition}
	On dit que les évènements $A$ et $B$ sont \textit{indépendants} si la
	réalisation d'une n'affecte pas l'autrea.
\end{definition}

Mathématiquement parlant, des évènements indépendants $A$ et $B$ sont tels que
$\P(A|B)=\P(A)$ et $\P(B|A)=\P(B)$.

\begin{theoreme}
	$\P(A\cap B)=\P(A)\P(B)\Leftrightarrow \P(A|B)=\P(A)\wedge \P(B|A)=\P(B)$
\end{theoreme}

\begin{theoreme}
	$\P(A\cap B)=\P(A)\P(B)\Leftrightarrow \P(A\cap B^c)=\P(A)\P(B^c)$.
\end{theoreme}

\begin{exemple}
	Soit un système en série ayant $n$ composantes comme à la figure suivante.
	\begin{figure}[H]
		\centering
		\input{figure/series}
	\end{figure}
	Un système en série fonction si et seulement si tous ses composantes
	fonctionnent. Quelle est la probabilité que le système fonctionne si chaque
	composante a une probabilité de \SI{75}{\percent} de fonctionner?

	Soit l'ensemble échantillion $S=\left\{F,D\right\}$ avec $F$ une composante
	qui fonctionne et $D$ une composante défectueuse. De plus, on définit
	$A_i=\left\{F\right\}\in S$ comme étant la composante $i$ qui fonctionne et
	$A_i^c=\left\{D\right\}\in S$ comme étant la composante $i$ qui est
	défecteuse.

	On suppose que les composantes fonctionnent et tombent en panne
	indépendamment les uns des autres. Le système fonctionne si
	\begin{equation*}
		A_S=\bigcap_{i=1}^nA_i,
	\end{equation*}
	de sorte que
	\begin{equation*}
		\P(A_S)
		=\P\left(\bigcap_{i=1}^nA_i\right)
		=\prod_{i=1}^n\P(A_i)
		=\SI{0.75}{}^n.
	\end{equation*}
\end{exemple}

\begin{exemple}
	On génère un point au hasard dans le carré $S=[0,10]\times[0,10]$. On
	définit $A$ comme ayant l'abscisse du point entre 2 et 4, et $B$ comme
	ayant l'ordonnée du point entre 3 et 6. Est-ce que $A$ et $B$ sont
	indépendants?
	\begin{figure}[H]
		\centering
		\input{figure/square}
	\end{figure}
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Puisqu'il y a équiprobabilité continue, alors les probabilités sont données
	par le rapport entre l'aire de $A$ ou $B$ sur $S$, alors
	$\P(A)=\nicefrac{2}{10}$ et $\P(B)=\nicefrac{3}{10}$. De plus,
	$\P(A\cap B)=\nicefrac{6}{10}$. Puisque $\P(A\cap B)=\P(A)\P(B)$, alors les
	évènements $A$ et $B$ sont indépendants.
\end{exemple}

\section{Variables Aléatoires}
\begin{definition}
	Une \textit{variable aléatoire} correspond à une expérience aléatoire dont
	les résultats sont quantitatifs.
\end{definition}

On peut décrire une variable aléatoire par sa
\begin{itemize}
	\item fonction de répartition
	\item fonction de masse (variable aléatoire discrète)
	\item fonction de densité (variable aléatoire continue)
\end{itemize}

\begin{exemple}
	Soit $X$ le nombre de défaults de soudure d'un transistor à 3 pattes. Quel
	est l'espace échantillion de $X$?

	L'espace échantillion est $S_X=\left\{0,1,2,3\right\}$, alors $X$ est une
	variable aléatoire discrète.
\end{exemple}

\begin{exemple}
	Soit $X$ un nombre réel choisie au hasard dans l'intervalle $[0,2]$. Quel
	est l'espace échantillion de $X$?

	L'espace échantillion est $S_X=[0,2]$, alors $X$ est une variable aléatoire
	continue.
\end{exemple}

\begin{exemple}
	Soit $X$ le temps d'attente d'un client au guichet automatique. Quel est
	l'espace d'échantillion de $X$?

	L'espace échantillion est $S_X={0}\cup [0,\infty[$. Il y a une partie
	discrète et continue, alors $X$ est une variable aléatoire mixte.
\end{exemple}

\subsection{Fonction de répartition}
\begin{definition}
	Un \textit{fonction de répartition} $F_X(x)$ est égal à la probabilité
	qu'une variable aléatoire $X$ soit plus petite qu'une valeur $x$,
	c'est-à-dire $F_X(x)=\P(X\leq x)$, $\forall x\in\mathbb{R}$.
\end{definition}

\begin{theoreme}
	$0\leq F_X(x)\leq 1$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\lim_{X\rightarrow-\infty}F_X(x)=0$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\lim_{X\rightarrow \infty}F_X(x)=1$.
\end{theoreme}

\begin{theoreme}[non décroissance]
	$x_0<x_1\Leftrightarrow F_X(x_0)<F_X(x_1)$.
\end{theoreme}

\begin{theoreme}[continuité à droite]
	$F_X(x^+)=F_X(x)$.
\end{theoreme}

Il en résulte que toutes fonctions de répartition $F_X(x)$ sont croissantes,
mais peuvent contenir des discontinuités. Elles peuvent représenté des
variables aléatoires discrètes, continues ou mixtes. La figure
\ref{fig:maxwell} est une exemple de la fonction de répartition d'une
distribution de Maxwell-Boltzmann pour certains paramètres différents.

\begin{figure}[H]
	\centering
	\caption{$F_X(x)$ de certaines distributions de Maxwell-Boltzmann}
	\input{figure/maxwell_boltzmann}
	\label{fig:maxwell}
\end{figure}

\subsection{Fonction de masse}
\begin{definition}
	Une \textit{fonction de masse} $P_X(x_k)$ est égal à la probabilité qu'une
	variable aléatoire $X$ soit égal à une valeur discrète $x_k\in S_X$,
	c'est-à-dire $P_X(x_k)=\P(X=x_k)$ avec
	$S_X=\left\{x_1,x_2,\dots,x_k|x_1<x_2<\cdots<x_k\right\}$
\end{definition}

\begin{theoreme}
	$P_X(x_k)\geq 0$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\sum_{a< x_k\leq b}P_X(x_k)=\P(a<X\leq b)$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\sum_{k=1}^\infty P_X(x_k)=1$
\end{theoreme}

Une fonction de masse est nulle en tout point sauf aux valeurs discrètes
possibles. De plus, la somme de toutes les valeurs discrètes donne 1. La figure
\ref{fig:fonction_masse} montre un exemple d'une fonction de masse.

\begin{figure}[H]
	\centering
	\caption{exemple d'une fonction de masse}
	\input{figure/fonction_masse}
	\label{fig:fonction_masse}
\end{figure}

\subsection{Fonction de densité}
\begin{definition}
	Une \textit{fonction de densité} $f_X(x)$ est égal à la probabilité de
	qu'une variable aléatoire $X$ soit autour d'une valeur $x$,
	c'est-à-dire
	\begin{equation*}
		f_X(x)=\lim_{\epsilon\rightarrow 0}\frac{1}{\epsilon}
		\P\left(x-\frac{\epsilon}{2}\leq X\leq x+\frac{\epsilon}{2}\right),
	\end{equation*}
	où $\epsilon$ est positif.
\end{definition}

\begin{theoreme}
	$f_X(x)\geq 0$
\end{theoreme}

\begin{theoreme}
	$\displaystyle\int_{-\infty}^\infty f_X(x)\d{x}=1$
\end{theoreme}

\subsection{Règles de calcul fondamentale}
\begin{theoreme}\label{th:calc_fond}
	$\P(a<X\leq b)=F_X(b)-F_X(a)$
\end{theoreme}

\begin{proof}
	Soit les ensembles $A=\left\{X\leq a\right\}$, $B=\left\{X\leq b\right\}$
	et $C=\left\{a< X\leq b\right\}$, avec $a<b$, comme à la figure
	\ref{fig:droite_num}.
	\begin{figure}[H]
		\centering
		\input{figure/droite_numerique}
		\caption{représentation sur une droite numérique}
		\label{fig:droite_num}
	\end{figure}
	
	On sait que $A\cap C=\emptyset$ et $A\cup C=B$. Par conséquent, 
	$\P(B)=\P(A)+\P(C)\Leftrightarrow \P(C)=\P(B)-\P(A)=F_X(b)-F_X(a)$.
\end{proof}

\begin{theoreme}
	$\P(X=x)=F_X(x)-F_X(x^-)$.
\end{theoreme}

\begin{proof}
	Soit $a=x-\epsilon$ et $b=x$, où $\epsilon$ est positif. Selon le théorème
	\ref{th:calc_fond}, on a
	\begin{equation*}
		\P(x-\epsilon<X\leq x)=F_X(x)-F_X(x-\epsilon).
	\end{equation*}
	En prenant la limite lorsque $\epsilon\rightarrow 0$, on obtient
	\begin{equation*}
		\lim_{\epsilon\rightarrow 0}\P(x-\epsilon<X\leq x)
		=F_X(x)-\lim_{\epsilon\rightarrow 0}F_X(x-\epsilon).
	\end{equation*}
	Avec $F_X(x^-)=\lim_{\epsilon\rightarrow 0}F_X(x-\epsilon)$ et
	$(x-\epsilon<X\leq x)\equiv (X=x)$ lorsque $\epsilon\rightarrow 0$, on
	obtient
	\begin{equation*}
		\P(X=x)=F_X(x)-F_X(x^-).\qedhere
	\end{equation*}
\end{proof}

\subsection{Liens entre les différentes fonctions}
Toutes variables aléatoires peuvent être décrites par une fonction de
répartition. Lorsque la variable aléatoire est continue, alors elle peut aussi
être décrite pas une fonction de densité de probabilité. Si la variable
aléatoire est discrète, alors elle peut être décrite pas une fonction de masse.

\begin{theoreme}
	$f_X(x)=\dfrac{\d{}}{\d{x}}F_X(x)$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\displaystyle\int_{-\infty}^xf_X(t)\d{t}$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}\label{th:masse_repartition}
	$F_X(x)=\displaystyle\sum_{x_k\leq x}P_X(x_k)$ si $X$ est discrète.
\end{theoreme}

\begin{theoreme}
	$P_X(x_k)=\left\{
		\begin{matrix}
			F_X(x_1), & k=1\\
			F_X(x_k)-F_X(x_{k-1}), & k\neq 1\\
		\end{matrix}
	\right.$.
\end{theoreme}

\begin{exemple}
	Soit $X$ le temps d'attente d'un client au guichet automatique. On suppose
	que \SI{10}{\percent} des visites sont sans attente. Quel est la fonction
	de répartition?
	
	La variable aléatoire est mixte. On sait que $F_X(0)=\nicefrac{1}{10}$ et
	que $F_X(x)=0$ si $x<0$. La fonction de répartition est
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				0, & x < 0\\
				\dfrac{x+1}{x+10}, & x \geq 0\\
			\end{matrix}
		\right.
	\end{equation*}
	Le graphique suivant montre la fonction de répartition $F_X(x)$.
	\begin{figure}[H]
		\centering
		\input{figure/fonction_repartition}
	\end{figure}
\end{exemple}

\pagebreak
\begin{exemple}
	Soit $X$ le nombre de défaults de soudure d'un transistor à 3 pattes. La
	fonction de masse de $X$ est donnée par
	\begin{equation*}
		P_X(k)=\left\{
			\begin{matrix}
				\SI{0.7}{}, & k=0\\
				\SI{0.1}{}, & k=1\\
				\SI{0.1}{}, & k=2\\
				\SI{0.1}{}, & k=3\\
			\end{matrix}
		\right.
	\end{equation*}
	Quelle est la fonction de répartition de $P_X(k)$?

	On applique le théorème \ref{th:masse_repartition} pour obtenir la
	fonction de répartition. Lorsque $x<0$, alors $F_X(x)=0$. Lorsque
	$0\leq x<1$, alors $F_X(x)=\SI{0.7}{}$. Lorsque $1\leq x<2$, alors
	$F_X(x)=\SI{0.8}{}$. En continuant, on obtient
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				\SI{0.0}{}, & \phantom{0\;\leq}k<0\\
				\SI{0.7}{}, & 0\leq k<1\\
				\SI{0.8}{}, & 1\leq k<2\\
				\SI{0.9}{}, & 2\leq k<3\\
				\SI{1.0}{}, & 3\leq k\phantom{<0\;}\\
			\end{matrix}
		\right.
	\end{equation*}

	Les fonctions de masse et de répartition sont montrées dans les figures
	suivantes.

	\noindent
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/transistor_PX}
		\end{figure}
	\end{minipage}%
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/transistor_FX}
		\end{figure}
	\end{minipage}
\end{exemple}

\begin{exemple}
	Soit $X$ un nombrbe réel choisie au hasard dans l'intervalle $[0,2]$.
	Quelle est la fonction de répartition de $X$?

	On sait que $F_X(x)=\P(X\leq x)=\nicefrac{x}{2}$ lorsque $0\leq x\leq 2$,
	$F_X(x)=0$ si $x<0$ et $F_X(x)=1$ si $x>2$. On obtient donc la fonction de
	répartition
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				0,            & \phantom{0\;\leq}x<0\\
				\dfrac{x}{2}, & 0\leq x\leq 2\\
				1,            & 2\leq x\phantom{\leq 0\;}\\
			\end{matrix}
		\right.
	\end{equation*}
\end{exemple}

\subsection{Fonction conditionnelle}
\begin{definition}
	Une \textit{fonction conditionnelle} est la probabilité qu'une variable
	aléatoire $X$ prenne une valeur plus petit ou égal à  $x$ sachant un
	évènement $A$.
\end{definition}

\begin{theoreme}
	$F_X(x|A)=\dfrac{\P(\left\{X\leq x\right\}\cap A)}{\P(A)}$.
\end{theoreme}

\begin{theoreme}
	$f_X(x|A)=\dfrac{\d{}}{\d{x}}F_X(x|A)$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}
	$P_X(x_k|A)=
	\left\{
		\begin{matrix}
			\dfrac{P_X(x_k)}{\P(A)}, & x_k\in A\\
			0, & x_k\notin A\\
		\end{matrix}
	\right.$
	si $X$ est discrète.
\end{theoreme}

\subsection{Médiane et quantile}
\begin{definition}
	La médiane d'une variable aléatoire $X$ continue est le nombre réel
	$x_{1/2}$ tel que $F_X(x_{1/2})=\nicefrac{1}{2}$.
\end{definition}

\begin{definition}
	Le \textit{quantile} d'ordre $p$ d'une variable aléatoire $X$ continue est
	le nombre réel $x_p$ tel que $F_X(x_p)=p$.
\end{definition}

\begin{exemple}
	Dans une certaine population, la taille $X$ d'un adulte choisi au hasard
	possède la fonction de répartition 
	\begin{equation*}
		F_X(x)=\left\{
			\begin{matrix}
				0,        & \phantom{1.2\leq\;}x<1.2\\
				1.5x-1.8, & 1.2\leq x<1.7\\
				0.5x-0.1, & 1.7\leq x<2.2\\
				1,        & 2.2\leq x\phantom{<2.2\;}\\
			\end{matrix}
		\right.
	\end{equation*}
	Calculer la médiane et le quantile d'ordre 95.

	Puisque $F_X(1.7)=0.75$, alors le quantile d'ordre 95 est dans la tranche
	$1.7\leq x<2.2$. Il suffit de résoudre $0.95=0.5x_{0.95}-0.1$ et on obtient
	que $x_{0.95}=2.1$.
\end{exemple}

\subsection{Lois de probabilités discrètes}
\subsubsection{Loi de Bernoulli}
Une expérience aléatoire où la variable aléatoire $X$ peut être un
\textit{succès} ou un \textit{échec} suit une loi de Bernoulli de paramètre
$p$ dénotée
\begin{equation*}
	X\sim\bin(1,p),
\end{equation*}
où $p$ est la probabilité de succès. On dénote un succès comme $X=1$ et un
échec comme $X=0$. Par conséquent, la probabilité d'un succès est $\P(X=1)=p$
tandis que celle d'un échec est $\P(X=0)=q$ où $q=1-p$.

\begin{theoreme}
	$P_X(k)=\left\{
		\begin{matrix}
			q &\text{si} & k=0\\
			p &\text{si} & k=1\\		
		\end{matrix}
	\right.$.
\end{theoreme}

\subsubsection{Loi binomiale}
Une expérience aléatoire où la variable aléatoire $X$ est le nombre de
\textit{succès} obtenu en $n$ essaie suit une loi binomiale dénotée
\begin{equation*}
	X\sim\bin(n,p),
\end{equation*}
où $p$ est la probabilité d'un succès individuel. L'espace échantillion est
$S_X=\left\{0,1,2,\dots,n\right\}$.

\begin{theoreme}
	$P_X(k)=\comb{k}{n}p^kq^{n-k}$ où $k=0,1,2,3,\dots,n$.
\end{theoreme}

\begin{proof}
	Soit $A_i$ le $i$-ème succès et $A_i^c$ le $i$-ème échec dans une
	expérience aléatoire à $n$ essai. La probabilité d'obtenir $k$ succès est
	donnée par
	\begin{equation*}
		\P\big(
			\underbrace{A_1\cap A_2\cap\cdots\cap A_k}_\text{$k$ fois}
			\cap
			\underbrace{A_{k+1}^c\cap A_{k+1}^c\cap\cdots\cap A_n^c}_
			\text{$n-k$ fois}
		\big),
	\end{equation*}
	où $k=0,1,2,3,\dots,n$.

	Hors, la probabilité d'un succès est $\P(A_i)=p$ et celle d'un échec est
	$\P(A_i^c)=q$, où $q=1-p$. Puisque chaque essai est indépendant des autres,
	la probabilité peut s'écrire
	\begin{equation*}
		\underbrace{
			\P(A_1)\P(A_2)\cdots P(A_k)
		}_\text{$k$ fois}
		\cdot
		\underbrace{
			\P(A_{k+1}^c)\P(A_{k+2}^c)\cdots P(A_n^c)
		}_\text{$n-k$ fois}
		=p^kq^{n-k}.
	\end{equation*}

	De plus, les succès peuvent être à n'importe quel essai. Par conséquent,
	il faut choisir $k$ essaies parmis les $n$ essaies de sorte que la
	probabilité d'obtenir $k$ succès en tout est
	$P_X(k)=\P(X=k)=\comb{k}{n}p^kq^{n-k}$.
\end{proof}

\begin{theoreme}
	$\P(S_X)=1$.
\end{theoreme}

\begin{proof}
	Par définition, on a
	\begin{equation*}
		\P(S_X)=\sum_{k=0}^n\comb{k}{n}p^kq^{n-k}
	\end{equation*}
	Selon le théorème binomial, soit
	\begin{equation*}
		(a+b)^n=\sum_{k=0}^n\comb{k}{n}p^kq^{n-k},
	\end{equation*}
	et que $q=1-p$, on peut simplifier de sorte à obtenir
	\begin{equation*}
		\P(S_X)=(p+q)^k=1^k=1.\qedhere
	\end{equation*}
\end{proof}

\vspace{-5mm}
\begin{exemple}
	Vous achetez un billet de 6/49 à chaque semaine depuis vos 18 ans. Quelle
	est la probabilité de gagner le gros lot au plus tard à votre $98^e$
	anniversaire?
	
	Ce problème se décrit à l'aide d'une loi binomiale, soit
	\begin{equation*}
		X\sim\bin\left(n,\frac{1}{\SI{13983816}{}}\right),
	\end{equation*}
	où $n$ est le nombre d'essaies et $X$ le nombre de gros lots gagnés.
	\vspace{-1mm}
\end{exemple}\pagebreak
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Puisqu'on joue chaque semaine de 18 ans à 98 ans, un total de
	$n=52\cdot(98-18)=4160$ billets sont achetés. La probabilité d'obtenir que
	des échecs est données par
	\begin{equation*}
		\left(1-\frac{1}{\SI{13983816}{}}\right)^{4160}=
		\left(\frac{\SI{13983815}{}}{\SI{13983816}{}}\right)^{4160},
	\end{equation*}
	de sorte que la probabilité d'obtenir au moins 1 billet gagnant est donnée
	par le complément, soit
	\begin{equation*}
		\P(X\geq 1)
		=1-\left(\frac{\SI{13983815}{}}{\SI{13983816}{}}\right)^{4160}
		\approx\SI{0.000297}{}.
	\end{equation*}
\end{exemple}

\subsubsection{Loi géométrique}
Une expérience aléatoire où la variable aléatoire $X$ est le nombre d'essais
requis pour obtenir un premier succès suit une loi géométrique dénotée
\begin{equation*}
	X\sim\geo(p),
\end{equation*}
où $p$ est la probabilité d'avoir un succès à un essai. L'espace échantillion
est $S_X=\left\{1,2,\dots\right\}$.

\begin{theoreme}
	$P_X(k)=q^{k-1}p$.
\end{theoreme}

\begin{proof}
	Soit $A_i$ le $i$-ième succès et $A_i^c$ le $i$-ème échec dans un
	expérience aléatoire où il faut $k$ essai pour obtenir un succès. La
	probabilité d'obtenir le succès au $k$-ième essai est donnée par
	\begin{equation*}
		\P\big(
			\underbrace{A_1^c\cap A_2^c\cap\cdots\cap A_{k-1}^c}_
			\text{$k-1$ fois}\cap A_k
		\big),
	\end{equation*}
	où $k=1,2,\dots$

	Hors, la probabilité d'un succès est $\P(A_i)=p$ et celle d'un échec
	est $\P(A_i^c)=q$, où $q=1-p$. Puisque chaque essai est indépendant des
	autres, la probabilité peut s'écrire
	\begin{equation*}
		P_X(k)=
		\underbrace{\P(A_1^c)\P(A_2^c)\cdots\P(A_{k-1})}_\text{$k-1$ fois}
		\cdot\P(A_k)=q^{k-1}p.\qedhere
	\end{equation*}
\end{proof}

\begin{theoreme}\label{th:repartition_geo}
	$F_X(n)=1-q^n$ où $n=1,2,\dots$
\end{theoreme}

\begin{proof}
	Selon une variante du théorème \ref{th:masse_repartition}, on a
	\begin{equation*}
		F_X(n)=\sum_{k=1}^nP_X(k)=\sum_{k=1}^nq^{k-1}p=p\sum_{k=1}^nq^{k-1}.
	\end{equation*}
	On remarque que la somme est une série géométrique, alors
	\begin{equation*}
		\sum_{k=1}^nq^{k-1}=\frac{1-q^n}{1-q}.
	\end{equation*}
	Puisque $q=1-p\Leftrightarrow p=1-q$, on obtient
	\begin{equation*}
		F_X(n)=(1-q)\frac{1-q^n}{1-q}=1-q^n.\qedhere
	\end{equation*}
\end{proof}

\begin{theoreme}
	$\P(S_X)=1$.
\end{theoreme}

\begin{proof}
	En utilisant à nouveau la série géométrique, on a
	\begin{equation*}
		\P(S_X)
		=\sum_{k=1}^\infty P_X(k)
		=p\sum_{k=1}^\infty q^{k-1}
		=p\frac{1}{1-q}
		=(1-q)\frac{1}{1-q}
		=1,
	\end{equation*}
	puisque $q=1-p\Leftrightarrow p=1-q$.
\end{proof}

\begin{theoreme}[absence de mémoire]
	$\P(X>k+j|X>j)=\P(X>k)$ où $k,j\in\mathbb{Z}$.
\end{theoreme}

\begin{proof}
	On sait que
	\begin{equation*}
		\P(X>k+j|X>j)=\frac{\P(
			\left\{X>k+j\right\}\cap
	   		\left\{X>j\right\})}{\P(X>j)}.
	\end{equation*}
	Puisque $k\in\mathbb{Z}$, alors $\left\{X>k+j\right\}\cap\left\{X>j\right\}
	=\left\{X>k+1\right\}$ de sorte que
	\begin{equation*}
		\P(X>k+j|X>j)=\frac{\P(X>k+j)}{\P(X>j)}.
	\end{equation*}
	Avec le complément du théorème \ref{th:repartition_geo}, on a
	\begin{equation*}
		\P(X>k+j|X>j)=\frac{q^{k+j}}{q^{j}}=q^{k}=\P(X>k).\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Loi de Poisson}
Cette loi est un cas limite de la loi binomiale lorsque $n\rightarrow\infty$.
On pose $p=\alpha/n$ où $\alpha$ est un nombre positif, de sorte que la 
probabilité lorsque $n$ est grand devient nulle. On dénote une loi de Poisson
par
\begin{equation*}
	X\sim\poi(\alpha)
	=\lim_{n\rightarrow\infty}\bin\left(n,\frac{\alpha}{n}\right).
\end{equation*}

Par conséquent, la probabilité que $X$ prend une valeur proche de $\alpha$ est
élevée. Puisqu'elle est basée sur une loi binomiale, alors l'espace
échantillion est $S_X=\left\{0,1,2,\dots\right\}$.

\begin{theoreme}
	$P_X(k)=\dfrac{\alpha^k}{k!}\e^{-\alpha}$.
\end{theoreme}

\begin{theoreme}
	$\P(S_X)=1$.
\end{theoreme}

\begin{proof}
	En utilisant le développement en série de la fonction exponentielle, on a
	\begin{equation*}
		\P(S_X)
		=\sum_{k=0}^\infty P_X(k)
		=\e^{-\alpha}\sum_{k=0}^\infty\frac{\alpha^k}{k!}
		=\e^{-\alpha}\e^\alpha
		=1.\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Approximation par une loi de Poisson}
Soit une loi binomiale $X\sim\bin(n,p)$. Si $p$ est près de $0$, alors
$X\approx\poi(np)$. Par conséquent,
\begin{equation*}
	\P(X=k)\approx\frac{(np)^k}{k!}\e^{-np}.
\end{equation*}

En général, l'approximation est bonne si $n\geq 30$ et $p\leq 0.05$. Si $p$
est proche de 1, alors on considère les échecs au lieu des succès. Dans ce cas,
\begin{equation*}
	\P(X=k)\approx\frac{(nq)^{n-k}}{(n-k)!}\e^{-nq}.
\end{equation*}

\subsection{Loi des probabilités continues}
\subsubsection{Loi uniforme (continue)}
Une expérience aléatoire où la variable aléatoire $X$ est le choix d'un nombre
réel dans un intervalle $[a,b]$ suit une loi uniforme dénotée
\begin{equation*}
	X\sim\uni([a,b]),
\end{equation*}
où l'ensemble échantillion est $S_X=[a,b]$.

\begin{theoreme}
	$f_X(x)=\left\{
		\begin{matrix}
			\dfrac{1}{b-a} &\text{si } a\leq x\leq b\\
			0              &\text{sinon}
		\end{matrix}
	\right..
	$
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\left\{
		\begin{matrix}
			0                &\text{si}& \;\phantom{a\leq}x<a\\
			\dfrac{x-a}{b-a} &\text{si}& a\leq x\leq b\\
			1                &\text{si}& b<x\phantom{\leq b}\;\\
		\end{matrix}
	\right.$.
\end{theoreme}

\subsubsection{Loi exponentielle}
Une expérience aléatoire où la variable aléatoire $X$ suit la loi exponentielle
dénotée
\begin{equation*}
	X\sim\Exp(\lambda),
\end{equation*}
où $\lambda>0$, a l'espace échantillion $S_X=[0,\infty[$.

\begin{theoreme}
	$f_X(x)=\left\{
		\begin{matrix}
			\lambda\e^{-\lambda x} &\text{si}& x\geq 0\\
			0                      &\text{si}& x\leq 0\\
		\end{matrix}
	\right.$.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\left\{
		\begin{matrix}
			1         -\e^{-\lambda x}      &\text{si}& x\geq 0\\
			0\phantom{-\e^{-\lambda x}}\;\; &\text{si}& x\leq 0\\
		\end{matrix}
	\right.$.
\end{theoreme}

\begin{theoreme}[absence de mémoire]
	$\P(X>s+t|X>t)=\P(X>t)$ où $s,t\in[0,\infty[$.
\end{theoreme}

\subsubsection{Loi gamma}
Une expérience aléatoire où la variable aléatoire $X$ suit la loi gamma dénotée
\begin{equation*}
	X\sim\Gam(\alpha,\lambda),
\end{equation*}
où $\alpha>0$ et $\lambda>0$, a l'espace échantillion $S_X=[0,\infty[$.

\begin{theoreme}
	$f_X(x)
	=\dfrac{(\lambda x)^{\alpha-1}\lambda\e^{-\lambda x}}{\Gamma(\alpha)}$.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=1-\displaystyle\sum_{k=0}^{n-1}
		\dfrac{(\lambda x)^k\e^{-\lambda x}}{k!}$
	si $\alpha=n=1,2,3,\dots$
\end{theoreme}

\begin{theoreme}
	$\P(S_X)=1$.
\end{theoreme}

\begin{proof}
	Il suffit d'intégrer la fonction de densité de probabilité, soit
	\begin{equation*}
		\int_0^\infty
			\frac{(\lambda x)^{\alpha-1}\lambda\e^{-\lambda x}}{\Gamma(\alpha)}
		\d{x}=\frac{1}{\Gamma(\alpha)}\int_0^\infty
			(\lambda x)^{\alpha-1}\lambda\e^{-\lambda x}\d{x},
	\end{equation*}
	et en posant $y=\lambda x$, on obtient
	\begin{equation*}
		\frac{1}{\Gamma(\alpha)}\int_0^\infty y^{\alpha-1}\e^{-y}\d{y}
		=\frac{1}{\Gamma(\alpha)}\Gamma{(\alpha)}=1.\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Loi normale}
On modèle souvent les erreurs d'observation et l'addition des résultats de
plusieurs expérience aléatoires d'une variable aléatoire $X$ par une loi
normale dénotée
\begin{equation*}
	X\sim\Norm(\mu,\sigma^2),
\end{equation*}
où $\mu\in\mathbb{R}$ est la moyenne et $\sigma^2>0$ la variance. On appel
aussi $sigma$ comme l'écart-type. L'espace échantillion est $S_X=\mathbb{R}$.

\begin{theoreme}
	$f_X(x)=\dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left(
		{-\dfrac{1}{2\sigma^2}(x-\mu)^2}
	\right)$.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\displaystyle\int_{-\infty}^{\frac{x-\mu}{\sigma}}
		\dfrac{1}{\sqrt{2\pi}}\exp\left(-\dfrac{z^2}{2}\right)\d{z}$
	où $z=\dfrac{t-\mu}{\sigma}$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\int_{-\infty}^\infty
	\frac{1}{\sqrt{2\pi}}\e^{-x^2/2}\d{x}=1$
\end{theoreme}

\begin{proof}
	Soit
	\begin{equation*}
		I=\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}\e^{-x^2/2}\d{x}
	\end{equation*}
	et
	\begin{equation*}
		I^2=
			\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}\e^{-x^2/2}\d{x}
			\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}\e^{-y^2/2}\d{y}.
	\end{equation*}

	Ensuite,
	\begin{equation*}
		I^2=\frac{1}{2\pi}\int_{-\infty}^\infty\int_{-\infty}^\infty
			\e^{-(x^2+y^2)/2}\d{x}\d{y}.
	\end{equation*}

	En utilisant les coordonnées polaires, on a
	\begin{equation*}
		I^2=\frac{1}{2\pi}\int_0^{2\pi}\int_0^\infty
			\e^{-r^2/2}r\d{r}\d{\theta},
	\end{equation*}
	de sorte qu'en posant posant $u=r^2$, on peut obtenir
	\begin{equation*}
		I^2=-\frac{1}{2\pi}\int_0^{2\pi}
			\e^{-r^2/2}\bigg\rvert_0^\infty\d{\theta}=
		-\frac{1}{2\pi}\int_0^{2\pi}\d{\theta}=1
	\end{equation*}
	de sorte que $I=1$.
\end{proof}

\subsection{Fonction d'une variable aléatoire}
\begin{exemple}
	On suppose que $X$ est la valeur d'ampliture d'un signal an temps $t$. Le
	signal numérisé peut s'écrire
	\begin{equation*}
		Y=\mathrm{signe}(X)\cdot\Delta\cdot\mathrm{part}\left(
			\frac{|X|}{\Delta}+\frac{1}{2}
		\right),
	\end{equation*}
	où $\Delta$ est le pas de quantification.
\end{exemple}

\subsubsection{$X$ et $Y$ sont des variables aléatoires discrètes}
%
% diagramme du mapping entre X -> Y selon Y(X)
%
\begin{exemple}
	Soit $X\sim\bin(2,\nicefrac{1}{4})$ et $Y=(X-1)^2$. Quelle est la fonction
	de masse de $Y$?

	On sait que $\P(Y=0)=\P(X=1)$ et $\P(Y=1)=\P(X=0)+\P(X=2)$. On sait que
	$\P(X=0)=\nicefrac{9}{16}$, $\P(X=1)=\nicefrac{6}{16}$ et $\P(X=0)=
	\nicefrac{1}{16}$ de sorte que $\P(Y=0)=\nicefrac{6}{16}$ et
	$\P(Y=1)=\nicefrac{10}{16}$.
\end{exemple}

\subsubsection{$X$ et $Y$ sont des variables aléatoires discrète et continue}
\begin{exemple}
	Soit $X\sim\Norm(0,1)$ et
	\begin{equation*}
		Y=\left\{
			\begin{matrix}
				-1&\text{si}&\phantom{-\nicefrac{1}{2}\leq}X<-\nicefrac{1}{2}\\
				 0&\text{si}&-\nicefrac{1}{2}\leq X<\nicefrac{1}{2}\\
				 1&\text{si}&\nicefrac{1}{2}\leq X\\
			\end{matrix}
		\right..
	\end{equation*}
	Quelle est la probabilité que $Y=1$?

	On cherche $\P(X\leq-\nicefrac{1}{2})=\Phi(-\nicefrac{1}{2})=
	\Phi(\nicefrac{1}{2})\approx\SI{0.3085}{}$. De plus, $\P(-\nicefrac{1}{2}
	\leq X\leq\nicefrac{1}{2})=\Phi(\nicefrac{1}{2})-\Phi(-\nicefrac{1}{2})
	\approx\SI{0.3830}{}$. Finalement, $\P(X<-\nicefrac{1}{2})\approx
	\SI{0.3085}{}$.
\end{exemple}

\subsubsection{$X$ et $Y$ sont de svariables aléatoires continues}
% P(Y<=y)=P(X^2<=y)=P(-sqrt(y)<=X<=sqrt(y)=F_X(sqrt(Y)-F_X(-sqrt(Y))
\begin{exemple}
	Soit $X\sim\uni(-1,2)$ et $Y=X^2$. Quelle est la fonction de répartition de
	$Y$?
\end{exemple}

\subsection{Espérance mathématique}
\begin{definition}
	Une \textit{espérance} d'une variable aléatoire $X$, dénotée $\Esp(X)$, est
	la somme des valeur possibles de $X$ pondérées par leur probabilité.
\end{definition}

\begin{theoreme}
	$\Esp(X)=\displaystyle\sum_{k=1}^\infty x_kP_X(x_k)$ si $X$ est discrète.
\end{theoreme}

\begin{theoreme}
	$\Esp(X)=\displaystyle\int_{-\infty}^\infty xf_X(x)\d{x}$ si $X$ est
	continue.
\end{theoreme}

\begin{exemple}
	Quelle est l'espérance d'un lancer d'un dé?

	On calcule l'espérance d'une variable discrète, soit
	\begin{equation*}
		\Esp(X)=
			1\cdot\frac{1}{6}+
			2\cdot\frac{1}{6}+
			3\cdot\frac{1}{6}+
			4\cdot\frac{1}{6}+
			5\cdot\frac{1}{6}+
			6\cdot\frac{1}{6}
		=3.5.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit $X\sim\poi(\alpha)$. Quelle est l'espérance de $X$?

	Par définition, on calcule l'espérance avec
	\begin{equation*}
		\Esp(X)
		=\sum_{k=0}^\infty k\cdot P_X(k)
		=\sum_{k=0}^\infty k\cdot\frac{\e^{-\alpha}\alpha^k}{k!}
		=\sum_{k=1}^\infty k\cdot\frac{\e^{-\alpha}\alpha^k}{k!},
	\end{equation*}
	car le premier terme à $k=0$ est nul. Par conséquent,
	\begin{equation*}
		\Esp(X)
		=\sum_{k=1}^\infty\frac{\e^{-\alpha}\alpha^k}{(k-1)!}
		=\e^{-\alpha}\sum_{k=1}^\infty\frac{\alpha^k}{(k-1)!}
		=\e^{-\alpha}\sum_{i=0}^\infty\frac{\alpha^{i+1}}{i!}
		=\e^{-\alpha}\alpha\sum_{i=0}^\infty\frac{\alpha^i}{i!}.
	\end{equation*}
	Hors, la somme est le développement en séries de la fonction exponentielle,
	alors
	\begin{equation*}
		\Esp(X)
		=\e^{-\alpha}\alpha\e^{\alpha}
		=\alpha.
	\end{equation*}
\end{exemple}

\pagebreak
\begin{exemple}
	Soit $X\sim\Exp(\lambda)$. Quelle est l'espérance de $X$?

	Par définition, on calcule l'espérance avec
	\begin{equation*}
		\Esp(X)
		=\int_{-\infty}^\infty xf_X(x)\d{x}
		=\int_0^\infty x\lambda\e^{-\lambda x}\d{x}.
	\end{equation*}
	En posant $u=x$ et $\d{v}=\lambda\e^{-\lambda x}\d{x}$, on obtient
	\begin{equation*}
		\Esp(X)
		=-x\e^{-\lambda x}\bigg\rvert_0^\infty
		+\int_0^\infty\e^{-\lambda x}\d{x}
		=\frac{1}{\lambda}\int_0^\infty\lambda\e^{-\lambda x}\d{x}
		=\frac{1}{\lambda},
	\end{equation*}
	car l'aire sous la fonction de densité de probabilités est égal à 1.
\end{exemple}

Soit $Y=g(X)$, où $X$ et $Y$ sont des variables aléatoires et $g$ une
tranformation. Si $X$ et $Y$ discrètes, alors
\begin{equation*}
	\Esp(Y)=\Esp(g(X))=\sum_{k=1}^\infty g(x_k)P_X(x_k),
\end{equation*}
et si $X$ et $Y$ sont continues, alors
\begin{equation*}
	\Esp(Y)=\Esp(g(X))=\int_{-\infty}^\infty g(x)f_X(x)\d{x}.
\end{equation*}

\begin{theoreme}
	$\Esp(c)=c$, où $c$ est une constante.
\end{theoreme}

\begin{theoreme}
	$\Esp(aX+b)=a\Esp(X)+b$, où $a$ et $b$ des constantes.
\end{theoreme}

\begin{theoreme}
	$\Esp(X|A)=\displaystyle\sum_{k=1}^\infty x_kP_X(x_k|A)$, où $X$ discrète.
\end{theoreme}

\begin{theoreme}
	$\Esp(X|A)=\displaystyle\int_{-\infty}^\infty xf_X(x|A)$, où $X$ continue.
\end{theoreme}

\begin{theoreme}
	$\Esp(X)=\displaystyle\sum_{i=1}^nE(X|B_i)\P(B_i)$, où $B_1,\dots,B_n$ des
	partitions de $S_X$.
\end{theoreme}

\begin{exemple}
	Soit $X$ une variable aléatoire mixte. Quelle est la forme de l'espérance
	de $X$?

	On définie $C$ comme l'événement où $X$ prend une valeur continue et $D$
	lorsque $X$ prend une valeur discrète. Par conséquent, on a
	\begin{equation*}
		\Esp(X)=\underbrace{\Esp(X|C)}_{\int}\P(C)
		+\underbrace{\Esp(X|D)}_{\sum}\P(D).
	\end{equation*}
\end{exemple}

\subsection{Variance}
\begin{definition}
	La \textit{variance} d'une variable aléatoire $X$, dénotée $\Var(X)$, est
	définie comme
	\begin{equation*}
		\Var(X)=\Esp\left[(X-\Esp(X))^2\right].
	\end{equation*}
\end{definition}

En développant le carré de la variance, on obtient
\begin{equation*}
	\Var(X)
	=\Esp\left[X^2-2X\Esp(X)+\Esp^2(X)\right]
	=\Esp(X^2)-2\Esp\left[X\Esp(X)\right]+\Esp\left[\Esp^2(X)\right],
\end{equation*}
et en simplifiant les constantes, on a
\begin{equation*}
	\Var(X)
	=\Esp(X^2)-2\Esp(X)\Esp(X)+\Esp^2(X)
	=\Esp(X^2)-\Esp^2(X)
\end{equation*}

\begin{exemple}
	Soit $X\sim\bin(1,p)$. Quelle est la variance de $X$?

	On sait que l'espérance de $X$ est
	\begin{equation*}
		\Esp(X)=0\cdot(1-p)+1\cdot p=p,
	\end{equation*}
	et celle de $X^2$ est
	\begin{equation*}
		\Esp(X^2)=0^2\cdot(1-p)+1^2\cdot p=p.
	\end{equation*}
	Par conséquent, la variance est
	\begin{equation*}
		\Var(X)=p-p^2=p(1-p).
	\end{equation*}
\end{exemple}

\begin{theoreme}
	$\Var(c)=c$, où $c$ est une constante.
\end{theoreme}

\begin{theoreme}
	$\Var(aX+b)=a^2\Var(X)$, où $a$ et $b$ des constantes.
\end{theoreme}

\begin{theoreme}
	$\Std(X)=\sqrt{\Var(X)}$.
\end{theoreme}

\begin{theoreme}
	$\Var(X|A)=\Esp(X^2|A)-\Esp^2(X|A)$.
\end{theoreme}

\subsection{Inégalité de Markov}
Soit $X$ une variable aléatoire prenant des valeurs non négatives. On peut
montrer que
\begin{equation*}
	\P(X\geq a)\leq\frac{\Esp(X)}{a},\forall a>0.
\end{equation*}

\subsection{Inégalité de Bienaymé-Tchebychev}
Soit $X$ une variable aléatoire dont la moyenne $\Esp(X)=\mu_X$ et la variance
$\Var(X)=\sigma_X^2$ existent. On peut montrer que
\begin{equation*}
	\P(|X-\mu_X|\geq a)\leq\frac{\sigma_X^2}{a^2},\forall a>0.
\end{equation*}

\begin{exemple}
	Soit le lancer d'une pièce de monnaie avec $X\sim\bin(n,\nicefrac{1}{2})$.
	On calcule la moyenne avec
	\begin{equation*}
		\Esp\left(\frac{X}{n}\right)
		=\frac{1}{n}\Esp(X)
		=\frac{1}{n}\cdot n\cdot\frac{1}{2}
		=\frac{1}{2}
	\end{equation*}
	et la variance avec
	\begin{equation*}
		\Var\left(\frac{X}{n}\right)
		=\frac{1}{n^2}\Var(X)
		=\frac{1}{n^2}\cdot n\cdot\frac{1}{2}\cdot\frac{1}{2}
		=\frac{1}{4n}.
	\end{equation*}

	Selon l'inégalité de Bienaymé-Tchebychev, on a
	\begin{equation*}
		\P\left(\left|\frac{X}{n}-\frac{1}{2}\right|\geq 0.01\right)
		\geq\frac{1/4n}{(0.01)^2}
		=\frac{10000}{4n}
		=\frac{2500}{n}.
	\end{equation*}
\end{exemple}

% nous ne faisons pas fonction moment/génératrice
\subsection{Fonction caractéristique}
\begin{definition}
	Une fonction \textit{caractéristique}, dénoté $\phi_X(\omega)$, est
	l'espérance d'une variable aléatoire $X$ tel que
	\begin{equation*}
		\phi_X(\omega)=\Esp\left(\e^{j\omega X}\right),
	\end{equation*}
	où $j$ est le nombre imaginaire tel que $j^2=-1$.
\end{definition}

\begin{theoreme}
	$\phi_X(\omega)=\displaystyle\sum_{k=1}^\infty\e^{j\omega x_k}\cdot
	P_X(x_k)$, si $X$ est discrète.
\end{theoreme}

\begin{theoreme}
	$\phi_X(\omega)=\displaystyle\int_{-\infty}^\infty\e^{j\omega X}\cdot
	f_X(x)\d{x}$, si $X$ est continue.
\end{theoreme}

\begin{theoreme}
	$\phi_X(0)=1$.
\end{theoreme}

\begin{theoreme}
	$\Esp(X^n)=(-j)^n\left[
	\dfrac{\d{}^n}{\d{\omega}^n}\phi_X(\omega)\right]_{\omega=0}$.
\end{theoreme}

\begin{exemple}
	Soit $X\sim\bin(n,p)$. Quelle est la fonction caractéristique de $X$?

	La fonction caractéristique de la loi binomiale est donnée par
	\begin{equation*}
		\phi_X(\omega)
		=\sum_{k=0}^n\e^{j\omega k}\cdot\comb{n}{k}p^kq^{n-k}
		=\sum_{k=0}^n\comb{n}{k}\left(p\e^{j\omega}\right)^kq^{n-k}
		=(p\e^{j\omega}+q)^n,
	\end{equation*}
	selon le binôme de Newton.
\end{exemple}

\pagebreak
\begin{exemple}
	Soit $X\sim\Norm(\mu,\sigma^2)$. On peut montrer que
	\begin{equation*}
		\phi_X(\omega)=\exp\left(j\omega\mu-\frac{1}{2}\omega^2\sigma^2\right).
	\end{equation*}
	Soit $Y=aX+b$. Quelle est la fonction caractéristique de $Y$?
	Par définition, on a
	\begin{equation*}
		\phi_Y(\omega)
		=\Esp\left(\e^{j\omega Y}\right)
		=\Esp\left(\e^{j\omega(aX+b)}\right)
		=\e^{j\omega b}\Esp\left(\e^{j\omega aX}\right)
		=\e^{j\omega b}\cdot\exp\left(j\omega a\mu+\frac{1}{2}
			\omega^2a^2\sigma^2
		\right)
	\end{equation*}
	de sorte à obtenir
	\begin{equation*}
		\phi_Y(\omega)=
		\exp\bigg[
			j\omega\underbrace{(a\mu+b)}_{\mu_Y}-
			\frac{1}{2}\omega^2\underbrace{(a^2\sigma)^2}_{\sigma_Y^2}
		\bigg].
	\end{equation*}
	La fonction $\phi_Y(\omega)$ est de la même forme que $\phi_X(\omega)$,
	alors on peut en déduire que $Y$ suit aussi une loi normale.
\end{exemple}

\subsection{Fiabilité}
On s'intéresse à la durée de vie des systèmes et de leurs composantes. Soit $T$
une variable aléatoire, continue et non-négative, représentant une durée de
vie. En général, l'espace d'échantillion est $S_T=[0,\infty[$.

On définie la fonction de fiabilité $R(T)$ tel que
\begin{equation*}
R(t)=\P(T>t)=1-F_T(t)
\end{equation*}
et le taux de défaillance $r(t)$ comme
\begin{equation*}
	r(t)
	=\lim_{s\rightarrow t}f_T(s|T>t)
	=\frac{f_T(t)}{1-F_T(t)}
	=-\frac{R\prime(t)}{R_(t)}
\end{equation*}

Sachant $r(t)$, il est possible de déterminer la fonction de fiabilité en
trouvant la solution de l'équation différentielle suivante, soit
\begin{equation*}
	r=-\frac{R\prime}{R},
\end{equation*}
avec $R(0)=1$, de sorte à obtenir
\begin{equation*}
	R(t)=\exp\left(-\int_0^tr(s)\d{s}\right).
\end{equation*}

\pagebreak
\begin{exemple}
	Soit $T\sim\Exp(\lambda)$. Quel est le taux de défaillance?

	On calcule la fonction de fiabilité avec
	\begin{equation*}
		R(t)
		=1-F_T(t)
		=1-(1-\e^{-\lambda t}
		=\e^{-\lambda t}
	\end{equation*}
	de sorte à obtenir le taux de défaillance
	\begin{equation*}
		r(t)
		=\frac{\lambda\e^{-\lambda t}}{\e^{-\lambda t}}
		=\lambda.
	\end{equation*}
	On remarque que si $T\sim\Exp(\lambda)$, alors $T$ a la propriété de
	non-vieillissement.
\end{exemple}

\subsubsection{Durée de vie moyenne}
La \textit{durée de vie moyenne} est l'espérance de $T$ et peut se calculer
selon
\begin{equation*}
	\Esp(T)=\int_0^\infty R(t)\d{t}.
\end{equation*}

\begin{proof}
	Par définition, on a
	\begin{equation*}
		\Esp(T)=\int_0^\infty t\cdot f_T(t)\d{t}.
	\end{equation*}
	Hors, on peut exprimé $t$ comme
	\begin{equation*}
		t=\int_0^t\d{s}
	\end{equation*}
	de sorte à obtenir
	\begin{equation*}
		\Esp(T)
		=\int_0^\infty\int_0^t f_T(t)\d{s}d{t}
		=\int_0^\infty\underbrace{\int_s^\infty f_T(t)\d{t}}_{\P(T>s)}\d{s}
		=\int_0^\infty R(t)\d{t},
	\end{equation*}
	en inversant l'ordre d'intégration.
\end{proof}

\begin{exemple}
	Soit un système en série à $n$ composantes. On suppose que les composantes
	fonctionnent indépendaments les un des autres. Quelle est la fonction de
	fiabilité du système?

	La probabilité que le système fonctionne après un temps $t$ est équivalent
	à la probabilité que tous les composantes fonctionnents après un temps $t$,
	soit
	\begin{equation*}
		\P(T>t)
		=\P(\left\{T_1>t\right\}\cap\cdots\cap\left\{T_N>t\right\})
		=\P(T_1>t)\cdots\P(T_n>t),
	\end{equation*}
	car les événements sont indépendants. 
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Par conséquent, on obtient
	\begin{equation*}
		R(t)=\prod_{k=1}^n R_k(t).
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit un système en parrallèle à $n$ composantes. On suppose que les
	composantes fonctionnents indépendaments les un de sautres. Quelle est la
	fonction de fiablité dus système?

	La probabilité que le système soit en panne est équivalent à la probabilité
	que tous les composantes soit en panne après un temps $t$, soit
	\begin{equation*}
		\P(T\leq t)
		=\P(\left\{T_1\leq t\right\}\cap\cdots\cap\left\{T_n\leq t\right\})
		=\P(T_1\leq t)\cdots\P(T_n\leq t),
	\end{equation*}
	car les événements sont indépendants. Par conséquent, on obtient
	\begin{equation*}
		R(t)=1-\prod_{k=1}^n\bigg[1-R_k(t)\bigg].
	\end{equation*}
\end{exemple}

\section{Vecteurs aléatoires}
\begin{definition}
	Un \textit{vecteur aléatoire} est formé de plusieurs variables aléatoires
	observées lors d'une même expérience.
\end{definition}

\begin{exemple}
	Soit le lancer de 2 dés.

	On pose $X$ comme étant le résultat du premier dé et $Y$ comme étant le 
	résultat du deuxième dé. On défini le vecteur aléatoire $(X,Y)$. L'espace
	échantillion est $S_{X,Y}=\left\{(1,1),(1,2),\dots,(6,5),(6,6)\right\}$.

	La probabilité d'avoir un évènement est
	\begin{equation*}
		\P(\left\{X=j\right\}]\cap\left\{Y=i\right\})=\frac{1}{36},
	\end{equation*}
	pour $i=1,\dots,6$ et $j=1,\dots,6$.
\end{exemple}

\pagebreak
\begin{exemple}
	On génère un point au hasard dans le triangle $T$ de coordonnées $(0,0)$,
	$(1,1)$ et $(0,1)$.

	Chaque point dans le triangle peut être considéré comme une vecteur
	aléatoire $(X,Y)$ où $X$ est l'abscisse et $Y$ l'ordonnée. Soit
	$f_{X,Y}(x,y)$ la fonction de masse du vecteur tel que
	\begin{equation*}
		f_{X,Y}(x,y)=\left\{
			\begin{array}{rl}
				c&\text{si $(x,y)$ est dans le triangle}\\
				0&\text{sinon}\\
			\end{array}
		\right.,
	\end{equation*}
	où la constante $c$ est donnée par
	\begin{equation*}
		\iint_Tf_{X,Y}(x,y)\d{A}=1\Leftrightarrow c=2.
	\end{equation*}
\end{exemple}

\subsection{Vecteur aléatoire discret}
\subsubsection{Fonction de masse conjointe}
\begin{theoreme}
	$p_{X,Y}(x_j,y_k)=\P(\left\{X=x_j\right\}\cap\left\{Y=y_k\right\})$.
\end{theoreme}

\begin{theoreme}
	$p_{X,Y}(x_j,y_k)\geq 0$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\sum_{j=1}^\infty\displaystyle\sum_{k=1}^\infty
	p_{X,Y}(x_j,y_k)=1$.
\end{theoreme}

\subsubsection{Fonction de masse marginale}
\begin{theoreme}
	$p_X(x_j)=\displaystyle\sum_{k=0}^\infty p_{X,Y}(x_j,y_k)$.
\end{theoreme}

\begin{theoreme}
	$p_Y(y_k)=\displaystyle\sum_{j=0}^\infty p_{X,Y}(x_j,y_k)$.
\end{theoreme}

\subsubsection{Fonction de masse conditionnelle}
\begin{theoreme}
	$p_{Y|X}(y_k|x_j)=\dfrac{p_{X,Y}(x_j,y_k)}{p_X(x_j)}$.
\end{theoreme}

\begin{theoreme}
	$p_{X|Y}(x_j|y_k)=\dfrac{p_{X,Y}(x_j,y_k)}{p_Y(y_k)}$.
\end{theoreme}

\begin{theoreme}
	$\P(A)=\displaystyle\sum\displaystyle\sum_Ap_{X,Y}(x_j,y_k)$.
\end{theoreme}

\subsection{Vecteur aléatoire continu}
\subsubsection{Fonction de densité conjointe}
\begin{theoreme}
	$f_{X,Y}(x,y)=\dfrac{1}{\delta\epsilon}\P\left(
		\left\{
			x-\dfrac{\delta}{2}\leq X\leq x+\dfrac{\delta}{2}
		\right\}\cap
		\left\{
			y-\dfrac{\epsilon}{2}\leq Y\leq y+\dfrac{\epsilon}{2}
		\right\}
	\right)$
\end{theoreme}

\begin{theoreme}
	$\displaystyle\int_{-\infty}^\infty\displaystyle\int_{-\infty}^\infty
		f_{X,Y}(x,y)\d{x}\d{y}=1$.
\end{theoreme}

\subsubsection{Fonction de densité marginale}
\begin{theoreme}
	$f_X(x)=\displaystyle\int_{-\infty}^\infty f_{X,Y}(x,y)\d{y}$.
\end{theoreme}

\begin{theoreme}
	$f_Y(y)=\displaystyle\int_{-\infty}^\infty f_{X,Y}(x,y)\d{x}$.
\end{theoreme}

\subsubsection{Fonction de densité conditionnelle}

\begin{theoreme}
	$f_{Y|X}(y|x)=\dfrac{f_{X,Y}(x,y)}{f_X(x)}$.
\end{theoreme}

\begin{theoreme}
	$f_{X|Y}(x|y)=\dfrac{f_{X,Y}(x,y)}{f_Y(y)}$.
\end{theoreme}

\begin{theoreme}
	$\P(A)=\displaystyle\iint_Af_{X,Y}(x,y)\d{A}$.
\end{theoreme}

\subsection{Fonction de répartition conjointe}
\begin{theoreme}
	$F_{X,Y}(x,y)=\P(\left\{X\leq x\right\}\cap\left\{Y\leq y\right\})$.
\end{theoreme}

\begin{theoreme}
	$F_{X,Y}(x,y)=\displaystyle\sum_{x_j\leq x}
	\displaystyle\sum_{y_k\leq y}p_{X,Y}(x_j,y_k)$ dans le cas discret.
\end{theoreme}

\begin{theoreme}
	$F_{X,Y}(x,y)=\displaystyle\int_{-\infty}^x
	\displaystyle\int_{-\infty}^yf_{X,Y}(s,t)\d{t}\d{s}$ dans le cas continue.
\end{theoreme}

\begin{theoreme}
	$f_{X,Y}(x,y)=\dfrac{\p{}^2}{\p{x}\p{y}}F_{X,Y}(x,y)$.
\end{theoreme}

\subsection{Fonction de répartition marginale}
\begin{theoreme}
	$F_X(x)=\displaystyle\lim_{y\leftarrow\infty}F_{X,Y}(x,y)$.
\end{theoreme}

\begin{theoreme}
	$F_Y(y)=\displaystyle\lim_{x\leftarrow\infty}F_{X,Y}(x,y)$.
\end{theoreme}

\begin{exemple}
	Quelle est la probabilité que $(X,Y)$ soit dans un rectangle $R$? On
	suppose qu'on connait $F_{X,Y}(x,y)=\P(\left\{X\leq x\right\}\cap
	\left\{Y\leq y\right\})$.

	\begin{equation*}
		\P(R)
		=F_{X,Y}(b,d)-F_{X,Y}(b,c)-F_{X,Y}(a,d)+2F_{X,Y}(a,c).
	\end{equation*}
\end{exemple}

\end{document}

