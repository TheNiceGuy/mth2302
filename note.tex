\documentclass[11pt]{article}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{standalone}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{siunitx}
\usepackage{gphys}
\usepackage{thmtools}
\usepackage{mdframed}
\usepackage{float}
\usepackage{tikz}
\usepackage{contour}
\usepackage{scrextend}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{nicefrac}
\usepackage{pgfplots}

\usetikzlibrary{babel}
\usetikzlibrary{calc}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{decorations.markings}

%%%%%%%%%%%%%%%%%%
% Configurations %
%%%%%%%%%%%%%%%%%%

\geometry{
	top=34mm,
	bottom=34mm
}

%%%%%%%%%%
% Macros %
%%%%%%%%%%

\input{eq}

\newcommand\card{%
	\ensuremath{%
		\mathrm{card}\,%
	}%
}%

\newcommand\comb[2]{%
	\ensuremath{%
		\mathcal{C}_{#2}^{#1}%
	}%
}%

\newcommand\perm[2]{%
	\ensuremath{%
		\mathcal{P}_{#2}^{#1}%
	}%
}%

\renewcommand\P[1]{%
	\ensuremath{%
		\mathbb{P}\left[\,#1\,\right]%
	}%
}%

\newcommand\Pg[2]{%
	\ensuremath{%
		\mathbb{P}\left[\,#1\middle\rvert#2\,\right]%
	}%
}%

\newcommand\e{%
	\ensuremath{%
		\mathrm{e}%
	}%
}%

\newcommand\Bin[2]{%
	\ensuremath{%
		\mathcal{B}\left(#1,#2\right)%
	}%
}%

\newcommand\Geo[1]{%
	\ensuremath{%
		\mathcal{G}\left(#1\right)%
	}%
}%

\newcommand\Poi[1]{%
	\ensuremath{%
		\mathrm{Poi}\left(#1\right)%
	}%
}%

\newcommand\Uni[2]{%
	\ensuremath{%
		\mathcal{U}\left(#1,#2\right)%
	}%
}%

\newcommand\Exp[1]{%
	\ensuremath{%
		\mathrm{Exp}\left(#1\right)%
	}%
}%

\newcommand\Gam[2]{%
	\ensuremath{%
		\mathrm{Gam}\left(#1,#2\right)%
	}%
}%

\newcommand\Norm[2]{%
	\ensuremath{%
		\mathcal{N}\left(#1,#2\right)%
	}%
}%

\newcommand\BiNorm[5]{%
	\ensuremath{%
		\mathcal{N}\left(#1,#2,#3,#4,#5\right)%
	}%
}%

\newcommand\Esp[1]{%
	\ensuremath{%
		\mathrm{E}\left[\,#1\,\right]%
	}%
}%

\newcommand\Espg[2]{%
	\ensuremath{%
		\mathrm{E}\left[\,#1\middle\vert#2\,\right]%
	}%
}%

\newcommand\Var[1]{%
	\ensuremath{%
		\mathrm{Var}\left[\,#1\,\right]%
	}%
}%

\newcommand\Varg[2]{%
	\ensuremath{%
		\mathrm{Var}\left[\,#1\middle\vert#2\,\right]%
	}%
}%

\newcommand\Std[1]{%
	\ensuremath{%
		\mathrm{Std}\left[\,#1\,\right]%
	}%
}%

\newcommand\Cov[2]{%
	\ensuremath{%
		\mathrm{Cov}\left[#1,#2\right]%
	}%
}%

\newcommand\Min{
	\ensuremath{%
		\mathrm{min}\,%
	}%
}

\newtheorem{axiome}{Axiome}
\newtheorem{theoreme}{Théoreme}[section]
\newtheorem*{proposition}{Proposition}
\newtheorem*{exemple*}{Exemple}

\declaretheoremstyle[
	notefont=\bfseries,
	notebraces={}{},
	bodyfont=\normalfont,
	postheadspace=0.5em,
	numbered=no,
]{basicstyle}
\declaretheorem[name=Définition,style=basicstyle]{definition}

\newmdtheoremenv[
	linecolor=black,
	backgroundcolor=gray!40,
	ntheorem
]{exemple}{Exemple}[section]

%%%%%%%%%%%%%
% Documents %
%%%%%%%%%%%%%

\begin{document}
\tableofcontents
\pagebreak

\section{Élèments de probabilités}
\begin{definition}
	Une expérience est \textit{aléatoire} si un observateur peut la répéter
	dans les mêmes conditions, mais sans pouvoir en prédire le résultat.
\end{definition}

\begin{definition}
	Un \textit{espace échantilion} est un ensemble $S$ des résultats possibles.
\end{definition}

Un espace d'échantilion peut être \textit{qualitatif} ou \textit{quantitatif},
ainsi que \textit{discret}, \textit{continu} ou \textit{mixte}. Il peut aussi
être \textit{dénombrable} ou \textit{non-dénombrable}.

\begin{definition}
	Un \textit{évènement} $A$ est un sous-ensemble de $S$ d'intêret à
	l'observateur.
\end{definition}

\begin{definition}
	Un \textit{évènement élémentaire} $A$ est un résultat particulier,
	c'est-à-dire, un élèment de $S$.
\end{definition}

La différence entre les deux dernières définitions est que la $\card(A)=1$ pour
un évènement élémentaire tandis que $\card(A)\geq 1$ pour un évènement.

\begin{exemple}
	On observe le résultat du lancer de deux pièces de monnaie. On note $P$
	comme un lancer pile et $F$ comme un lancer face. L'ensemble est donc
	\begin{equation*}
		S=\left\{PP, FF, PF, FP\right\}
	\end{equation*}
	avec chaque résultat ayant \SI{25}{\percent} d'arriver. L'ensemble $S$ est
	qualitatif, soit pile ou face, et discret.
\end{exemple}

\begin{exemple}
	On observer la somme obtenue lors du lancer de deux dés à 6 faces.
	L'ensemble de résultat possible est
	\begin{equation*}
		S=\left\{2,3,4,5,6,7,8,9,10,11,12\right\}.
	\end{equation*}
	Il est quantitatif et discret.
\end{exemple}

\begin{exemple}
	On compte le nombre de lancers d'une pièce pour obtenir une première fois
	un pile. L'espace échantilion est
	\begin{equation*}
		S=\left\{1,2,3,4,5,\dots\right\},
	\end{equation*}
	car il est possible qu'une grande quantité de lancer est effectuée avant
	d'obtenir un pile. L'ensemble $S$ est quantitatif, discret et
	infini dénombrable.
\end{exemple}

\pagebreak
\begin{exemple}
	On mesure le temps d'attente à l'arrêt d'autobus. L'espace échantilion est
	\begin{equation*}
		S=\left[0,\infty\right[.
	\end{equation*}
	L'ensemble $S$ est quantitatif, continu et infini non-dénombrable.
\end{exemple}

\begin{exemple}
	On mesure le temps d'attente à l'arrêt d'autobus ainsi que le nombre de
	personnes en file à l'arrivée de l'autobus. L'espace échantilion est
	\begin{equation*}
		S=T\times U,
	\end{equation*}
	avec
	\begin{equation*}
		T=\left[0,\infty\right[
	\end{equation*}
	et
	\begin{equation*}
		U=\left\{1,2,3,4,5,\dots\right\}.
	\end{equation*}
	L'ensemble $S$ est quantitatif et mixte. Un exemple d'évènement élèmentaire
	peut être un couple tel que $\left(\SI{4.25}{\second}, 4\right)$.
\end{exemple}

\subsection{Lien entre l'expérience aléatoire et son modèle}
\begin{definition}
	La \textit{fréquence relative} $f_A$ d'un évènement $A$ est le rapport
	entre le nombre d'observations $n_A$ de l'évèmenent et le nombre $n$ de
	répétition de l'expérience, c'est-à-dire
	\begin{equation*}
		f_A=\frac{n_A}{n}.
	\end{equation*}
	La limite lorsque l'expérience est répétée infiniment est la probabilité de
	l'évènement $A$, dénotée
	\begin{equation*}
		\P{A}=\lim_{n\rightarrow\infty}f_A.
	\end{equation*}
\end{definition}

\subsection{Opérations sur les ensembles}
Soit deux ensembles $A$ et $B$ tel que $A,B\subset S$. La figure
\ref{fig:venn_intersection} montre une intersection tandis que la figure
\ref{fig:venn_union} montre une union entre $A$ et $B$. La figure
\ref{fig:venn_complement} montre le complémenet de $A$ et la figure
\ref{fig:venn_exclusion} montre l'exclusion de deux ensembles.

\begin{figure}[H]
	\centering
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/intersection}
		\caption{$A\cap B$}
		\label{fig:venn_intersection}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/union}
		\caption{$A\cup B$}
		\label{fig:venn_union}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/complement}
		\caption{$A^c$}
		\label{fig:venn_complement}
	\end{subfigure}
	~
	\begin{subfigure}[H]{0.21\textwidth}
		\centering
		\input{figure/combinaison}
		\caption{$\left(A\cup B\right)\setminus\left(A\cap B\right)$}
		\label{fig:venn_exclusion}
	\end{subfigure}
	\caption{Opérations d'ensembles démontrées sur des diagrammes de Venn}
\end{figure}

\subsection{Axiomes fondamentales de la probabilité}
\begin{axiome}
	La probabilité d'un évèment $A$ est plus grand ou égal à 0, c'est-à-dire
	\begin{equation*}
		\P{A}\geq 0,
	\end{equation*}
	pour tout $A\in S$.
\end{axiome}

\begin{axiome}
	La probabilité de l'espace d'échantilion $S$ est 1, c'est-à-dire
	\begin{equation*}
		\P{S}=1.
	\end{equation*}
\end{axiome}

\begin{axiome}
	La probabilité d'un évènement $A$ ou d'un évènement $B$ est équivalent à la
	somme de leur probabilité, c'est-à-dire
	\begin{equation*}
		\P{A\cup B}=\P{A}+\P{B},
	\end{equation*}
	si $A\cup B=\emptyset$.

	En général, 
	\begin{equation*}
		\P{\bigcup_{k=1}^\infty A_k}=\sum_{k=1}^\infty\P{A_k},
	\end{equation*}
	si $A_i\cup A_j=\emptyset$, $\forall i,j$.
\end{axiome}

\begin{theoreme}
	$\P{A^c}=1-\P{A}$.
\end{theoreme}

\begin{proof}
	On sait que $A\cup A^c=S$ et $A\cap A^c=\emptyset$. Hors,
	$\P{A\cup A^c}=\P{S}\Leftrightarrow\P{A}+\P{A^c}=1$, car
	$A\cup A^c=\emptyset$.
	En réarrangeant, on obtient que $\P{A^c}=1-\P{A}.$
\end{proof}

\begin{theoreme}
	$\P{A}\leq 1$.
\end{theoreme}

\begin{proof}
	On sait que $\P{A^c}\geq 0$ et $\P{A^c}=1-\P{A}$. En réarrangeant, on
	obtient que $\P{A}\leq 1$.
\end{proof}

\begin{theoreme}
	$\P{\emptyset}=0$.
\end{theoreme}

\begin{proof}
	On sait que $S^c=\emptyset$. Par conséquent, $\P{\emptyset}=1-\P{S}=1-1=0$.
\end{proof}

\begin{theoreme}
	$A\subset B\Rightarrow \P{A}\leq\P{B}$.
\end{theoreme}

\begin{proof}
	La différence entre $A$ et $B$ est $A^c\cap B$ de sorte qu'on peut écrire
	$B=A\cup(A^c\cap B)$. Par conséquent, $\P{B}=\P{A}+\P{A^c\cap B}
	\Leftrightarrow\P{B}-\P{A}=\P{A^c\cap B}\geq 0$, car
	$A\cup(A^c\cap B)=\emptyset$. En réarrangeant, on obtient $\P{A}\leq\P{B}$.
\end{proof}

\begin{theoreme}
	$\P{A\cup B}=\P{A}+\P{B}-\P{A\cap B}$.
\end{theoreme}

\subsection{Principe d'équiprobabilité}
\subsubsection{Ensemble fini}
On suppose que $S$ est fini, soit $S=\left\{e_1,e_2,\dots,e_n\right\}$. On dit
que les résultats d'une expérience aléatoire sont \textit{équiprobables} si
\begin{equation*}
	\P{e_1}=\P{e_2}=\cdots=\P{e_n}=\frac{1}{n}.
\end{equation*}
Dans ce cas, on a que la probabilité d'un évènement $A\subset S$ est
\begin{equation*}
	\P{A}=\frac{n_A}{n},
\end{equation*}
où $n_A$ est le nombre d'élément dans $A$ et $n$ celui dans $S$.

\subsubsection{Ensemble infini dénombrable}
On suppose que $S$ est infini dénombrable, alors l'équiprobabilité est
impossible. Si la probabilité d'un évènement élémentaire est $\P{e_i}=\epsilon$,
on obtient que $\P{S}=\P{e_1}+\P{e_2}+\cdots=\infty$, Ce qui est en contradiction
avec les axiomes. D'une manière similaire, si $\P{e_i}=0$, alors $\P{S}=0$, ce
qui est aussi en contradiction avec les axiomes.

\subsubsection{Ensemble infini non-dénombrable}
On suppose que $S=[a,b]$ est infini non-dénombrable. Soit un évènement
$A=[c,d]\subset S$. La probabilité de l'évènement est
\begin{equation*}
	\P{A}=\frac{l_A}{l}=\frac{d-c}{b-a},
\end{equation*}
où $l_A$ est la longueur de $A$ et $l$ la longueur de $S$. Par conséquent, la
probabilité d'un évènement élémentaire $e\in S$ est
\begin{equation*}
	\P{e}=0,
\end{equation*}
car $l_A=0$.

\begin{exemple}
	On calcule la somme de deux dés lancés. Quelle est la probabilité d'obtenir
	chaque somme possible?
	
	L'espace échantilion est $S=L\times L$, où $L=\left\{1,2,3,4,5,6\right\}$
	est le résultat possible d'un dé. On peut écrire
	$S=\left\{(1,1),(1,2),\dots,(6,6)\right\}$. On suppose qu'il y a
	équiprobabilité de sorte que la probabilité d'obtenir un évènement
	élémentaire est $\nicefrac{1}{36}$.

	Hors, certaines des sommes sont dupliquées de sorte que la probabilité
	d'obtenir une somme particulière est donnée par la table suivante.
	\begin{table}[H]
		\centering
		\begin{tabular}{r|ccccccccccc}
			$\phantom{\mathbb{P}(}A\phantom{)}$ &
			$2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$ & $11$ & $12$\\
			$\P{A}$ &
			$\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ &
			$\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ &
			$\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\\
		\end{tabular}
	\end{table}
\end{exemple}

\section{Analyse combinatoire}
\subsection{Diagramme en arbre}

\begin{exemple}
	On lance un dé, puis une pièce de monnaie. Combien de résultats est-il
	possible?
	
	On peut représenté la situation par l'arbre suivant.
	\begin{figure}[H]
		\centering
		\input{figure/diagramme_arbre.tex}
	\end{figure}
	Par le principe de multiplication, il y a $6\cdot 2=12$ possibilités.
\end{exemple}

\subsection{Permutations}
\begin{definition}
	Une \textit{permutation} correspond à un choix de $k$ objets parmi $n$
	objets distincts. Le choix se fait sans remise et dans un ordre spécifique.
\end{definition}

La table \ref{tb:permutation} résume le principe d'une permutation à l'aide
d'une pige d'objet. À chaque pige, la quantité d'objets diminue de 1.

\begin{table}[H]
	\centering
	\caption{Pige d'objets}
	\begin{tabular}{r|cccc}
		\toprule
		Choix d'objet   &   1 &   2   & $\dots$ & k\\
		Objets restants & $n$ & $n-1$ & $\dots$ & $n-(n-k)$\\
		\bottomrule
	\end{tabular}
	\label{tb:permutation}
\end{table}

À l'aide du principe de multiplication, il y a $n\cdot(n-1)\cdots(n-k+1)$
combinaisons. On dénote le nombre de permutations sans remise de $n$ éléments
par
\begin{equation*}
	\perm{k}{n}=\frac{n!}{(n-k)!}.
\end{equation*}
Lorsqu'il y a remise, le nombre de permutations de $n$ éléments est $n^k$.

\begin{exemple}
	On dispose de 10 composantes dont 4 défectueuses. On pige 3 composantes au
	hasard et sans remise. Quelle est la probabilité d'obtenir uniquement des
	composantes non défectueuses?

	On suppose qu'il y a équiprobabilité du choix des 3 composantes. La
	probabilité d'obtenir 3 composantes non défecteuses $F$ est
	\begin{equation*}
		\P{F}
		=\frac{6\cdot 5\cdot 4}{10\cdot 9\cdot 8}
		=\frac{\perm{3}{6}}{\perm{3}{10}}
		=\frac{1}{6}.
	\end{equation*}
\end{exemple}

\subsection{Combinaisons}
\begin{definition}
	Une \textit{combinaison} correspond au choix de $k$ objets parmis $n$
	objets distincts. Le choix se fait sans remise et sans ordre spécifique.
\end{definition}

Soit $C_k^n$ le nombre de combinaisons. Le nombre de permutations est égal au
nombre de combinaisons multiplié par le nombre d'arrangement possible $k!$,
c'est-à-dire 
\begin{equation*}
	\perm{k}{n}=\comb{k}{n}\cdot k!,
\end{equation*}
et en réarrangeant, on obtient
\begin{equation*}
	\comb{k}{n}={{n}\choose{k}}=\frac{n!}{k!(n-k)!}.
\end{equation*}

\begin{exemple}
	Combien de codes de deux lettres peut-on former à partir du mot
	\texttt{OUI}?
	
	Avec ordre, il y a $\perm{2}{3}=6$ permutations et sans ordre, il y a
	$\comb{2}{3}=3$ combinaisons.
\end{exemple}

\begin{exemple}
	Quel est la probabilité de gagner le gros lot à la $6/49$?
	
	Sans ordre, il y a 1 seul cas favorable et $\comb{6}{49}$ cas possibles.
	Par conséquent, la probabilité de gagner $G$ est
	\begin{equation*}
		\P{G}=\frac{1}{\comb{6}{49}}=\frac{1}{\SI{13983816}{}}.
	\end{equation*}
	Avec ordre, il y a $6!$ cas favorables et $\perm{6}{49}$ cas possibles de
	sorte que la probabilité est
	\begin{equation*}
		\P{G}=\frac{6!}{\perm{6}{49}}=\frac{720}{\SI{10068347520}{}}.
	\end{equation*}
\end{exemple}

\subsubsection{Triangle de Pascal}
Une combinaison peut se représenter avec le triangle de Pascal comme le montre
la figure \ref{fig:pascal}.

\begin{figure}[H]
	\centering
	\input{figure/pascal}
	\caption{représentation du triangle de Pascal}
	\label{fig:pascal}
\end{figure}

\begin{theoreme}
	$\comb{k}{n}=\comb{k-1}{n-1}+\comb{k}{n-1}$
\end{theoreme}

\begin{proof}
	La démonstration est triviale et est laissée au lecteur.
\end{proof}

\subsubsection{Binôme de Newton}
La combinaison est souvent appliquée dans le cas de la puissance d'un binôme
tel que
\begin{equation*}
	\left(a+b\right)^n
	=\sum_{k=0}^n\comb{k}{n}\cdot a^k\cdot b^{n-k}
	=\sum_{k=0}^n{{k}\choose{n}}\cdot a^k\cdot b^{n-k},
\end{equation*}
mais elle est plus souvent utilisée avec la deuxième notation.

\subsection{Permutation d'objets semblables}
\begin{exemple}
	Combien y a-t-il d'ordres possibles des lettres <<ppfff>>?

	Soit 5 cases distinctes représentant l'ordre d'une pige dans les lettres.
	Il faut choisir les cases où mettre les <<p>>, soit
	\begin{equation*}
		\comb{2}{5}=\frac{5!}{2!3!}=10
	\end{equation*}
	ordres possibles.
	
	Si on échange un <<p>> et un <<f>> dans une séquence $A$, on obtient une
	séquence $B$ différente de $A$. Si on échange un <<p>> avec une autre <<p>>
	dans une séquence $A$, on retrouve la même séquence $A$.

	En analysant la formule, le facteur $5!$ représente le nombre d'ordres si
	toutes les lettres étaient différentes. Le facteur $2!$ représente les
	<<p>> s'ils étaient différents et le facteur $3!$ représente le nombres des
	<<f>> s'ils étaient différents.
\end{exemple}

En général, avec $n$ objets objets comprenant $n_1$ objets de classes $1$,
$n_2$ objets de classe $2$, $\dots$, $n_k$ objets à la classe $k$, on a
\begin{equation*}
	\frac{n!}{n_1!n_2!\cdots n_k!}
\end{equation*}
ordres possibles.

\begin{exemple}\label{ex:network}
	Soit le graphe $n\times n$ suivant.
	\begin{figure}[H]
		\vspace{-3mm}
		\centering
		\input{figure/network}
		\vspace{-3mm}
	\end{figure}

	Combien de chemins existe entre $A$ et $B$ suivant, s'il est seulement
	permis d'aller vers la droite ou vert le haut?
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	On suppose que tous les chemins possibles sont équiprobables. Peu importe
	le chemin, il faut avancer $n-1$ vers la droite et $n-1$ vers le haut pour
	un total de $2n-2$ mouvement.
	
	Il suffit de calculer le nombre de permutations de ces mouvements sachant
	qu'il y des objets semblables. Le nombre chemins est
	\begin{equation*}
		\frac{\left(2n-2\right)!}{\left(n-1\right)!\left(n-1\right)!},
	\end{equation*}
	ce qui est équivalent à $\comb{n-1}{2n-2}$.
\end{exemple}


\subsection{Équivalence}
\begin{exemple}\label{ex:network_broken}
	Soit le graphe $n\times n$ suivant.
	\begin{figure}[H]
		\vspace{-3mm}
		\centering
		\input{figure/network_broken}
		\vspace{-3mm}
	\end{figure}

	Quel est la probabilité qu'un chemin choisi au hasard, allant seulement
	vers la droite ou le haut, ne traverse pas les noeuds au-dessus de la
	diagonale entre $A$ et $B$?

	Les <<bons>> chemins ne passent pas par la ligne critique en pointillée et
	les <<mauvais>> chemins passent par la ligne critique. La probabilité d'un
	bon chemin $B$ peut s'écrire en fonction de son complément, soit
	\begin{equation*}
		\P{B}=1-\P{M}=1-\frac{n_m}{C_{n-1}^{2n-2}},
	\end{equation*}
	où $M$ est un mauvais chemin et $n_m$ est le nombre de mauvais chemins.

	Soit un mauvais chemin $M$. On définit le point $X$ comme étant le premier
	point au-dessus de la diagonale que $M$ atteint. On applique une
	transformation mirroir au chemin suivant $X$ comme la prochaine figure.

	\noindent
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/network_bad}
		\end{figure}
	\end{minipage}%
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/network_transformed}
		\end{figure}
	\end{minipage}
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Il s'avère que tous les mauvais chemins dans le graphe $n\times n$
	correspond à un chemin unique dans le graphe $(n-1)\times(n+1)$.
	C'est-à-dire la transformation est bijective.
	
	Par conséquent, le problème est équivalent à trouver le nombre de chemins
	dans un graphe $(n-1)\times(n+1)$. D'une manière similaire à l'exemple
	\ref{ex:network}, ce nombre de chemins est donné par
	\begin{equation*}
		\frac{(2n-2)!}{(n-2)!n!},
	\end{equation*}
	ce qui est équivalent à $\comb{2n-2}{n-2}$.

	Par conséquent, la probabilité d'avoir un bon chemin au hasard est
	\begin{equation*}
		\P{B}
		=1-\frac{\comb{n-2}{2n-2}}{\comb{n-1}{2n-2}}
		=\frac{1}{n}.
	\end{equation*}
\end{exemple}

\subsection{Réccurence}
\begin{exemple}
	Il est possible de résoudre le problème à l'exemple \ref{ex:network_broken}
	à l'aide de la réccursion.
	
	En effet, le nombre de <<bons>> chemins $b_{i,j}$ à partir d'un noeud est
	la somme des <<bons>> chemins des noeuds à droite et en haut, c'est-à-dire
	\begin{equation*}
		b_{i,j}=b_{i-1,j}+b_{i,j-1},
	\end{equation*}
	où $i$ est le nombre de noeuds restants vers la droite et $j$ la nombre de
	noeuds restants vers le haut.

	On sait que $b_{0,0}=1$, car il ne reste plus de noeud à parcourir fois
	rendu à $B$. Aussi, $b_{0,j}=1$, car il est seulement possible de se rendre
	à $B$ en allant vers le haut. Sur la diagonale, on a $b_{i,i}=b_{i-1,j}$,
	car on ne peut pas aller vers le haut. Par conséquent, on obtient le
	système d'équations à reccurence suivant,
	\begin{equation*}
		b_{i,j}=\left\{
			\begin{matrix}
				0,                 &\text{si}&i=0,j=0\\
				1,                 &\text{si}&i=0\\
				b_{i-1,j},         &\text{si}&i=j\\
				b_{i-1,j}+b_{i,j-1}&\text{sinon}
			\end{matrix}
		\right.
	\end{equation*}

	Pour une grille $4\times 4$, on peut calculer le nombre de <<bons>> chemins
	en développant la récurrence afin d'obtenir $b_{3,3}=5$ chemins.
\end{exemple}

\section{Probabilité conditionnelles}
\begin{definition}
	Une \textit{probabilité conditionnelle} est la probabilité qu'un évènement
	$A$ se réalise, si $B$ s'est réalisé.
\end{definition}

Mathématiquement, on dénote une probabilité conditionnelle avec
\begin{equation*}
	\Pg{A}{B}=\frac{\P{A\cap B}}{\P{B}}.
\end{equation*}
où $A$ et $B$ sont des évènements.

\begin{exemple}
	Soit le lancement d'un dé avec les évènements $A=\left\{5,6\right\}$ et
	$B=\left\{2,4,6\right\}$, alors $\P{A}=\nicefrac{1}{3}$ et 
	\begin{equation*}
		\Pg{A}{B}=\frac{\P{\left\{6\right\}}}{\P{B}}=\frac{1}{3}.
	\end{equation*}

	Si $A=\left\{6\right\}$, alors $\P{A}=\nicefrac{1}{6}$ et
	\begin{equation*}
		\Pg{A}{B}=\frac{\P{\left\{6\right\}}}{\P{B}}=\frac{1}{3}.
	\end{equation*}
\end{exemple}

\subsection{Propriétés}
\begin{theoreme}
	$\P{A\cap B}=\Pg{A}{B}\cdot\P{B}$.
\end{theoreme}

\begin{theoreme}
	$\Pg{A}{B}=\Pg{B}{A}=0$, si $A\cap B=\emptyset$.
\end{theoreme}

\begin{theoreme}
	$\Pg{A}{B}\neq\Pg{B}{A}$, en général.
\end{theoreme}

\begin{theoreme}
	$\Pg{A}{S}=\P{A}$, $A\in S$.
\end{theoreme}

Le dernier théorème résulte que toutes probabilités peut s'exprimer sous la
forme d'une probabilité conditionnelle.

\begin{exemple}
	On pige sans remise 3 composantes non défecteuses parmis 10 composantes, donc 4 sont
	défecteuses. Soit $A_i$ le $i^\text{e}$ composante non défecteuses.
	\begin{equation*}
		\P{A_1\cap A_2\cap A_3}=\Pg{A_3}{A_1\cap A_2}\Pg{A_2}{A_1}\P{A_1}
		=\frac{4}{8}\cdot\frac{5}{9}\cdot\frac{6}{10}
	\end{equation*}
\end{exemple}

\subsection{Probabilités totales}
\begin{definition}
	Les évènements $B_1,B_2,\dots,B_n$ forment une \textit{partition} si les
	évènements $B_i\cap B_j=\emptyset$, $\forall i\neq j$, et
	$B_1\cup B_2\cup\cdots\cup B_n=S$.
\end{definition}

Avec une partition, on a la règle de probabilités totales, soit
\begin{equation*}
	\P{A}=\sum_{i=1}^n\P{A\cap B_i}=\sum_{i=1}^n\Pg{A}{B_i}\P{B_i}.
\end{equation*}

\begin{exemple}
	Soit la partition $B_1$, $B_2$ et $B_3$ représenté dans l'arbre suivant.
	\begin{figure}[H]
		\centering
		\input{figure/partition_arbre}
	\end{figure}

	À partir de l'arbre, il facile de déterminer les probabilités
	conditionnelles de $A$ et $A^c$. Par exemple,
	$\Pg{A}{B_1}=\nicefrac{3}{6}\cdot\nicefrac{1}{10}=\nicefrac{1}{30}$ et
	$\Pg{A^c}{B_2}=\nicefrac{2}{6}\cdot\nicefrac{9}{10}=\nicefrac{3}{10}$.
\end{exemple}

\begin{theoreme}[Règle d'inversion]
	$\Pg{B}{A}=\dfrac{\Pg{A}{B}\P{B}}{\P{A}}$
\end{theoreme}

\begin{theoreme}[Règles de Bayes]
	$\Pg{B_j}{A}=\dfrac{\Pg{A}{B_j}\P{B_j}}{\sum_{i=1}^n\Pg{A}{B_i}\P{B_i}}$, où $B_i$ et
	$B_j$ sont des partitions.
\end{theoreme}

\begin{exemple}
	On dépiste le cancer du poumon dans une clinique. On sait que 
	\begin{itemize}
		\item\SI{25}{\percent} des individus sont fumeurs
		\item\SI{75}{\percent} des individus sont non fumeurs
		\item\SI{10}{\percent} des fumeurs développent un cancer
		\item\SI{1 }{\percent} des non fumeurs développent un cancer
	\end{itemize}

	On détecte un cancer des poumons chez un individu sélectionné au hasard
	pour dépistage. Quelle est la probabilité que ça soit un fumeur?

	Soit $B$ les fumeurs, $B^c$ les non fumeurs, $A$ la présence de cancer du
	poumon et $A^c$ son absence. Par conséquent, on cherche $\Pg{B}{A}$. Avec les
	données, on sait que $\Pg{A}{B}=\SI{0.10}{}$, $\Pg{A}{B^c}=\SI{0.01}{}$,
	$\P{B}=\SI{0.25}{}$ et $\P{B^c}=\SI{0.75}{}$, de sorte que le théorème de
	Bayes nous donne
	\begin{equation*}
		\Pg{B}{A}
		=\frac{\Pg{A}{B}\P{B}}{\Pg{A}{B}\P{B}+\Pg{A}{B^c}\P{B^c}}
		\approx\SI{0.769}{}.
	\end{equation*}
\end{exemple}

\subsection{Notion d'indépendance}
\begin{definition}
	On dit que les évènements $A$ et $B$ sont \textit{indépendants} si la
	réalisation d'une n'affecte pas l'autrea.
\end{definition}

Mathématiquement parlant, des évènements indépendants $A$ et $B$ sont tels que
$\Pg{A}{B}=\P{A}$ et $\Pg{B}{A}=\P{B}$.

\begin{theoreme}
	$\P{A\cap B}=\P{A}\P{B}\Leftrightarrow\Pg{A}{B}=\P{A}\wedge\Pg{B}{A}=\P{B}$
\end{theoreme}

\begin{theoreme}
	$\P{A\cap B}=\P{A}\P{B}\Leftrightarrow\P{A\cap B^c}=\P{A}\P{B^c}$.
\end{theoreme}

\begin{exemple}
	Soit un système en série ayant $n$ composantes comme à la figure suivante.
	\begin{figure}[H]
		\centering
		\input{figure/series}
	\end{figure}
	Un système en série fonction si et seulement si tous ses composantes
	fonctionnent. Quelle est la probabilité que le système fonctionne si chaque
	composante a une probabilité de \SI{75}{\percent} de fonctionner?

	Soit l'ensemble échantillion $S=\left\{F,D\right\}$ avec $F$ une composante
	qui fonctionne et $D$ une composante défectueuse. De plus, on définit
	$A_i=\left\{F\right\}\in S$ comme étant la composante $i$ qui fonctionne et
	$A_i^c=\left\{D\right\}\in S$ comme étant la composante $i$ qui est
	défecteuse.

	On suppose que les composantes fonctionnent et tombent en panne
	indépendamment les uns des autres. Le système fonctionne si
	\begin{equation*}
		A_S=\bigcap_{i=1}^nA_i,
	\end{equation*}
	de sorte que
	\begin{equation*}
		\P{A_S}
		=\P{\bigcap_{i=1}^nA_i}
		=\prod_{i=1}^n\P{A_i}
		=\SI{0.75}{}^n.
	\end{equation*}
\end{exemple}

\begin{exemple}
	On génère un point au hasard dans le carré $S=[0,10]\times[0,10]$. On
	définit $A$ comme ayant l'abscisse du point entre 2 et 4, et $B$ comme
	ayant l'ordonnée du point entre 3 et 6. Est-ce que $A$ et $B$ sont
	indépendants?
	\begin{figure}[H]
		\centering
		\input{figure/square}
	\end{figure}
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Puisqu'il y a équiprobabilité continue, alors les probabilités sont données
	par le rapport entre l'aire de $A$ ou $B$ sur $S$, alors
	$\P{A}=\nicefrac{2}{10}$ et $\P{B}=\nicefrac{3}{10}$. De plus,
	$\P{A\cap B}=\nicefrac{6}{10}$. Puisque $\P{A\cap B}=\P{A}\P{B}$, alors les
	évènements $A$ et $B$ sont indépendants.
\end{exemple}

\section{Variables Aléatoires}
\begin{definition}
	Une \textit{variable aléatoire} correspond à une expérience aléatoire dont
	les résultats sont quantitatifs.
\end{definition}

On peut décrire une variable aléatoire par sa
\begin{itemize}
	\item fonction de répartition
	\item fonction de masse (variable aléatoire discrète)
	\item fonction de densité (variable aléatoire continue)
\end{itemize}

\begin{exemple}
	Soit $X$ le nombre de défaults de soudure d'un transistor à 3 pattes. Quel
	est l'espace échantillion de $X$?

	L'espace échantillion est $S_X=\left\{0,1,2,3\right\}$, alors $X$ est une
	variable aléatoire discrète.
\end{exemple}

\begin{exemple}
	Soit $X$ un nombre réel choisie au hasard dans l'intervalle $[0,2]$. Quel
	est l'espace échantillion de $X$?

	L'espace échantillion est $S_X=[0,2]$, alors $X$ est une variable aléatoire
	continue.
\end{exemple}

\begin{exemple}
	Soit $X$ le temps d'attente d'un client au guichet automatique. Quel est
	l'espace d'échantillion de $X$?

	L'espace échantillion est $S_X={0}\cup [0,\infty[$. Il y a une partie
	discrète et continue, alors $X$ est une variable aléatoire mixte.
\end{exemple}

\subsection{Fonction de répartition}
\begin{definition}
	Un \textit{fonction de répartition} $F_X(x)$ est égal à la probabilité
	qu'une variable aléatoire $X$ soit plus petite qu'une valeur $x$,
	c'est-à-dire $F_X(x)=\P{X\leq x}$, $\forall x\in\mathbb{R}$.
\end{definition}

\begin{theoreme}
	$0\leq F_X(x)\leq 1$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\lim_{X\rightarrow-\infty}F_X(x)=0$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\lim_{X\rightarrow \infty}F_X(x)=1$.
\end{theoreme}

\begin{theoreme}[non décroissance]
	$x_0<x_1\Leftrightarrow F_X(x_0)<F_X(x_1)$.
\end{theoreme}

\begin{theoreme}[continuité à droite]
	$F_X(x^+)=F_X(x)$.
\end{theoreme}

Il en résulte que toutes fonctions de répartition $F_X(x)$ sont croissantes,
mais peuvent contenir des discontinuités. Elles peuvent représenté des
variables aléatoires discrètes, continues ou mixtes. La figure
\ref{fig:maxwell} est une exemple de la fonction de répartition d'une
distribution de Maxwell-Boltzmann pour certains paramètres différents.

\begin{figure}[H]
	\centering
	\caption{$F_X(x)$ de certaines distributions de Maxwell-Boltzmann}
	\input{figure/maxwell_boltzmann}
	\label{fig:maxwell}
\end{figure}

\subsection{Fonction de masse}
\begin{definition}
	Une \textit{fonction de masse} $P_X(x_k)$ est égal à la probabilité qu'une
	variable aléatoire $X$ soit égal à une valeur discrète $x_k\in S_X$,
	c'est-à-dire $P_X(x_k)=\P{X=x_k}$ avec
	$S_X=\left\{x_1,x_2,\dots,x_k|x_1<x_2<\cdots<x_k\right\}$
\end{definition}

\begin{theoreme}
	$P_X(x_k)\geq 0$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\sum_{a< x_k\leq b}P_X(x_k)=\P{a<X\leq b}$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\sum_{k=1}^\infty P_X(x_k)=1$
\end{theoreme}

Une fonction de masse est nulle en tout point sauf aux valeurs discrètes
possibles. De plus, la somme de toutes les valeurs discrètes donne 1. La figure
\ref{fig:fonction_masse} montre un exemple d'une fonction de masse.

\begin{figure}[H]
	\centering
	\caption{exemple d'une fonction de masse}
	\input{figure/fonction_masse}
	\label{fig:fonction_masse}
\end{figure}

\subsection{Fonction de densité}
\begin{definition}
	Une \textit{fonction de densité} $f_X(x)$ est égal à la probabilité de
	qu'une variable aléatoire $X$ soit autour d'une valeur $x$,
	c'est-à-dire
	\begin{equation*}
		f_X(x)=\lim_{\epsilon\rightarrow 0}\frac{1}{\epsilon}
		\P{x-\frac{\epsilon}{2}\leq X\leq x+\frac{\epsilon}{2}},
	\end{equation*}
	où $\epsilon$ est positif.
\end{definition}

\begin{theoreme}
	$f_X(x)\geq 0$
\end{theoreme}

\begin{theoreme}
	$\displaystyle\int_{-\infty}^\infty f_X(x)\d{x}=1$
\end{theoreme}

\subsection{Règles de calcul fondamentale}
\begin{theoreme}\label{th:calc_fond}
	$\P{a<X\leq b}=F_X(b)-F_X(a)$
\end{theoreme}

\begin{proof}
	Soit les ensembles $A=\left\{X\leq a\right\}$, $B=\left\{X\leq b\right\}$
	et $C=\left\{a< X\leq b\right\}$, avec $a<b$, comme à la figure
	\ref{fig:droite_num}.
	\begin{figure}[H]
		\centering
		\input{figure/droite_numerique}
		\caption{représentation sur une droite numérique}
		\label{fig:droite_num}
	\end{figure}
	
	On sait que $A\cap C=\emptyset$ et $A\cup C=B$. Par conséquent, 
	$\P{B}=\P{A}+\P{C}\Leftrightarrow\P{C}=\P{B}-\P{A}=F_X(b)-F_X(a)$.
\end{proof}

\begin{theoreme}
	$\P{X=x}=F_X(x)-F_X(x^-)$.
\end{theoreme}

\begin{proof}
	Soit $a=x-\epsilon$ et $b=x$, où $\epsilon$ est positif. Selon le théorème
	\ref{th:calc_fond}, on a
	\begin{equation*}
		\P{x-\epsilon<X\leq x}=F_X(x)-F_X(x-\epsilon).
	\end{equation*}
	En prenant la limite lorsque $\epsilon\rightarrow 0$, on obtient
	\begin{equation*}
		\lim_{\epsilon\rightarrow 0}\P{x-\epsilon<X\leq x}
		=F_X(x)-\lim_{\epsilon\rightarrow 0}F_X(x-\epsilon).
	\end{equation*}
	Avec $F_X(x^-)=\lim_{\epsilon\rightarrow 0}F_X(x-\epsilon)$ et
	$(x-\epsilon<X\leq x)\equiv (X=x)$ lorsque $\epsilon\rightarrow 0$, on
	obtient
	\begin{equation*}
		\P{X=x}=F_X(x)-F_X(x^-).\qedhere
	\end{equation*}
\end{proof}

\subsection{Liens entre les différentes fonctions}
Toutes variables aléatoires peuvent être décrites par une fonction de
répartition. Lorsque la variable aléatoire est continue, alors elle peut aussi
être décrite pas une fonction de densité de probabilité. Si la variable
aléatoire est discrète, alors elle peut être décrite pas une fonction de masse.

\begin{theoreme}
	$f_X(x)=\dfrac{\d{}}{\d{x}}F_X(x)$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\displaystyle\int_{-\infty}^xf_X(t)\d{t}$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}\label{th:masse_repartition}
	$F_X(x)=\displaystyle\sum_{x_k\leq x}P_X(x_k)$ si $X$ est discrète.
\end{theoreme}

\begin{theoreme}
	$P_X(x_k)=\left\{
		\begin{matrix}
			F_X(x_1), & k=1\\
			F_X(x_k)-F_X(x_{k-1}), & k\neq 1\\
		\end{matrix}
	\right.$.
\end{theoreme}

\begin{exemple}
	Soit $X$ le temps d'attente d'un client au guichet automatique. On suppose
	que \SI{10}{\percent} des visites sont sans attente. Quel est la fonction
	de répartition?
	
	La variable aléatoire est mixte. On sait que $F_X(0)=\nicefrac{1}{10}$ et
	que $F_X(x)=0$ si $x<0$. La fonction de répartition est
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				0, & x < 0\\
				\dfrac{x+1}{x+10}, & x \geq 0\\
			\end{matrix}
		\right.
	\end{equation*}
	Le graphique suivant montre la fonction de répartition $F_X(x)$.
	\begin{figure}[H]
		\centering
		\input{figure/fonction_repartition}
	\end{figure}
\end{exemple}

\pagebreak
\begin{exemple}
	Soit $X$ le nombre de défaults de soudure d'un transistor à 3 pattes. La
	fonction de masse de $X$ est donnée par
	\begin{equation*}
		P_X(k)=\left\{
			\begin{matrix}
				\SI{0.7}{}, & k=0\\
				\SI{0.1}{}, & k=1\\
				\SI{0.1}{}, & k=2\\
				\SI{0.1}{}, & k=3\\
			\end{matrix}
		\right.
	\end{equation*}
	Quelle est la fonction de répartition de $P_X(k)$?

	On applique le théorème \ref{th:masse_repartition} pour obtenir la
	fonction de répartition. Lorsque $x<0$, alors $F_X(x)=0$. Lorsque
	$0\leq x<1$, alors $F_X(x)=\SI{0.7}{}$. Lorsque $1\leq x<2$, alors
	$F_X(x)=\SI{0.8}{}$. En continuant, on obtient
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				\SI{0.0}{}, & \phantom{0\;\leq}k<0\\
				\SI{0.7}{}, & 0\leq k<1\\
				\SI{0.8}{}, & 1\leq k<2\\
				\SI{0.9}{}, & 2\leq k<3\\
				\SI{1.0}{}, & 3\leq k\phantom{<0\;}\\
			\end{matrix}
		\right.
	\end{equation*}

	Les fonctions de masse et de répartition sont montrées dans les figures
	suivantes.

	\noindent
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/transistor_PX}
		\end{figure}
	\end{minipage}%
	\begin{minipage}{0.5\textwidth}
		\begin{figure}[H]
			\centering
			\input{figure/transistor_FX}
		\end{figure}
	\end{minipage}
\end{exemple}

\begin{exemple}
	Soit $X$ un nombrbe réel choisie au hasard dans l'intervalle $[0,2]$.
	Quelle est la fonction de répartition de $X$?

	On sait que $F_X(x)=\P{X\leq x}=\nicefrac{x}{2}$ lorsque $0\leq x\leq 2$,
	$F_X(x)=0$ si $x<0$ et $F_X(x)=1$ si $x>2$. On obtient donc la fonction de
	répartition
	\begin{equation*}
		F_X(k)=\left\{
			\begin{matrix}
				0,            & \phantom{0\;\leq}x<0\\
				\dfrac{x}{2}, & 0\leq x\leq 2\\
				1,            & 2\leq x\phantom{\leq 0\;}\\
			\end{matrix}
		\right.
	\end{equation*}
\end{exemple}

\subsection{Fonction conditionnelle}
\begin{definition}
	Une \textit{fonction conditionnelle} est la probabilité qu'une variable
	aléatoire $X$ prenne une valeur plus petit ou égal à  $x$ sachant un
	évènement $A$.
\end{definition}

\begin{theoreme}
	$F_X(x|A)=\dfrac{\P{\left\{X\leq x\right\}\cap A}}{\P{A}}$.
\end{theoreme}

\begin{theoreme}
	$f_X(x|A)=\dfrac{\d{}}{\d{x}}F_X(x|A)$ si $X$ est continue.
\end{theoreme}

\begin{theoreme}
	$P_X(x_k|A)=
	\left\{
		\begin{matrix}
			\dfrac{P_X(x_k)}{\P{A}}, & x_k\in A\\
			0, & x_k\notin A\\
		\end{matrix}
	\right.$
	si $X$ est discrète.
\end{theoreme}

\subsection{Médiane et quantile}
\begin{definition}
	La médiane d'une variable aléatoire $X$ continue est le nombre réel
	$x_{1/2}$ tel que $F_X(x_{1/2})=\nicefrac{1}{2}$.
\end{definition}

\begin{definition}
	Le \textit{quantile} d'ordre $p$ d'une variable aléatoire $X$ continue est
	le nombre réel $x_p$ tel que $F_X(x_p)=p$.
\end{definition}

\begin{exemple}
	Dans une certaine population, la taille $X$ d'un adulte choisi au hasard
	possède la fonction de répartition 
	\begin{equation*}
		F_X(x)=\left\{
			\begin{matrix}
				0,        & \phantom{1.2\leq\;}x<1.2\\
				1.5x-1.8, & 1.2\leq x<1.7\\
				0.5x-0.1, & 1.7\leq x<2.2\\
				1,        & 2.2\leq x\phantom{<2.2\;}\\
			\end{matrix}
		\right.
	\end{equation*}
	Calculer la médiane et le quantile d'ordre 95.

	Puisque $F_X(1.7)=0.75$, alors le quantile d'ordre 95 est dans la tranche
	$1.7\leq x<2.2$. Il suffit de résoudre $0.95=0.5x_{0.95}-0.1$ et on obtient
	que $x_{0.95}=2.1$.
\end{exemple}

\subsection{Lois de probabilités discrètes}
\subsubsection{Loi de Bernoulli}
Une expérience aléatoire où la variable aléatoire $X$ peut être un
\textit{succès} ou un \textit{échec} suit une loi de Bernoulli de paramètre
$p$ dénotée
\begin{equation*}
	X\sim\Bin{1}{p},
\end{equation*}
où $p$ est la probabilité de succès. On dénote un succès comme $X=1$ et un
échec comme $X=0$. Par conséquent, la probabilité d'un succès est $\P{X=1}=p$
tandis que celle d'un échec est $\P{X=0}=q$ où $q=1-p$.

\begin{theoreme}
	$P_X(k)=\left\{
		\begin{matrix}
			q &\text{si} & k=0\\
			p &\text{si} & k=1\\		
		\end{matrix}
	\right.$.
\end{theoreme}

\subsubsection{Loi binomiale}
Une expérience aléatoire où la variable aléatoire $X$ est le nombre de
\textit{succès} obtenu en $n$ essaie suit une loi binomiale dénotée
\begin{equation*}
	X\sim\Bin{n}{p},
\end{equation*}
où $p$ est la probabilité d'un succès individuel. L'espace échantillion est
$S_X=\left\{0,1,2,\dots,n\right\}$.

\begin{theoreme}
	$P_X(k)=\comb{k}{n}p^kq^{n-k}$ où $k=0,1,2,3,\dots,n$.
\end{theoreme}

\begin{proof}
	Soit $A_i$ le $i$-ème succès et $A_i^c$ le $i$-ème échec dans une
	expérience aléatoire à $n$ essai. La probabilité d'obtenir $k$ succès est
	donnée par
	\begin{equation*}
		\P{
			\underbrace{A_1\cap A_2\cap\cdots\cap A_k}_\text{$k$ fois}
			\cap
			\underbrace{A_{k+1}^c\cap A_{k+1}^c\cap\cdots\cap A_n^c}_
			\text{$n-k$ fois}
		},
	\end{equation*}
	où $k=0,1,2,3,\dots,n$.

	Hors, la probabilité d'un succès est $\P{A_i}=p$ et celle d'un échec est
	$\P{A_i^c}=q$, où $q=1-p$. Puisque chaque essai est indépendant des autres,
	la probabilité peut s'écrire
	\begin{equation*}
		\underbrace{
			\P{A_1}\P{A_2}\cdots P(A_k)
		}_\text{$k$ fois}
		\cdot
		\underbrace{
			\P{A_{k+1}^c}\P{A_{k+2}^c}\cdots P(A_n^c)
		}_\text{$n-k$ fois}
		=p^kq^{n-k}.
	\end{equation*}

	De plus, les succès peuvent être à n'importe quel essai. Par conséquent,
	il faut choisir $k$ essaies parmis les $n$ essaies de sorte que la
	probabilité d'obtenir $k$ succès en tout est
	$P_X(k)=\P{X=k}=\comb{k}{n}p^kq^{n-k}$.
\end{proof}

\begin{proposition}
	$\P{S_X}=1$.
\end{proposition}

\begin{proof}
	Par définition, on a
	\begin{equation*}
		\P{S_X}=\sum_{k=0}^n\comb{k}{n}p^kq^{n-k}
	\end{equation*}
	Selon le théorème binomial, soit
	\begin{equation*}
		(a+b)^n=\sum_{k=0}^n\comb{k}{n}p^kq^{n-k},
	\end{equation*}
	et que $q=1-p$, on peut simplifier de sorte à obtenir
	\begin{equation*}
		\P{S_X}=(p+q)^k=1^k=1.\qedhere
	\end{equation*}
\end{proof}

\vspace{-5mm}
\begin{exemple}
	Vous achetez un billet de 6/49 à chaque semaine depuis vos 18 ans. Quelle
	est la probabilité de gagner le gros lot au plus tard à votre $98^e$
	anniversaire?
	
	Ce problème se décrit à l'aide d'une loi binomiale, soit
	\begin{equation*}
		X\sim\Bin{n}{\frac{1}{\SI{13983816}{}}},
	\end{equation*}
	où $n$ est le nombre d'essaies et $X$ le nombre de gros lots gagnés.
	\vspace{-1mm}
\end{exemple}\pagebreak
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Puisqu'on joue chaque semaine de 18 ans à 98 ans, un total de
	$n=52\cdot(98-18)=4160$ billets sont achetés. La probabilité d'obtenir que
	des échecs est données par
	\begin{equation*}
		\left(1-\frac{1}{\SI{13983816}{}}\right)^{4160}=
		\left(\frac{\SI{13983815}{}}{\SI{13983816}{}}\right)^{4160},
	\end{equation*}
	de sorte que la probabilité d'obtenir au moins 1 billet gagnant est donnée
	par le complément, soit
	\begin{equation*}
		\P{X\geq 1}
		=1-\left(\frac{\SI{13983815}{}}{\SI{13983816}{}}\right)^{4160}
		\approx\SI{0.000297}{}.
	\end{equation*}
\end{exemple}

\subsubsection{Loi géométrique}
Une expérience aléatoire où la variable aléatoire $X$ est le nombre d'essais
requis pour obtenir un premier succès suit une loi géométrique dénotée
\begin{equation*}
	X\sim\Geo{p},
\end{equation*}
où $p$ est la probabilité d'avoir un succès à un essai. L'espace échantillion
est $S_X=\left\{1,2,\dots\right\}$.

\begin{theoreme}
	$p_X(k)=q^{k-1}p$.
\end{theoreme}

\begin{proof}
	Soit $A_i$ le $i$-ième succès et $A_i^c$ le $i$-ème échec dans un
	expérience aléatoire où il faut $k$ essai pour obtenir un succès. La
	probabilité d'obtenir le succès au $k$-ième essai est donnée par
	\begin{equation*}
		\P{
			\underbrace{A_1^c\cap A_2^c\cap\cdots\cap A_{k-1}^c}_
			\text{$k-1$ fois}\cap A_k
		},
	\end{equation*}
	où $k=1,2,\dots$

	Hors, la probabilité d'un succès est $\P{A_i}=p$ et celle d'un échec
	est $\P{A_i^c}=q$, où $q=1-p$. Puisque chaque essai est indépendant des
	autres, la probabilité peut s'écrire
	\begin{equation*}
		p_X(k)=
		\underbrace{\P{A_1^c}\P{A_2^c}\cdots\P{A_{k-1}}}_\text{$k-1$ fois}
		\cdot\P{A_k}=q^{k-1}p.\qedhere
	\end{equation*}
\end{proof}

\begin{theoreme}\label{th:repartition_geo}
	$F_X(n)=1-q^n$ où $n=1,2,\dots$
\end{theoreme}

\begin{proof}
	Selon une variante du théorème \ref{th:masse_repartition}, on a
	\begin{equation*}
		F_X(n)=\sum_{k=1}^np_X(k)=\sum_{k=1}^nq^{k-1}p=p\sum_{k=1}^nq^{k-1}.
	\end{equation*}
	On remarque que la somme est une série géométrique, alors
	\begin{equation*}
		\sum_{k=1}^nq^{k-1}=\frac{1-q^n}{1-q}.
	\end{equation*}
	Puisque $q=1-p\Leftrightarrow p=1-q$, on obtient
	\begin{equation*}
		F_X(n)=(1-q)\frac{1-q^n}{1-q}=1-q^n.\qedhere
	\end{equation*}
\end{proof}

\begin{theoreme}
	$\P{S_X}=1$.
\end{theoreme}

\begin{proof}
	En utilisant à nouveau la série géométrique, on a
	\begin{equation*}
		\P{S_X}
		=\sum_{k=1}^\infty p_X(k)
		=p\sum_{k=1}^\infty q^{k-1}
		=p\frac{1}{1-q}
		=(1-q)\frac{1}{1-q}
		=1,
	\end{equation*}
	puisque $q=1-p\Leftrightarrow p=1-q$.
\end{proof}

\begin{theoreme}[absence de mémoire]
	$\Pg{X>k+j}{X>j}=\P{X>k}$ où $k,j\in\mathbb{Z}$.
\end{theoreme}

\begin{proof}
	On sait que
	\begin{equation*}
		\Pg{X>k+j}{X>j}=\frac{\P{
			\left\{X>k+j\right\}\cap
	   		\left\{X>j\right\}}}{\P{X>j}}.
	\end{equation*}
	Puisque $k\in\mathbb{Z}$, alors $\left\{X>k+j\right\}\cap\left\{X>j\right\}
	=\left\{X>k+1\right\}$ de sorte que
	\begin{equation*}
		\Pg{X>k+j}{X>j}=\frac{\P{X>k+j}}{\P{X>j}}.
	\end{equation*}
	Avec le complément du théorème \ref{th:repartition_geo}, on a
	\begin{equation*}
		\Pg{X>k+j}{X>j}=\frac{q^{k+j}}{q^{j}}=q^{k}=\P{X>k}.\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Loi de Poisson}
Cette loi est un cas limite de la loi binomiale lorsque $n\rightarrow\infty$.
On pose $p=\alpha/n$ où $\alpha$ est un nombre positif, de sorte que la 
probabilité lorsque $n$ est grand devient nulle. On dénote une loi de Poisson
par
\begin{equation*}
	X\sim\Poi{\alpha}
	=\lim_{n\rightarrow\infty}\Bin{n}{\frac{\alpha}{n}}.
\end{equation*}

Par conséquent, la probabilité que $X$ prend une valeur proche de $\alpha$ est
élevée. Puisqu'elle est basée sur une loi binomiale, alors l'espace
échantillion est $S_X=\left\{0,1,2,\dots\right\}$.

\begin{theoreme}
	$p_X(k)=\dfrac{\alpha^k}{k!}\e^{-\alpha}$.
\end{theoreme}

\begin{theoreme}
	$\P{S_X}=1$.
\end{theoreme}

\begin{proof}
	En utilisant le développement en série de la fonction exponentielle, on a
	\begin{equation*}
		\P{S_X}
		=\sum_{k=0}^\infty p_X(k)
		=\e^{-\alpha}\sum_{k=0}^\infty\frac{\alpha^k}{k!}
		=\e^{-\alpha}\e^\alpha
		=1.\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Approximation par une loi de Poisson}
Soit une loi binomiale $X\sim\Bin{n}{p}$. Si $p$ est près de $0$, alors
$X\approx\Poi{np}$. Par conséquent,
\begin{equation*}
	\P{X=k}\approx\frac{(np)^k}{k!}\e^{-np}.
\end{equation*}

En général, l'approximation est bonne si $n\geq 30$ et $p\leq 0.05$. Si $p$
est proche de 1, alors on considère les échecs au lieu des succès. Dans ce cas,
\begin{equation*}
	\P{X=k}\approx\frac{(nq)^{n-k}}{(n-k)!}\e^{-nq}.
\end{equation*}

\subsection{Loi des probabilités continues}
\subsubsection{Loi uniforme (continue)}
Une expérience aléatoire où la variable aléatoire $X$ est le choix d'un nombre
réel dans un intervalle $[a,b]$ suit une loi uniforme dénotée
\begin{equation*}
	X\sim\Uni{a}{b},
\end{equation*}
où l'ensemble échantillion est $S_X=[a,b]$.

\begin{theoreme}
	$f_X(x)=\left\{
		\begin{matrix}
			\dfrac{1}{b-a} &\text{si } a\leq x\leq b\\
			0              &\text{sinon}
		\end{matrix}
	\right..
	$
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\left\{
		\begin{matrix}
			0                &\text{si}& \;\phantom{a\leq}x<a\\
			\dfrac{x-a}{b-a} &\text{si}& a\leq x\leq b\\
			1                &\text{si}& b<x\phantom{\leq b}\;\\
		\end{matrix}
	\right.$.
\end{theoreme}

\subsubsection{Loi exponentielle}
Une expérience aléatoire où la variable aléatoire $X$ suit la loi exponentielle
dénotée
\begin{equation*}
	X\sim\Exp{\lambda},
\end{equation*}
où $\lambda>0$, a l'espace échantillion $S_X=[0,\infty[$.

\begin{theoreme}
	$f_X(x)=\left\{
		\begin{matrix}
			\lambda\e^{-\lambda x} &\text{si}& x\geq 0\\
			0                      &\text{si}& x\leq 0\\
		\end{matrix}
	\right.$.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\left\{
		\begin{matrix}
			1         -\e^{-\lambda x}      &\text{si}& x\geq 0\\
			0\phantom{-\e^{-\lambda x}}\;\; &\text{si}& x\leq 0\\
		\end{matrix}
	\right.$.
\end{theoreme}

\begin{theoreme}[absence de mémoire]
	$\Pg{X>s+t}{X>t}=\P{X>t}$ où $s,t\in[0,\infty[$.
\end{theoreme}

\subsubsection{Loi gamma}
Une expérience aléatoire où la variable aléatoire $X$ suit la loi gamma dénotée
\begin{equation*}
	X\sim\Gam{\alpha}{\lambda},
\end{equation*}
où $\alpha>0$ et $\lambda>0$, a l'espace échantillion $S_X=[0,\infty[$.

\begin{theoreme}
	$f_X(x)
	=\dfrac{(\lambda x)^{\alpha-1}\lambda\e^{-\lambda x}}{\Gamma(\alpha)}$.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=1-\displaystyle\sum_{k=0}^{n-1}
		\dfrac{(\lambda x)^k\e^{-\lambda x}}{k!}$
	si $\alpha=n=1,2,3,\dots$
\end{theoreme}

\begin{theoreme}
	$\P{S_X}=1$.
\end{theoreme}

\begin{proof}
	Il suffit d'intégrer la fonction de densité de probabilité, soit
	\begin{equation*}
		\int_0^\infty
			\frac{(\lambda x)^{\alpha-1}\lambda\e^{-\lambda x}}{\Gamma(\alpha)}
		\d{x}=\frac{1}{\Gamma(\alpha)}\int_0^\infty
			(\lambda x)^{\alpha-1}\lambda\e^{-\lambda x}\d{x},
	\end{equation*}
	et en posant $y=\lambda x$, on obtient
	\begin{equation*}
		\frac{1}{\Gamma(\alpha)}\int_0^\infty y^{\alpha-1}\e^{-y}\d{y}
		=\frac{1}{\Gamma(\alpha)}\Gamma{(\alpha)}=1.\qedhere
	\end{equation*}
\end{proof}

\subsubsection{Loi normale}
On modèle souvent les erreurs d'observation et l'addition des résultats de
plusieurs expérience aléatoires d'une variable aléatoire $X$ par une loi
normale dénotée
\begin{equation*}
	X\sim\Norm{\mu}{\sigma^2},
\end{equation*}
où $\mu\in\mathbb{R}$ est la moyenne et $\sigma^2>0$ la variance. On appel
aussi $sigma$ comme l'écart-type. L'espace échantillion est $S_X=\mathbb{R}$.

\begin{theoreme}
	$f_X(x)=\dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left(
		{-\dfrac{1}{2\sigma^2}(x-\mu)^2}
	\right)$.
\end{theoreme}

\begin{theoreme}
	$F_X(x)=\displaystyle\int_{-\infty}^{\frac{x-\mu}{\sigma}}
		\dfrac{1}{\sqrt{2\pi}}\exp\left(-\dfrac{z^2}{2}\right)\d{z}$
	où $z=\dfrac{t-\mu}{\sigma}$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\int_{-\infty}^\infty
	\frac{1}{\sqrt{2\pi}}\e^{-x^2/2}\d{x}=1$
\end{theoreme}

\begin{proof}
	Soit
	\begin{equation*}
		I=\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}\e^{-x^2/2}\d{x}
	\end{equation*}
	et
	\begin{equation*}
		I^2=
			\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}\e^{-x^2/2}\d{x}
			\int_{-\infty}^\infty\frac{1}{\sqrt{2\pi}}\e^{-y^2/2}\d{y}.
	\end{equation*}

	Ensuite,
	\begin{equation*}
		I^2=\frac{1}{2\pi}\int_{-\infty}^\infty\int_{-\infty}^\infty
			\e^{-(x^2+y^2)/2}\d{x}\d{y}.
	\end{equation*}

	En utilisant les coordonnées polaires, on a
	\begin{equation*}
		I^2=\frac{1}{2\pi}\int_0^{2\pi}\int_0^\infty
			\e^{-r^2/2}r\d{r}\d{\theta},
	\end{equation*}
	de sorte qu'en posant posant $u=r^2$, on peut obtenir
	\begin{equation*}
		I^2=-\frac{1}{2\pi}\int_0^{2\pi}
			\e^{-r^2/2}\bigg\rvert_0^\infty\d{\theta}=
		-\frac{1}{2\pi}\int_0^{2\pi}\d{\theta}=1
	\end{equation*}
	de sorte que $I=1$.
\end{proof}

\subsection{Fonction d'une variable aléatoire}
\begin{exemple}
	On suppose que $X$ est la valeur d'ampliture d'un signal an temps $t$. Le
	signal numérisé peut s'écrire
	\begin{equation*}
		Y=\mathrm{signe}(X)\cdot\Delta\cdot\mathrm{part}\left(
			\frac{|X|}{\Delta}+\frac{1}{2}
		\right),
	\end{equation*}
	où $\Delta$ est le pas de quantification.
\end{exemple}

\subsubsection{$X$ et $Y$ sont des variables aléatoires discrètes}
%
% diagramme du mapping entre X -> Y selon Y(X)
%
\begin{exemple}
	Soit $X\sim\Bin{2}{\nicefrac{1}{4}}$ et $Y=(X-1)^2$. Quelle est la fonction
	de masse de $Y$?

	On sait que $\P{Y=0}=\P{X=1}$ et $\P{Y=1}=\P{X=0}+\P{X=2}$. On sait que
	$\P{X=0}=\nicefrac{9}{16}$, $\P{X=1}=\nicefrac{6}{16}$ et $\P{X=0}=
	\nicefrac{1}{16}$ de sorte que $\P{Y=0}=\nicefrac{6}{16}$ et
	$\P{Y=1}=\nicefrac{10}{16}$.
\end{exemple}

\subsubsection{$X$ et $Y$ sont des variables aléatoires discrète et continue}
\begin{exemple}
	Soit $X\sim\Norm{0}{1}$ et
	\begin{equation*}
		Y=\left\{
			\begin{matrix}
				-1&\text{si}&\phantom{-\nicefrac{1}{2}\leq}X<-\nicefrac{1}{2}\\
				 0&\text{si}&-\nicefrac{1}{2}\leq X<\nicefrac{1}{2}\\
				 1&\text{si}&\nicefrac{1}{2}\leq X\\
			\end{matrix}
		\right..
	\end{equation*}
	Quelle est la probabilité que $Y=1$?

	On cherche $\P{X\leq-\nicefrac{1}{2}}=\Phi(-\nicefrac{1}{2})=
	\Phi(\nicefrac{1}{2})\approx\SI{0.3085}{}$. De plus, $\P{-\nicefrac{1}{2}
	\leq X\leq\nicefrac{1}{2}}=\Phi(\nicefrac{1}{2})-\Phi(-\nicefrac{1}{2})
	\approx\SI{0.3830}{}$. Finalement, $\P{X<-\nicefrac{1}{2}}\approx
	\SI{0.3085}{}$.
\end{exemple}

\subsubsection{$X$ et $Y$ sont de svariables aléatoires continues}
% P(Y<=y)=P(X^2<=y)=P(-sqrt(y)<=X<=sqrt(y)=F_X(sqrt(Y)-F_X(-sqrt(Y))
\begin{exemple}
	Soit $X\sim\Uni{-1}{2}$ et $Y=X^2$. Quelle est la fonction de répartition de
	$Y$?
\end{exemple}

\subsection{Espérance mathématique}
\begin{definition}
	Une \textit{espérance} d'une variable aléatoire $X$, dénotée $\Esp{X}$, est
	la somme des valeur possibles de $X$ pondérées par leur probabilité.
\end{definition}

\begin{theoreme}
	$\Esp{X}=\displaystyle\sum_{k=1}^\infty x_kp_X(x_k)$ si $X$ est discrète.
\end{theoreme}

\begin{theoreme}
	$\Esp{X}=\displaystyle\int_{-\infty}^\infty xf_X(x)\d{x}$ si $X$ est
	continue.
\end{theoreme}

\begin{exemple}
	Quelle est l'espérance d'un lancer d'un dé?

	On calcule l'espérance d'une variable discrète, soit
	\begin{equation*}
		\Esp{X}=
			1\cdot\frac{1}{6}+
			2\cdot\frac{1}{6}+
			3\cdot\frac{1}{6}+
			4\cdot\frac{1}{6}+
			5\cdot\frac{1}{6}+
			6\cdot\frac{1}{6}
		=3.5.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit $X\sim\Poi{\alpha}$. Quelle est l'espérance de $X$?

	Par définition, on calcule l'espérance avec
	\begin{equation*}
		\Esp{X}
		=\sum_{k=0}^\infty k\cdot p_X(k)
		=\sum_{k=0}^\infty k\cdot\frac{\e^{-\alpha}\alpha^k}{k!}
		=\sum_{k=1}^\infty k\cdot\frac{\e^{-\alpha}\alpha^k}{k!},
	\end{equation*}
	car le premier terme à $k=0$ est nul. Par conséquent,
	\begin{equation*}
		\Esp{X}
		=\sum_{k=1}^\infty\frac{\e^{-\alpha}\alpha^k}{(k-1)!}
		=\e^{-\alpha}\sum_{k=1}^\infty\frac{\alpha^k}{(k-1)!}
		=\e^{-\alpha}\sum_{i=0}^\infty\frac{\alpha^{i+1}}{i!}
		=\e^{-\alpha}\alpha\sum_{i=0}^\infty\frac{\alpha^i}{i!}.
	\end{equation*}
	Hors, la somme est le développement en séries de la fonction exponentielle,
	alors
	\begin{equation*}
		\Esp{X}
		=\e^{-\alpha}\alpha\e^{\alpha}
		=\alpha.
	\end{equation*}
\end{exemple}

\pagebreak
\begin{exemple}
	Soit $X\sim\Exp{\lambda}$. Quelle est l'espérance de $X$?

	Par définition, on calcule l'espérance avec
	\begin{equation*}
		\Esp{X}
		=\int_{-\infty}^\infty xf_X(x)\d{x}
		=\int_0^\infty x\lambda\e^{-\lambda x}\d{x}.
	\end{equation*}
	En posant $u=x$ et $\d{v}=\lambda\e^{-\lambda x}\d{x}$, on obtient
	\begin{equation*}
		\Esp{X}
		=-x\e^{-\lambda x}\bigg\rvert_0^\infty
		+\int_0^\infty\e^{-\lambda x}\d{x}
		=\frac{1}{\lambda}\int_0^\infty\lambda\e^{-\lambda x}\d{x}
		=\frac{1}{\lambda},
	\end{equation*}
	car l'aire sous la fonction de densité de probabilités est égal à 1.
\end{exemple}

Soit $Y=g(X)$, où $X$ et $Y$ sont des variables aléatoires et $g$ une
tranformation. Si $X$ et $Y$ discrètes, alors
\begin{equation*}
	\Esp{Y}=\Esp{g(X)}=\sum_{k=1}^\infty g(x_k)p_X(x_k),
\end{equation*}
et si $X$ et $Y$ sont continues, alors
\begin{equation*}
	\Esp{Y}=\Esp{g(X)}=\int_{-\infty}^\infty g(x)f_X(x)\d{x}.
\end{equation*}

\begin{theoreme}
	$\Esp{c}=c$, où $c$ est une constante.
\end{theoreme}

\begin{theoreme}
	$\Esp{aX+b}=a\Esp{X}+b$, où $a$ et $b$ des constantes.
\end{theoreme}

\begin{theoreme}
	$\Espg{X}{A}=\displaystyle\sum_{k=1}^\infty x_kp_X(x_k|A)$, où $X$ discrète.
\end{theoreme}

\begin{theoreme}
	$\Espg{X}{A}=\displaystyle\int_{-\infty}^\infty xf_X(x|A)$, où $X$ continue.
\end{theoreme}

\begin{theoreme}
	$\Esp{X}=\displaystyle\sum_{i=1}^nE(X|B_i)\P{B_i}$, où $B_1,\dots,B_n$ des
	partitions de $S_X$.
\end{theoreme}

\begin{exemple}
	Soit $X$ une variable aléatoire mixte. Quelle est la forme de l'espérance
	de $X$?

	On définie $C$ comme l'événement où $X$ prend une valeur continue et $D$
	lorsque $X$ prend une valeur discrète. Par conséquent, on a
	\begin{equation*}
		\Esp{X}=\underbrace{\Espg{X}{C}}_{\int}\P{C}
		+\underbrace{\Espg{X}{D}}_{\sum}\P{D}.
	\end{equation*}
\end{exemple}

\subsection{Variance}
\begin{definition}
	La \textit{variance} d'une variable aléatoire $X$, dénotée $\Var{X}$, est
	définie comme
	\begin{equation*}
		\Var{X}=\Esp{\left(X-\Esp{X}\right)^2}.
	\end{equation*}
\end{definition}

En développant le carré de la variance, on obtient
\begin{equation*}
	\Var{X}
	=\Esp{X^2-2X\Esp{X}+\Esp{X}^2}
	=\Esp{X^2}-2\Esp{X\Esp{X}}+\Esp{\Esp{X}^2},
\end{equation*}
et en simplifiant les constantes, on a
\begin{equation*}
	\Var{X}
	=\Esp{X^2}-2\Esp{X}\Esp{X}+\Esp{X}^2
	=\Esp{X^2}-\Esp{X}^2
\end{equation*}

\begin{exemple}
	Soit $X\sim\Bin{1}{p}$. Quelle est la variance de $X$?

	On sait que l'espérance de $X$ est
	\begin{equation*}
		\Esp{X}=0\cdot(1-p)+1\cdot p=p,
	\end{equation*}
	et celle de $X^2$ est
	\begin{equation*}
		\Esp{X^2}=0^2\cdot(1-p)+1^2\cdot p=p.
	\end{equation*}
	Par conséquent, la variance est
	\begin{equation*}
		\Var{X}=p-p^2=p(1-p).
	\end{equation*}
\end{exemple}

\begin{theoreme}
	$\Var{c}=c$, où $c$ est une constante.
\end{theoreme}

\begin{theoreme}
	$\Var{aX+b}=a^2\Var{X}$, où $a$ et $b$ des constantes.
\end{theoreme}

\begin{theoreme}
	$\Std{X}=\sqrt{\Var{X}}$.
\end{theoreme}

\begin{theoreme}
	$\Varg{X}{A}=\Espg{X^2}{A}-\Espg{X}{A}^2$.
\end{theoreme}

\subsection{Inégalité de Markov}
Soit $X$ une variable aléatoire prenant des valeurs non négatives. On peut
montrer que
\begin{equation*}
	\P{X\geq a}\leq\frac{\Esp{X}}{a},\forall a>0.
\end{equation*}

\subsection{Inégalité de Bienaymé-Tchebychev}
Soit $X$ une variable aléatoire dont la moyenne $\Esp{X}=\mu_X$ et la variance
$\Var{X}=\sigma_X^2$ existent. On peut montrer que
\begin{equation*}
	\P{|X-\mu_X|\geq a}\leq\frac{\sigma_X^2}{a^2},\forall a>0.
\end{equation*}

\begin{exemple}
	Soit le lancer d'une pièce de monnaie avec $X\sim\Bin{n}{\nicefrac{1}{2}}$.
	On calcule la moyenne avec
	\begin{equation*}
		\Esp{\frac{X}{n}}
		=\frac{1}{n}\Esp{X}
		=\frac{1}{n}\cdot n\cdot\frac{1}{2}
		=\frac{1}{2}
	\end{equation*}
	et la variance avec
	\begin{equation*}
		\Var{\frac{X}{n}}
		=\frac{1}{n^2}\Var{X}
		=\frac{1}{n^2}\cdot n\cdot\frac{1}{2}\cdot\frac{1}{2}
		=\frac{1}{4n}.
	\end{equation*}

	Selon l'inégalité de Bienaymé-Tchebychev, on a
	\begin{equation*}
		\P{\left|\frac{X}{n}-\frac{1}{2}\right|\geq 0.01}
		\geq\frac{1/4n}{(0.01)^2}
		=\frac{10000}{4n}
		=\frac{2500}{n}.
	\end{equation*}
\end{exemple}

% nous ne faisons pas fonction moment/génératrice
\subsection{Fonction caractéristique}
\begin{definition}
	Une fonction \textit{caractéristique}, dénoté $\phi_X(\omega)$, est
	l'espérance d'une variable aléatoire $X$ tel que
	\begin{equation*}
		\phi_X(\omega)=\Esp{\e^{j\omega X}},
	\end{equation*}
	où $j$ est le nombre imaginaire tel que $j^2=-1$.
\end{definition}

\begin{theoreme}
	$\phi_X(\omega)=\displaystyle\sum_{k=1}^\infty\e^{j\omega x_k}\cdot
	p_X(x_k)$, si $X$ est discrète.
\end{theoreme}

\begin{theoreme}
	$\phi_X(\omega)=\displaystyle\int_{-\infty}^\infty\e^{j\omega X}\cdot
	f_X(x)\d{x}$, si $X$ est continue.
\end{theoreme}

\begin{theoreme}
	$\phi_X(0)=1$.
\end{theoreme}

\begin{theoreme}
	$\Esp{X^n}=(-j)^n\left[
	\dfrac{\d{}^n}{\d{\omega}^n}\phi_X(\omega)\right]_{\omega=0}$.
\end{theoreme}

\begin{exemple}
	Soit $X\sim\Bin{n}{p}$. Quelle est la fonction caractéristique de $X$?

	La fonction caractéristique de la loi binomiale est donnée par
	\begin{equation*}
		\phi_X(\omega)
		=\sum_{k=0}^n\e^{j\omega k}\cdot\comb{n}{k}p^kq^{n-k}
		=\sum_{k=0}^n\comb{n}{k}\left(p\e^{j\omega}\right)^kq^{n-k}
		=(p\e^{j\omega}+q)^n,
	\end{equation*}
	selon le binôme de Newton.
\end{exemple}

\pagebreak
\begin{exemple}
	Soit $X\sim\Norm{\mu}{\sigma^2}$. On peut montrer que
	\begin{equation*}
		\phi_X(\omega)=\exp\left(j\omega\mu-\frac{1}{2}\omega^2\sigma^2\right).
	\end{equation*}
	Soit $Y=aX+b$. Quelle est la fonction caractéristique de $Y$?
	Par définition, on a
	\begin{equation*}
		\phi_Y(\omega)
		=\Esp{\e^{j\omega Y}}
		=\Esp{\e^{j\omega(aX+b)}}
		=\e^{j\omega b}\Esp{\e^{j\omega aX}}
		=\e^{j\omega b}\cdot\exp\left(j\omega a\mu+\frac{1}{2}
			\omega^2a^2\sigma^2
		\right)
	\end{equation*}
	de sorte à obtenir
	\begin{equation*}
		\phi_Y(\omega)=
		\exp\bigg[
			j\omega\underbrace{(a\mu+b)}_{\mu_Y}-
			\frac{1}{2}\omega^2\underbrace{(a^2\sigma)^2}_{\sigma_Y^2}
		\bigg].
	\end{equation*}
	La fonction $\phi_Y(\omega)$ est de la même forme que $\phi_X(\omega)$,
	alors on peut en déduire que $Y$ suit aussi une loi normale.
\end{exemple}

\subsection{Fiabilité}
On s'intéresse à la durée de vie des systèmes et de leurs composantes. Soit $T$
une variable aléatoire, continue et non-négative, représentant une durée de
vie. En général, l'espace d'échantillion est $S_T=[0,\infty[$.

On définie la fonction de fiabilité $R(T)$ tel que
\begin{equation*}
	R(t)=\P{T>t}=1-F_T(t)
\end{equation*}
et le taux de défaillance $r(t)$ comme
\begin{equation*}
	r(t)
	=\lim_{s\rightarrow t}f_T(s|T>t)
	=\frac{f_T(t)}{1-F_T(t)}
	=-\frac{R\prime(t)}{R_(t)}
\end{equation*}

Sachant $r(t)$, il est possible de déterminer la fonction de fiabilité en
trouvant la solution de l'équation différentielle suivante, soit
\begin{equation*}
	r=-\frac{R\prime}{R},
\end{equation*}
avec $R(0)=1$, de sorte à obtenir
\begin{equation*}
	R(t)=\exp\left(-\int_0^tr(s)\d{s}\right).
\end{equation*}

\pagebreak
\begin{exemple}
	Soit $T\sim\Exp{\lambda}$. Quel est le taux de défaillance?

	On calcule la fonction de fiabilité avec
	\begin{equation*}
		R(t)
		=1-F_T(t)
		=1-(1-\e^{-\lambda t}
		=\e^{-\lambda t}
	\end{equation*}
	de sorte à obtenir le taux de défaillance
	\begin{equation*}
		r(t)
		=\frac{\lambda\e^{-\lambda t}}{\e^{-\lambda t}}
		=\lambda.
	\end{equation*}
	On remarque que si $T\sim\Exp{\lambda}$, alors $T$ a la propriété de
	non-vieillissement.
\end{exemple}

\subsubsection{Durée de vie moyenne}
La \textit{durée de vie moyenne} est l'espérance de $T$ et peut se calculer
selon
\begin{equation*}
	\Esp{T}=\int_0^\infty R(t)\d{t}.
\end{equation*}

\begin{proof}
	Par définition, on a
	\begin{equation*}
		\Esp{T}=\int_0^\infty t\cdot f_T(t)\d{t}.
	\end{equation*}
	Hors, on peut exprimé $t$ comme
	\begin{equation*}
		t=\int_0^t\d{s}
	\end{equation*}
	de sorte à obtenir
	\begin{equation*}
		\Esp{T}
		=\int_0^\infty\int_0^t f_T(t)\d{s}d{t}
		=\int_0^\infty\underbrace{\int_s^\infty f_T(t)\d{t}}_{\P{T>s}}\d{s}
		=\int_0^\infty R(t)\d{t},
	\end{equation*}
	en inversant l'ordre d'intégration.
\end{proof}

\begin{exemple}
	Soit un système en série à $n$ composantes. On suppose que les composantes
	fonctionnent indépendaments les un des autres. Quelle est la fonction de
	fiabilité du système?

	La probabilité que le système fonctionne après un temps $t$ est équivalent
	à la probabilité que tous les composantes fonctionnents après un temps $t$,
	soit
	\begin{equation*}
		\P{T>t}
		=\P{\left\{T_1>t\right\}\cap\cdots\cap\left\{T_N>t\right\}}
		=\P{T_1>t}\cdots\P{T_n>t},
	\end{equation*}
	car les événements sont indépendants. 
\end{exemple}
\addtocounter{exemple}{-1}
\begin{exemple}[suite]
	Par conséquent, on obtient
	\begin{equation*}
		R(t)=\prod_{k=1}^n R_k(t).
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit un système en parrallèle à $n$ composantes. On suppose que les
	composantes fonctionnents indépendaments les un de sautres. Quelle est la
	fonction de fiablité dus système?

	La probabilité que le système soit en panne est équivalent à la probabilité
	que tous les composantes soit en panne après un temps $t$, soit
	\begin{equation*}
		\P{T\leq t}
		=\P{\left\{T_1\leq t\right\}\cap\cdots\cap\left\{T_n\leq t\right\}}
		=\P{T_1\leq t}\cdots\P{T_n\leq t},
	\end{equation*}
	car les événements sont indépendants. Par conséquent, on obtient
	\begin{equation*}
		R(t)=1-\prod_{k=1}^n\bigg[1-R_k(t)\bigg].
	\end{equation*}
\end{exemple}

\section{Vecteurs aléatoires}
\begin{definition}
	Un \textit{vecteur aléatoire} est formé de plusieurs variables aléatoires
	observées lors d'une même expérience.
\end{definition}

\begin{exemple}
	Soit le lancer de 2 dés.

	On pose $X$ comme étant le résultat du premier dé et $Y$ comme étant le 
	résultat du deuxième dé. On défini le vecteur aléatoire $(X,Y)$. L'espace
	échantillion est $S_{X,Y}=\left\{(1,1),(1,2),\dots,(6,5),(6,6)\right\}$.

	La probabilité d'avoir un évènement est
	\begin{equation*}
		\P{\left\{X=j\right\}]\cap\left\{Y=i\right\}}=\frac{1}{36},
	\end{equation*}
	pour $i=1,\dots,6$ et $j=1,\dots,6$.
\end{exemple}

\pagebreak
\begin{exemple}
	On génère un point au hasard dans le triangle $T$ de coordonnées $(0,0)$,
	$(1,1)$ et $(0,1)$.

	Chaque point dans le triangle peut être considéré comme une vecteur
	aléatoire $(X,Y)$ où $X$ est l'abscisse et $Y$ l'ordonnée. Soit
	$f_{X,Y}(x,y)$ la fonction de masse du vecteur tel que
	\begin{equation*}
		f_{X,Y}(x,y)=\left\{
			\begin{array}{rl}
				c&\text{si $(x,y)$ est dans le triangle}\\
				0&\text{sinon}\\
			\end{array}
		\right.,
	\end{equation*}
	où la constante $c$ est donnée par
	\begin{equation*}
		\iint_Tf_{X,Y}(x,y)\d{A}=1\Leftrightarrow c=2.
	\end{equation*}
\end{exemple}

\subsection{Vecteur aléatoire discret}
\subsubsection{Fonction de masse conjointe}
\begin{theoreme}
	$p_{X,Y}(x_j,y_k)=\P{\left\{X=x_j\right\}\cap\left\{Y=y_k\right\}}$.
\end{theoreme}

\begin{theoreme}
	$p_{X,Y}(x_j,y_k)\geq 0$.
\end{theoreme}

\begin{theoreme}
	$\displaystyle\sum_{j=1}^\infty\displaystyle\sum_{k=1}^\infty
	p_{X,Y}(x_j,y_k)=1$.
\end{theoreme}

\subsubsection{Fonction de masse marginale}
\begin{theoreme}
	$p_X(x_j)=\displaystyle\sum_{k=0}^\infty p_{X,Y}(x_j,y_k)$.
\end{theoreme}

\begin{theoreme}
	$p_Y(y_k)=\displaystyle\sum_{j=0}^\infty p_{X,Y}(x_j,y_k)$.
\end{theoreme}

\subsubsection{Fonction de masse conditionnelle}
\begin{theoreme}
	$p_{Y|X}(y_k|x_j)=\dfrac{p_{X,Y}(x_j,y_k)}{p_X(x_j)}$.
\end{theoreme}

\begin{theoreme}
	$p_{X|Y}(x_j|y_k)=\dfrac{p_{X,Y}(x_j,y_k)}{p_Y(y_k)}$.
\end{theoreme}

\begin{theoreme}
	$\P{A}=\displaystyle\sum\displaystyle\sum_Ap_{X,Y}(x_j,y_k)$.
\end{theoreme}

\subsection{Vecteur aléatoire continu}
\subsubsection{Fonction de densité conjointe}
\begin{theoreme}
	$f_{X,Y}(x,y)=\dfrac{1}{\delta\epsilon}\P{
		\left\{
			x-\dfrac{\delta}{2}\leq X\leq x+\dfrac{\delta}{2}
		\right\}\cap
		\left\{
			y-\dfrac{\epsilon}{2}\leq Y\leq y+\dfrac{\epsilon}{2}
		\right\}
	}$
\end{theoreme}

\begin{theoreme}
	$\displaystyle\int_{-\infty}^\infty\displaystyle\int_{-\infty}^\infty
		f_{X,Y}(x,y)\d{x}\d{y}=1$.
\end{theoreme}

\subsubsection{Fonction de densité marginale}
\begin{theoreme}
	$f_X(x)=\displaystyle\int_{-\infty}^\infty f_{X,Y}(x,y)\d{y}$.
\end{theoreme}

\begin{theoreme}
	$f_Y(y)=\displaystyle\int_{-\infty}^\infty f_{X,Y}(x,y)\d{x}$.
\end{theoreme}

\subsubsection{Fonction de densité conditionnelle}

\begin{theoreme}
	$f_{Y|X}(y|x)=\dfrac{f_{X,Y}(x,y)}{f_X(x)}$.
\end{theoreme}

\begin{theoreme}
	$f_{X|Y}(x|y)=\dfrac{f_{X,Y}(x,y)}{f_Y(y)}$.
\end{theoreme}

\begin{theoreme}
	$\P{A}=\displaystyle\iint_Af_{X,Y}(x,y)\d{A}$.
\end{theoreme}

\subsection{Fonction de répartition conjointe}
\begin{theoreme}
	$F_{X,Y}(x,y)=\P{\left\{X\leq x\right\}\cap\left\{Y\leq y\right\}}$.
\end{theoreme}

\begin{theoreme}
	$F_{X,Y}(x,y)=\displaystyle\sum_{x_j\leq x}
	\displaystyle\sum_{y_k\leq y}p_{X,Y}(x_j,y_k)$ dans le cas discret.
\end{theoreme}

\begin{theoreme}
	$F_{X,Y}(x,y)=\displaystyle\int_{-\infty}^x
	\displaystyle\int_{-\infty}^yf_{X,Y}(s,t)\d{t}\d{s}$ dans le cas continue.
\end{theoreme}

\begin{theoreme}
	$f_{X,Y}(x,y)=\dfrac{\p{}^2}{\p{x}\p{y}}F_{X,Y}(x,y)$.
\end{theoreme}

\subsection{Fonction de répartition marginale}
\begin{theoreme}
	$F_X(x)=\displaystyle\lim_{y\rightarrow\infty}F_{X,Y}(x,y)$.
\end{theoreme}

\begin{theoreme}
	$F_Y(y)=\displaystyle\lim_{x\rightarrow\infty}F_{X,Y}(x,y)$.
\end{theoreme}

\begin{exemple}
	Quelle est la probabilité que $(X,Y)$ soit dans un rectangle $R$? On
	suppose qu'on connait $F_{X,Y}(x,y)=\P{\left\{X\leq x\right\}\cap
	\left\{Y\leq y\right\}}$.

	\begin{equation*}
		\P{R}
		=F_{X,Y}(b,d)-F_{X,Y}(b,c)-F_{X,Y}(a,d)+2F_{X,Y}(a,c).
	\end{equation*}
\end{exemple}

\subsection{Indépendance dans un vecteur}
\begin{definition}
	Si $X$ et $Y$ sont des variables aléatoires \textit{indépendantes}, il faut
	que
	\begin{enumerate}
		\item $S_{X,Y}=S_X\times S_Y$
		\item $F_{X,Y}(x,y)=F_X(x)F_Y(y)$
		\begin{itemize}
			\item[$\Rightarrow$] $p_{X,Y}(x_j,y_k)=p_X(x_j)p_Y(y_k)$
			\item[$\Rightarrow$] $f_{X,Y}(x,y)=f_X(x)f_Y(y)$
		\end{itemize}
	\end{enumerate}
\end{definition}

Si on veut déterminer s'il y a indépendance ou non entre deux variables
aléatoires $X$ et $Y$, il faut vérifier les deux conditions. Si elles sont
respectées, alors $X$ et $Y$ sont des variables aléatoires indépendantes.

\begin{exemple}
	La veille d'un examen, un professeur estime qu'il recevra $X$ questions par
	courriel, où $X\sim\Poi{\alpha}$. Le professeur répond à chaque question
	avec une probabilité $p$, et ce indépendamment d'une question à l'autre.
	Soit $Y$ le nombre de réponses du professeur. Déterminer $p_Y(y)$.

	L'espace échantillion du vecteur aléatoire discrète $(X,Y)$ est
	\begin{equation*}
		S_{X,Y}=\left\{(0,0),(1,0),(1,1),\dots,(j,0),\dots,(j,j)\right\},
	\end{equation*}
	soit un triangle, où $j$ est le nombre de questions reçus.
	\begin{equation*}
		p_{X|Y}(j|k)
		=\underbrace{p_{Y|X}(k,j)}_{\Bin{j}{p}}
		 \underbrace{p_X(j)}_{\Poi{\alpha}}
		=\comb{k}{j}p^kq^{j-k}\frac{\e^{-\alpha}}{j!}\alpha^j
	\end{equation*}

	\begin{equation*}
		p_Y(k)
		=\sum_{j=k}^\infty p_{X,Y}(j,k)
		=\frac{\e^{-\alpha}p^k}{k!}
		 \sum_{k=0}^\infty\frac{\alpha^jq^{j-k}}{(j-k)!}
		=\frac{\e^{-\alpha}(\alpha p)^k\e^{\alpha q}}{k!}
		=\frac{\e^{-\alpha p}(\alpha p)^k}{k!}
	\end{equation*}
\end{exemple}

\subsection{Espérance conditionelle}
\begin{theoreme}
	$\Espg{Y}{X=x_j}=\displaystyle\sum_{k=1}^\infty y_kp_{Y|X}(y_k|x_j)$
	dans un cas discret.
\end{theoreme}

\begin{theoreme}
	$\Espg{Y}{X=x_j}=\displaystyle\int_{-\infty}^\infty yf_{Y|X}(y|x)\d{x}$
	dans un cas continu.
\end{theoreme}

En général, $\Espg{Y}{X=x}=g_1(x)$ et $\Espg{X}{Y=y}=g_2(y)$. On a donc,
$\Espg{Y}{X}=g_1(X)$ et $\Espg{X}{Y}=g_2(Y)$, c'est-à-dire que l'espérance
conditionelle est une variable aléatoire.

\begin{theoreme}
	$\Esp{Y}=\Esp{\Espg{Y}{X}}$.
\end{theoreme}

\begin{proof}
	On sait que
	\begin{equation*}
		\Esp{\Espg{Y}{X})}
		=\int_{-\infty}^\infty\Espg{Y}{X=x}f_X(x)\d{x}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty
         yf_{Y|X}(y|x)\d{y}\cdot f_X(x)\d{x}
	\end{equation*}
	\begin{equation*}
		\Esp{\Espg{Y}{X}}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty
		 y\cdot\underbrace{f_{Y|X}(y|x)f_X(x)}_{f_{X,Y}(x,y)}\d{y}\d{x}
		=\int_{-\infty}^\infty y\int_{-\infty}^\infty
         f_{X,Y}(x,y)\d{x}\d{y}
	\end{equation*}
	\begin{equation*}
		\Esp{\Espg{Y}{X}}
		=\int_{-\infty}^\infty yf_Y(y)\d{y}
		=\Esp{Y}.\qedhere
	\end{equation*}
\end{proof}

\begin{theoreme}
	$\Esp{h(Y)}=\Esp{\Espg{h(y)}{X}}$.
\end{theoreme}

\subsection{Variance conditionnelle}

\begin{theoreme}
	$\Varg{Y}{X}=\Espg{Y^2}{X}-\left(\Espg{Y}{X}\right)^2$.
\end{theoreme}


\begin{theoreme}
	$\Var{Y}=\Esp{\Varg{Y}{X}}+\Var{\Espg{Y}{X}}$.
\end{theoreme}

\subsection{Covariance et corrélation}

\begin{theoreme}
	$\Esp{g(X,Y)}=\displaystyle\sum_{j=1}^\infty
	\displaystyle\sum_{k=1}^\infty g(x_j,y_k)p_{X,Y}(x_j,y_k)$
	dans un cas discret.
\end{theoreme}
\begin{theoreme}
	$\Esp{g(X,Y)}=\displaystyle\int_{-\infty}^\infty
	\displaystyle\int_{-\infty}^\infty g(x,y)f_{X,Y}(x,y)\d{x}\d{y}$
	dans un cas discret.
\end{theoreme}

\begin{exemple}
	Soit la fonction de masse conjointe de $X$ et $Y$ suivante.
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccc}
			  & 0 & 1 & 2\\
			\hline
			0 & $\nicefrac{1}{6}$ & $\nicefrac{1}{6}$ & $\nicefrac{1}{6}$\\
			1 & 0                 & $\nicefrac{1}{6}$ & $\nicefrac{1}{6}$\\
			2 & 0                 & 0                 & $\nicefrac{1}{6}$\\
		\end{tabular}
	\end{table}


	\begin{equation*}
		\Esp{X^2Y}=\sum_{j=0}^2\sum_{k=0}^2j^2kp_{X,Y}(j,k)=\frac{13}{6}
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit $X\sim\Norm{1}{1}$ et $Y\sim\Norm{2}{1}$ des variables aléatoires
	indépendates de sorte que
	\begin{equation*}
		f_{X,Y}(x,y)=f_X(x)f_Y(y).
	\end{equation*}
	Calculer $\Esp{X+Y}$ et $\Esp{XY}$.

	Par définition, on a
	\begin{equation*}
		\Esp{X+Y}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty(x+y)f_{X,Y}(x,y)\d{x}\d{y},
	\end{equation*}
	et avec la linéarité de l'espérance, on obtient
	\begin{equation*}
		\Esp{X+Y}
		=\underbrace{\int_{-\infty}^\infty\int_{-\infty}^\infty
		 xf_{X,Y}(x,y)\d{x}\d{y}}_{\Esp{X}}
		+\underbrace{\int_{-\infty}^\infty
		 y\int_{-\infty}^\infty f_{X,Y}(x,y)\d{x}\d{y}}_{\Esp{Y}}
	\end{equation*}
	de sorte à obtenir que $\Esp{X+Y}=3$. L'indépendance des variables n'est
	pas importante.

	Par définition, on a
	\begin{equation*}
		\Esp{XY}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty(xy)f_{X,Y}(x,y)\d{x}\d{y},
	\end{equation*}
	et avec l'indépendance des variables, on obtient
	\begin{equation*}
		\Esp{XY}
		=\int_{-\infty}^\infty xf_X(x)\d{x}\int_{-\infty}^\infty yf_Y(y)\d{y}
		=\Esp{X}\Esp{Y}
	\end{equation*}
	de sorte à obtenir $\Esp{XY}=2$.
\end{exemple}

\begin{theoreme}
	$\Esp{aX+bY}=a\Esp{X}+b\Esp{Y}$.
\end{theoreme}

\begin{theoreme}
	$\Esp{g(X,Y)}=\Esp{g_1(X)}\Esp{g_2(Y)}$ si $X$ et $Y$ indépendants et
	$g(X,Y)=g_1(X)g_2(Y)$.
\end{theoreme}

\begin{definition}
	La \textit{corrélation} de deux variables aléatoires $X$ et $Y$ est
	l'espérance de leur produit, soit $\Esp{XY}$.
\end{definition}

Lorsque $\Esp{XY}=0$, on dit que $X$ et $Y$ sont orthogonales.

\begin{definition}
	La \textit{covariance} de deux variables aléatoires $X$ et $Y$ est donnée
	par
	\begin{equation*}
		\Cov{X}{Y}=\Esp{XY}-\Esp{X}\Esp{Y}.
	\end{equation*}
\end{definition}

\begin{theoreme}
	$\Cov{X}{X}=\Var{X}$.
\end{theoreme}

\begin{definition}
	Le \textit{coefficient de corrélation} $\rho_{X,Y}$ de deux variables
	aléatoires $X$ et $Y$ est donné par
	\begin{equation*}
		\rho_{X,Y}=\frac{\Cov{X}{Y}}{\Std{X}\Std{Y}}.
	\end{equation*}
\end{definition}

\begin{theoreme}
	$-1\leq\rho_{X,Y}\leq 1$.
\end{theoreme}

\begin{theoreme}
	$\rho_{X,Y}=1\Leftrightarrow Y=aX+b$ où $a>0$.
\end{theoreme}

\begin{theoreme}
	$\rho_{X,Y}=-1\Leftrightarrow Y=aX+b$ où $a<0$.
\end{theoreme}

\begin{theoreme}
	$\rho_{X,Y}=0$ si $X$ et $Y$ sont indépendants.
\end{theoreme}

% rho=0, X et Y ne sont pas nécessairement indépendantes (varialbes non-corrélées)

% rho=0, X,Y suivent des normales, alors on peut dire que $X$ et $Y$ sont indépendantes

\subsection{Loi binormale}
Soit un vecteur aléatoire $(X,Y)$, où la fonction de probabilité de densité suit
l'équation 
\begin{equation*}
	f_{X,Y}(x,y)=\frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}}
	\exp\left\{-\frac{1}{2(1-\rho)^2}\left[
		\left(\frac{x-\mu_X}{\sigma_X}\right)^2+
		\left(\frac{x-\mu_Y}{\sigma_Y}\right)^2+
		2\rho\frac{(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}
	\right]\right\},
\end{equation*}
suit une loi binormale dénotée
\begin{equation*}
	(X,Y)\sim\BiNorm{\mu_X}{\mu_Y}{\sigma_X^2}{\sigma_Y^2}{\rho}.
\end{equation*}
où $\rho=\rho_{X,Y}$.

Il en résulte que $X\sim\Norm{\mu_X}{\sigma_X^2}$ $Y\sim\Norm{\mu_Y}{\sigma_Y^2}$.
Si $\rho=0$, alors on voit que $f_{X,Y}=f_X(x)f_Y(y)$ de sorte que $X$ et $Y$
sont indépendants.

La conditionelle de cette loi est une loi normale, soit
\begin{equation*}
	X|\left\{Y=y\right\}\sim\Norm{\mu}{\sigma^2},
\end{equation*}
où
\begin{equation*}
	\mu=\mu_X+\rho\frac{\sigma_X}{\sigma_Y}(y-\mu_Y)
\end{equation*}
et
\begin{equation*}
	\sigma^2=\sigma_X^2(1-\rho^2).
\end{equation*}

\subsection{Estimation}
On cherche à prévoir la valeur inconnue de $Y$ à partir de $X$. Soit $g(X)$ la
fonction estimateur de $Y$. On cherche à minimiser l'erreur quadratique moyenne
définie par
\begin{equation*}
	\Esp{\left(Y-g(X)\right)^2}.
\end{equation*}

\subsubsection{Estimateur constant}
Dans un cas où $g(X)=c$, le problème se résume à
\begin{equation*}
	\Min\Esp{\left(Y-c\right)^2}.
\end{equation*}
En dérivant, on obtient la constante optimale
\begin{equation*}
	\hat{c}=\Esp{Y},
\end{equation*}
de sorte à obtenir l'erreur d'estimation
\begin{equation*}
	\Esp{\left(Y-\Esp{Y}\right)^2}=\Var{Y}.
\end{equation*}

\subsubsection{Estimateur linéaire}
Dans un cas où $g(X)=aX+b$, le problème se résume à
\begin{equation*}
	\Min\Esp{\left(Y-aX-b\right)^2}
\end{equation*}
de sorte à obtenir
\begin{equation*}
	\hat{a}=\frac{\Cov{X}{Y}}{\Var{X}}=\frac{\Std{Y}}{\Std{X}}\rho_{X,Y}
\end{equation*}
et
\begin{equation*}
	\hat{b}=\Esp{Y}-\hat{a}\Esp{X}.
\end{equation*}

Par conséquent, le meilleur estimateur linéaire de $Y$ en fonction de $X$ est
\begin{equation*}
	g(X)=\hat{a}X+\hat{b}
\end{equation*}
avec une erreur d'estimation donnée par
\begin{equation*}
	\Var{Y}\left(1-\rho_{X,Y}^2\right).
\end{equation*}

\subsubsection{Estimateur non linéaire}
\begin{equation*}
	\Min\Esp{\left(Y-g(X)\right)^2}
\end{equation*}

\begin{equation*}
	\Min\Esp{\Espg{\left(Y-g(X)\right)^2}{X}}
\end{equation*}

\begin{equation*}
	\Min\sum_{j=1}^\infty
		\underbrace{\Espg{Y-g(X)}{X=x_j}}_{\text{fonction positive à min $\forall x_j$}}
		p_X(x_j)
\end{equation*}

On minimise pour chaque $x_j$ avec une constante différente. On retrouve un cas
$g(X)=c$, mais conditionnel à $X=x_j$. On retrouve que le meilleur estimateur
de $Y$ en fonction de $X$ est
\begin{equation*}
	g(X)=\Espg{Y}{X}.
\end{equation*}

\subsubsection{Estimateur d'une binormale}
Lorsque $X$ et $Y$ suivents des distributions normales, alors on retrouve que
\begin{equation*}
	\Espg{Y}{X}=\hat{a}X+\hat{b}.
\end{equation*}

\begin{exemple}
	Soit $(X,Y)\sim\BiNorm{\mu_X=3}{\mu_Y=1}{\sigma_X^2=4}{\sigma_Y^2=9}{\rho=\nicefrac{1}{4}}$,
	quel est le meilleur estimateur de $Y$ en fonction de $X$?

	\begin{equation*}
		\hat{a}=\frac{\Std{Y}}{\Std{X}}\rho=\frac{3}{8}
	\end{equation*}

	\begin{equation*}
		\hat{b}=\Esp{Y}-\hat{a}\Esp{X}=-\frac{1}{8}
	\end{equation*}

	\begin{equation*}
		g(X)=\frac{3}{8}X-\frac{1}{8}
	\end{equation*}
\end{exemple}

\subsection{Combinaisons linéaires}
\begin{definition}
	Soit un vecteur aléatoire ($X_1,X_2,\dots,X_n$), alors la
	\textit{combinaison linéaire} $Z$ des variables aléatoires est
	\begin{equation*}
		Z=a_0+a_1X_1+a_2X_2+\cdots+a_nX_n=a_0+\sum_{i=1}^na_iX_i.
	\end{equation*}
\end{definition}

\begin{theoreme}
	$\Esp{Z}=a_0+\displaystyle\sum_{i=1}^na_i\Esp{X_i}$.
\end{theoreme}

\begin{theoreme}
	$\Var{Z}=\displaystyle\sum_{i=1}^n\displaystyle\sum_{j=1}^na_ia_j
	\Cov{X_i}{X_j}$.
\end{theoreme}
\begin{proof}
	Soit $Y_i=a_iX_i$. Par définition, on a
	\begin{equation*}
		\Var{Z}
		=\Var{\sum_{i=1}^nY_i}
		=\Esp{\left(\sum_{i=1}^nY_i\right)^2}
		-\left(\sum_{i=1}^n\Esp{Y_i}\right)^2.
	\end{equation*}

	En écrivant explicitement le carré, on a
	\begin{equation*}
		\Var{Z}
		=\Esp{\left(\sum_{i=1}^nY_i\right)\left(\sum_{j=1}^nY_j\right)}
		-\left(\sum_{i=1}^n\Esp{Y_i}\right)\left(\sum_{j=1}^n\Esp{Y_j}\right)
	\end{equation*}

	\begin{equation*}
		\Var{Z}
		=\Esp{\sum_{i=1}^n\sum_{j=1}^nY_iY_j}
		-\sum_{i=1}^n\sum_{j=1}^n\Esp{Y_i}\Esp{Y_j}
	\end{equation*}

	\begin{equation*}
		\Var{Z}
		=\sum_{i=1}^n\sum_{j=1}^n\bigg(
			\Esp{Y_iY_j}-\Esp{Y_i}\Esp{Y_j}\bigg)
		=\sum_{i=1}^n\sum_{j=1}^na_ia_j\Cov{X_i}{X_j}
		\qedhere
	\end{equation*}
\end{proof}

\begin{theoreme}
	$\Var{Z}=
	\displaystyle\sum_{i=1}^na_i^2\Var{X_i}+
	2\underbrace{\displaystyle\sum_{i=1}^n\displaystyle\sum_{j=1}^n}_{i<j}
	a_ia_j\Cov{X_i}{X_j}$.
\end{theoreme}

\subsection{Variables aléatoires indépendantes}
Soit $X_1,X_2,\dots,X_n$ des variables aléatoires indépendantes.

\begin{theoreme}
	$\Cov{X_i}{X_j}=0$.
\end{theoreme}

\begin{theoreme}
	$\Esp{Z}=a_0+\displaystyle\sum_{i=1}^n\Esp{X_i}$.
\end{theoreme}

\begin{theoreme}
	$\Var{Z}=\displaystyle\sum_{i=1}^na_i^2\Var{X_i}$.
\end{theoreme}

\subsection{Variables aléatoires indépendantes et identiquement distributées}
Soit $X_1,X_2,\dots,X_n$ des variables aléatoires indépendantes et
identiquement distribuées de sorte que $Z=X_1+X_2+\dots+X_n$.

\begin{theoreme}
	$\Esp{S_n}=n\Esp{X_1}$.
\end{theoreme}

\begin{theoreme}
	$\Var{S_n}=n\Var{X_1}$.
\end{theoreme}

\begin{exemple}
	Si $X_1\sim\Bin{n_1}{p}$ et $X_2\sim\Bin{n_2}{p}$ sont des variables
	aléatoires indépendantes, alors on a
	\begin{equation*}
		X_1+X_2\sim\Bin{n_1+n_2}{p}.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Si $X_i\sim\Poi{\alpha_i}$ sont des variables aléatoires indépendantes,
	alors on a
	\begin{equation*}
		X_1+X_2+\dots+X_n\sim\Poi{\alpha},
	\end{equation*}
	où $\alpha=\alpha_1+\alpha_2+\dots+\alpha_n$.
\end{exemple}

\begin{exemple}
	Si $X_i\sim\Exp{\lambda}$ sont des variables aléatoires indépendantes,
	alors on a
	\begin{equation*}
		X_1+X_2+\dots+X_n\sim\Gam{n}{\lambda}.
	\end{equation*}
\end{exemple}

\begin{exemple}
	Si $X_i\sim\Gam{\alpha_i}{\lambda}$ sont des variables aléatoires
	indépendantes, alors on a
	\begin{equation*}
		X_1+X_2+\dots+X_n\sim\Gam{\alpha}{\lambda},
	\end{equation*}
	où $\alpha=\alpha_1+\alpha_2+\dots+\alpha_n$.
\end{exemple}

\begin{exemple}
	Si $X_i\sim\Norm{\mu_i}{\sigma_i^2}$ sont des variables aléatoires
	indépendantes, alors on a
	\begin{equation*}
		a_0+\sum_{i=1}^na_iX_i\sim\Norm{\mu}{\sigma^2},
	\end{equation*}
	où
	\begin{equation*}
		\mu=a_0+\sum_{i=1}^na_i\mu_i
		\quad\text{et}\quad
		\sigma^2=\sum_{i=1}^n\sum_{j=1}^na_ia_j\Cov{X_i}{X_j}.
	\end{equation*}
\end{exemple}

\subsection{Loi faible des grands nombres}
Soit $X_1,X_2,\dots,X_n$ des variables aléatoires indépendantes et
indentiquements distributées de moyenne $\mu$. Soit $S_n=X_1+X_2+\dots+X_n$,
alors on a
\begin{equation*}
	\lim_{n\rightarrow\infty}\P{\left\rvert
		\frac{S_n}{n}-\mu
	\right\rvert<c}=1,
\end{equation*}
pour tout $c>0$.

\subsection{Loi forte des grands nombres}
Soit $X_1,X_2,\dots,X_n$ des variables aléatoires indépendantes et
indentiquements distributées de moyenne $\mu$ et de variance $\sigma^2$. Soit
$S_n=X_1+X_2+\dots+X_n$, alors on a
\begin{equation*}
	\P{\lim_{n\rightarrow\infty}\frac{S_n}{n}=\mu}=1.
\end{equation*}

\subsection{Théorème central limite}
Soit $X_1,X_2,\dots,X_n$ des variables aléatoires indépendantes et
identiquements distribuées de moyenne $\mu$ et de variance $\sigma^2$. Si $n$
est \textit{grand}, on a que $S_n\approx\Norm{n\mu}{n\sigma^2}$ ou encore
\begin{equation*}
	\frac{S_n}{n}\approx\Norm{\mu}{\frac{\sigma^2}{n}},
\end{equation*}
et
\begin{equation*}
	\frac{S_n-n\mu}{\sqrt{n\sigma^2}}\approx\Norm{0}{1}.
\end{equation*}

En général, le théorème central limite est applicable si $n\geq 30$.

\begin{exemple}
	Soit $X\sim\Bin{2000}{\nicefrac{1}{2}}$. Calculez $\P{990\leq X\leq 1100}$.

	On approxime la loi binomiale par la loi normale. Par conséquent,
	\begin{equation*}
		X\approx\Norm{np}{npq}
	\end{equation*}
	et
	\begin{equation*}
		\P{X=k}\approx\frac{1}{\sqrt{2\pi\cdot npq}}\exp\left\{
			-\frac{(k-np)^2}{2npq}
		\right\}
	\end{equation*}
	% Phi(4.5)-Phi(-4.5)\approx 1

	Puisqu'on approxime une loi discrète par une loi continue, on applique
	une \textit{correction de continuité}, c'est-à-dire
	\begin{equation*}
		\P{a\leq X\leq b}
		\approx\Phi\left(
			\frac{b+\nicefrac{1}{2}-np}{\sqrt{npq}}
		\right)-\Phi\left(
			\frac{a-\nicefrac{1}{2}-np}{\sqrt{npq}}
		\right)
	\end{equation*}

	\begin{equation*}
		\P{a<X\leq b}
		\approx\Phi\left(
			\frac{b+\nicefrac{1}{2}-np}{\sqrt{npq}}
		\right)-\Phi\left(
			\frac{a+\nicefrac{1}{2}-np}{\sqrt{npq}}
		\right)
	\end{equation*}
\end{exemple}

\begin{exemple}
	Soit la fonction de probabilité de densité suivante
	\begin{equation*}
		f_{X,Y}(x,y)=\frac{3}{4},
	\end{equation*}
	si $0<y<1$ et $x^2<y$. Calculez $f_X(x)$ et $f_Y(y)$, ainsi que
	$\P{Y\geq X}$.

	Pour un $x$, on a
	\begin{equation*}
		f_X(x)
		=\int_{x^2}^1f_{X,Y}(x,y)\d{y}
		=\frac{3}{4}(1-x^2),
	\end{equation*}
	si $-1<x<1$. Pour un $y$, on a
	\begin{equation*}
		f_Y(y)
		=\int_{-\sqrt{y}}^{\sqrt{y}}f_{X,Y}(x,y)\d{x}
		=\frac{3}{2}\sqrt{y},
	\end{equation*}
	si $0<y<1$.

	Par définition, on a
	\begin{equation*}
		\P{Y\geq X}
		=\int_0^1\int_{-\sqrt{y}}^{\sqrt{y}}f_{X,Y}(x,y)\d{x}\d{y}
		=\frac{7}{8}.
	\end{equation*}
\end{exemple}

\section{Processus Stochastique}
\begin{definition}
	Un \textit{processus stochastique} est une expérience aléatoire qui ce
	déroule dans le temps.
\end{definition}

On note $X(t)$ la variable aléatoire correspondant à l'observation du processus
au un temps $t\geq 0$. Les valeurs possibles pour ces observations forment
\textit{l'espace des états} du processus. Les événements élémentaires du
processus sont ce qu'on appel des \textit{trajectoires} dans l'espace des
états.

Un processus stochastique est défini par l'ensemble $\left\{X(t)\middle\rvert
t\in T\right\}$, avec $T=\left\{0,1,2,\dots\right\}$ dans un cas discret ou
$T=\left[0,\infty\right[$ dans un cas continue.

Il existe 4 types de processus stochastique:
\begin{enumerate}
	\item processus stochastique à temps discret et état discret (PSTDED)
	\begin{exemple*}
		Le nombre de personnes en classe à chaque jour.
	\end{exemple*}
	\item processus stochastique à temps discret et état continu (PSTDEC)
	\begin{exemple*}
		La valeur d'une action à la fermmeture de la bourse.
	\end{exemple*}
	\item processus stochastique à temps continu et étant discret (PSTCED)
	\begin{exemple*}
		La longueur de la file d'attente à la cafétéria.
	\end{exemple*}
	\item processus stochastique à temps discret et état discret (PSTCEC)
	\begin{exemple*}
		La valeur d'une action en temps réel.
	\end{exemple*}
\end{enumerate}

\subsection{Caractéristique des processus stochastiques}
\subsubsection{PSTCEC}
Dans le cas d'un processus stochastique à temps continu et état continu, 


Dans le cas d'un PSTCEC, il y a une fonction de répartition du 1er ordre, soit
$F(x;t)=\P{X(t)\leq x}$, une fonction de répartition du 2e ordre, soit
$F(x_1,X_2;t_1,t_2)=\P{X(t_1)\leq x_1,X(t_2)\leq x_2}$.

\begin{equation*}
	F(x_1,\dots,x_n;t_1,\dots,t_n)=\P{
		\left\{X(t_1)\leq x_1\right\}\cap\cdots\cap
		\left\{X(t_n)\leq x_n\right\}\cap
	}
\end{equation*}

\begin{equation*}
	f(x_1,\dots,x_n;t_1,\dots,t_n)=\frac{\p[n]{}}{\p{x_1}\cdots\p{x_n}}
		F(x_1,\dots,x_n;t_1,\dots,t_n)
\end{equation*}

\begin{exemple}
	Soit un patient qui recoit une dose de médicament $Y$. Au temps $t=0$, on
	a $Y\sim\Uni{0}{1}$. La quantité active $X$ après $t$ unités de temps est
	définie par $X(t)=Y\e^{-t}$. Quel est la fonction de répartition du
	premier ordre, soit $F(x;t)$?
	% faire graphique maybe?

	Par définition, on a $F(x;t)=\P{X(t)\leq x}=\P{Y}\leq x\e^t$. Pour une loi
	uniforme, on a
	\begin{equation*}
		F(x;t)=\left\{
			\begin{matrix}
				0     &\text{si}& x\e^t<0\\
				x\e^t &\text{si}& 0\leq x\leq e^{-t}\\
				1     &\text{si}& \e^{-t}<x\\
			\end{matrix}
		\right..
	\end{equation*}
\end{exemple}

\subsubsection{Moyenne d'un processus stochastique}
\begin{definition}
	La \textit{moyenne} d'un processus stochastique, dénotée $m_X(t)$, est
	donnée par
	\begin{equation*}
		m_X(t)=\Esp{X(t)}=\int_{-\infty}^\infty xf(x;t)\d{x}.
	\end{equation*}
\end{definition}

\begin{exemple}
	Quel est la moyenne de $X(t)=Y\e^{-t}$, où $Y\sim\Uni{0}{1}$?

	Avec l'exemple précèdent, on obtient que la fonction de densité de
	probabilité est donnée par
	\begin{equation*}
		f(x;t)=\frac{\p{}}{\p{x}}F(x;t)=\e^t,
	\end{equation*}
	pour $0\leq x\leq e^{-t}$. La moyenne est donc
	\begin{equation*}
		m_X(t)=\int_0^{e^{-t}}x\e^t\d{x}=\frac{e^{-t}}{2}.
	\end{equation*}
\end{exemple}

\subsubsection{Fonction d'autocorrélation}
\begin{definition}
	La \textit{fonction d'autocorrélation}, dénotée $R_X(t_1,t_2)$, est la
	corrélation d'un processus stochastique $X(t)$ avec lui-même à deux temps
	différents. Par conséquent, on a
	\begin{equation*}
		R_X(t_1,t_2)
		=\Esp{X(t_1)X(t_2)}
		=\int_{-\infty}^\infty\int_{-\infty}^\infty
			f(x_1,x_2;t_1,t_2)\d{x_1}\d{x_2}.
	\end{equation*}
\end{definition}

\subsubsection{Fonction d'autocovariance}
\begin{definition}
	La \textit{fonction d'autocovariance}, dénotée $C_X(t_1,t_2)$, est la
	covariance d'un processus stochastique $X(t)$ avec lui-même à deux temps
	différents. Par conséquent, on a
	\begin{equation*}
		C_X(t_2,t_2)
		=R_X(t_1,t_2)-m_X(t_1)m_X(t_2).
	\end{equation*}
\end{definition}

\subsubsection{Processus stochastique stationnaire au sens large}
\begin{definition}
	Si $m_X(t)=c$, où $c$ est une constante, et si $R_X(t_1,t_2)=h(t_2-t_1)$, où
	$h$ est une fonction, alors on dit que le processus stochastique est
	\textit{stationnaire au sens large} (SSL).
\end{definition}

\begin{exemple}
	Soit un signal aléatoire $X(t)=Y\sin{(t+Z)}$, où $Y$ et $Z$ sont des
	variables aléatoires indépendantes et $Z\sim\Uni{0}{2\pi}$. Montrer que ce
	signal est SSL.

	Par définition, la moyenne est donnée par
	\begin{equation*}
		m_X(t)
		=\Esp{Y\sin{(t+Z)}}
		=\Esp{Y}\underbrace{\Esp{\sin{(t+Z)}}}_{
			\int_0^{2\pi}\sin{(t+z)}\frac{1}{2\pi}=0}\d{z}
		=0,
	\end{equation*}
	ce qui est une constante. De plus, la fonction d'autocorrélation est donnée
	par
	\begin{equation*}
		R_X(t_1,t_2)
		=\Esp{Y\sin{(t_1+Z)}Y\sin{(t_2+Z)}}
		=\Esp{Y^2}\Esp{\sin{(t_1+Z)}\sin{(t_2+Z)}},
	\end{equation*}
	et avec une identité trigonométrique, on obtient
	\begin{equation*}
		\begin{split}
			R_X(t_1,t_2)
			&=\frac{1}{2}\Esp{Y^2}\left(
				\Esp{\cos{(t_1-t_2)}}-\underbrace{\Esp{\cos{(t_1-t_2+2Z)}}}_0
			\right)\\
			&=\frac{1}{2}\Esp{Y^2}\cos{(t_1-t_2)}
			=h(t_2-t_1).
		\end{split}
	\end{equation*}

	Par conséquent, ce signal est stationnaire au sens large.
\end{exemple}

\subsection{Chaîne de Markov}
\begin{definition}
	Une \textit{chaîne de Markov} est un processus stochastique à temps discret
	et état continu où :
	\begin{enumerate}
		\item l'espace des états est un ensemble de nombres entiers
		\item on dénote $X_n$ l'état au temps $n=0,1,\dots$
		\item la probabilité de passer de l'état $i$ à l'état $j$ suivant ne
		      dépend ni de $n$ ni de la trajectoire passée
	\end{enumerate}
\end{definition}

On dénote $p_{i,j}$ la probabilité de passer de l'état $i$ vers l'état $j$
comme étant la probabilité conditionnelle suivante, soit
\begin{equation*}
	p_{i,j}=\Pg{X_{n+1}=j}{X_n=i}.
\end{equation*}

\subsubsection{Représentation graphique}
Une chaîne de Markov peut se représenter comme un graphe, où les noeuds sont
les états et les segments sont les probabilité $p_{i,j}$. La figure
\ref{fig:markov_graph} représente le graphe d'une chaîne de Markov ayant 4
états.

\begin{figure}[H]
	\centering
	\input{figure/markov_chain}
	\caption{Chaîne de Markov à 4 états}
	\label{fig:markov_graph}
\end{figure}

\subsubsection{Représentation matricielle}
Soit $P$ la matrice des probabilités de transition en une étape où chaque
cellule $p_{i,j}$ sont les probabilités de passer de l'état $i$ vers l'état $j$
tel que
\begin{equation*}
	P=\left[
		\begin{matrix}
			p_{0,0} & p_{0,1} & \cdots & p_{0,n}\\
			p_{1,0} & p_{1,1} & \cdots & p_{1,n}\\
			\vdots  & \vdots  & \ddots & \vdots \\
			p_{n,0} & p_{n,1} & \cdots & p_{n,n}\\
		\end{matrix}
	\right].
\end{equation*}

\begin{exemple}
	On établie des prévisions météorologiques. On défini 3 états suivants : une
	journée ensoleillée (état 0), une journée nuageuse (état 1) et une journée
	pluvieuse (état 2). Les probabilités de transition sont définies par la
	matrice suivante :
	\begin{equation*}
		P=\left[
			\begin{matrix}
				0.40 & 0.35 & 0.25\\
				0.30 & 0.20 & 0.50\\
				0.75 & 0.10 & 0.15\\
			\end{matrix}
		\right]
	\end{equation*}
	Quelle est la probabilité qu'il pleuve le lendemain d'une journée
	ensoleillée? Quelle est la probabilité que trois journées ensoleillées
	suivent une journée nuageuse? Quelle est la probabilité qu'il ne pleuve pas
	pendant les deux jours suivant une journée pluvieuse?

	On cherche $p_{0,2}$ dans la matrice, alors la probabilité est de 25\%.

	On cherche $p_{1,0}p_{0,0}p_{0,0}$, alors la probabilité est de 4.8\%.

	Il faut calculer toutes les trajectoires possibles. Il y a 4 cas possibles
	%
	% 2 -- 0 -- 0
	%   |    |
	%   |    -- 1
	%   |
	%   -- 1 -- 0
	%        |
	%        -- 1
	On cherche $p_{2,0}p_{0,0}+p_{2,0}p_{0,1}+p_{2,1}p_{1,0}+p_{2,1}p_{1,1}$,
	alors la probabilité est 61.25\%.
\end{exemple}

\subsubsection{Probabilité de transition en \boldmath{$n$} étapes}
La probabilité d'atteindre un état $j$ à partir d'un état $i$ en $n$ étapes,
dénotée $p_{i,j}^n=\Pg{X_{m+n}=j}{X_m=i}$, est donnée en multipliant $n$ fois
la matrice des probabilités $P$, c'est à dire
\begin{equation*}
	P^{(n)}=\underbrace{P\cdot P\cdots P}_{\text{$n$ fois}}.
\end{equation*}

% exemple particule déplacement aléatoire 1/2 gauche/droite
% représentation d'une matrice infini difficile
% 
% D : nb de déplacements vers la droite
% D suit Bin(n, 1/2)
%
% G : nb de déplacements vers la gauche
% G = n - D
%
% X_n = D - G = D - (n - D) = 2D - n
%
% P_{0,i} = P(X_n=i | X_0=0) = P(2D-n=i) = P(D=(n+i)/2)
% si n et i impair
% ou n et i paire

\subsection{Processus de Poisson}
\begin{definition}
	Un processus de poisson est un processus stochastique à temps continue et
	état discret. On dénote $N(t)$ le nombre d'événements qui se sont produits
	dans l'intervalle de temps $[0,t]$.

	Trois conditions fondamentales définissent le processus de Poisson :
	\begin{enumerate}
		\item $N(0)=0$
		\item Soit les intervalles $]t_1,t_2]$ et $]t_3,t_4]$, avec
		      $t_1<t_2\leq t_3<t_4$, et les variables aléatoires indépendantes
			  $X=N(t_2)-N(t_1)$ et $Y=N(t_4)-N(t_3)$. On dit que les
			  \textit{accroissements} du processus de Poisson sont
			  indépendants.
		\item Soit $X=N(\tau+t)-N(\tau)$ l'accroissement dans l'intervalle
		      $]\tau,\tau+t]$. Alors, on a que $X\sim\Poi{\lambda t}$, où
			  $\lambda$ est le taux du processus de poisson. On dit que les
			  accroissements sont \textit{stationnaire}.
	\end{enumerate}
\end{definition}

\begin{theoreme}
	$C_N(t_1,t_2)=\lambda\cdot\text{min}(t_1,t_2)$.
\end{theoreme}

\begin{exemple}
	Soit un processus de Poisson de taux $\lambda=2$ avec $t_1=3$, $t_2=7$,
	$t_3=8$ et $t_4=10$. Quelle est la probabilité d'avoir 1 événement dans
	l'intervalle $]t_1,t_2]$ et plus de 1 événement dans l'intervalle
	$]t_3,t_4]$?
	
	On définie $X$ et $Y$ comme étant le nombre d'événements dans $]t_1,t_2]$
	et $]t_3,t_4]$ respectivement, alors $X\sim\Poi{\lambda t}$ et
	$Y\sim\Poi{\lambda t}$. Par conséquent, on cherche
	\begin{equation*}
		\P{\left\{X=1\right\}\cap\left\{Y\geq 1\right\}}
		=\P{X=1}\P{Y\geq 1}
		=8\e^{-8}\left(1-\e^{-4}\right).
	\end{equation*}
\end{exemple}

\subsubsection{Temps d'arrivé}
Soit $T_1$ le temps d'arrivée du premier événement. On cherche
\begin{equation*}
	\P{T_1>t}
	=\P{N(t)=0}
	=\e^{-\lambda t}
\end{equation*}
% ligne du temps
%
% 0-----------t----x------>
% |                |
% '----------------'

Par conséquent,
\begin{equation*}
	T_1\sim\Exp{\lambda}
\end{equation*}

Soit $T_2$ le temps entre le premier et le deuxième événement. On a que 
$T_1=t_1$ lorsque le premier événement ce produit. On définit
\begin{equation*}
	M(t)=N(t+t_1)-\underbrace{N(t_1)}_{1},
\end{equation*}
le nombre d'événements dans $[t_1,t]$. On remarque que $M(t)$ est aussi un
processus de Poisson de sorte que $T_2\sim\Exp{\lambda}$.

Soit $T_k$ le temps entre $(k-1)$-ième et le $k$-ième événement. Par
conséquent, $T_k\sim\Exp{\lambda}$ avec $T_1,T_2,\dots$ sont des variables
aléatoires indépendantes et identiquement distributées.

Soit
\begin{equation*}
	S_n=\sum_{k=1}^nT_k,
\end{equation*}
le temps d'arrivée du $n$-ième événement. Par conséquent,
\begin{equation*}
	S_n\sim\Gam{n}{\lambda}.
\end{equation*}

\begin{exemple}
	En observant un processus de Poisson de taux $\lambda$, on remarque qu'un
	seul événement s'est produit avant le temps $t$. Quelle est la distribution
	du temps d'arrivée $T$ de cet événement?

	Le problème s'écrit en terme d'une conditionnelle, soit
	\begin{equation*}
		\begin{split}
		\Pg{T\leq\tau}{N(t)=1}
		&=\Pg{N(\tau)=1}{N(t)=1}
		=\frac{\P{\left\{N(\tau)=1\right\}\cap\left\{N(t)=1\right\}}}
			{\P{N(t)=1}}\\
		&=\frac{\P{\left\{X=1\right\}\cap\left\{Y=0\right\}}}
			{\P{N(t)=1}}
		=\frac{\tau}{t}
		\end{split}
	\end{equation*}
	$N(\tau)=X$, $N(t)=X+Y$, $X\sim\Poi{\lambda\tau}$, $Y\sim\Poi{\lambda(t-\tau)}$
	%
	% ^   l             l
	% |   l    .--------l--------
	% |   l             l
	% +---l----T--tau---l--t----->
	%
\end{exemple}

\subsection{Processus de Wiener}
\begin{definition}
	Un \textit{processus de Wiener} est un processus stochastique à temps
	continu et état continu où $W(t)$ représente la position aléatoire d'une
	particule à un temps $t$ le long d'un axe.
\end{definition}

On construit ce processus à partir de la position $X(t)$ de la particule
soumise à une marche aléatoire avec des pas de $\pm\epsilon$ de distance à
chaque $\delta$ unité de temps. On pose $\epsilon=\sigma\sqrt{\delta}$, où 
$\sigma$ est une constante, on obtient
\begin{equation*}
	W(t)=\lim_{\delta\rightarrow 0^+}X(t)
\end{equation*}
On définit 3 conditions pour qu'un processus stochastique soit un processus
de Wiener :
\begin{enumerate}
	\item $W(0)=0$
	\item les accroissements du processus sont indépendants et stationnaires
	\item $W(t)\sim\Norm{0}{\sigma^2 t}$
\end{enumerate}

On appel $\sigma^2$ le \textit{coefficient de diffusion}. Si $\sigma^2=1$, on
appel ce processus le \textit{mouvement brownien standard} et on dénote parfois
$B(t)=W(t)$. Les trajectoires de ce processus sont des fonctions continues,
mais dérivables nulle part, comme une fractale. La \textit{dérivée généralisée}
de ce processus s'appele \textit{bruit blanc gaussien}.

\begin{exemple}
	Calculer la fonction de densité du 2-ième ordre $f(w_1,w_2,t_1,t_2)$ du
	processus de Wiener. On suppose que $t_1<t_2$.

	On peut écrire la densité de l'événement suivant en terme d'accroissements,
	soit
	\begin{equation*}
		\left\{W(t_1)=w_1,W(t_2)=w_2\right\}
		\equiv\left\{W(t_1)=w_1,W(t_2)-W(t_1)=w_2-w_1\right\}.
	\end{equation*}
	On définie $X=W(t_1)\sim\Norm{0}{\sigma^2t_1}$ et $Y=W(t_2)-W(t_1)\sim
	\Norm{0}{\sigma^2t_2}$. Par conséquent, on a
	\begin{equation*}
		\begin{split}
			f(w_1,w_2;t_1,t_2)
			&=f_X(w_1)f_Y(w_2-w_1)\\
			&=\frac{1}{\sqrt{2\pi\sigma^2t_1}}\exp{-
				\frac{(w_1^2)}{2\sigma^2t_1}
			}\cdot\frac{1}{\sqrt{2\pi\sigma^2(t_2-t_1)}}\exp{-
				\frac{(w_2-w_1)^2}{2\sigma^2(t_2-t_1)}
			}.
		\end{split}
	\end{equation*}
\end{exemple}

\subsubsection{Fonction d'autocovariance}
\begin{equation*}
	C_w(t_1,t_2)=\sigma^2\cdot\min{t_1}{t_2}
\end{equation*}
\end{document}

